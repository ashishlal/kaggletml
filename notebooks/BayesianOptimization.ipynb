{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require(data.table)\n",
    "# require(bit64)\n",
    "# require(dbscan)\n",
    "# require(doParallel)\n",
    "# require(rBayesianOptimization)\n",
    "# path='../input/train_1/'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan as _hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import hdbscan\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# https://www.ellicium.com/python-multiprocessing-pool-process/\n",
    "# http://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import hdbscan as _hdbscan\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(hits):        \n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#     for i, rz_scale in enumerate(self.rz_scales):\n",
    "#         X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "def make_counts(labels):\n",
    "    \n",
    "    \n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def one_loop(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r, a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da \n",
    "    zr = z/r\n",
    "    \n",
    "    X = StandardScaler().fit_transform(np.column_stack([aa, aa/zr, zr, 1/zr, aa/zr + 1/zr]))\n",
    "    _,l = dbscan(X, eps=0.0035, min_samples=1,)\n",
    "\n",
    "    return l\n",
    "\n",
    "def one_loop1(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def one_loop2(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def do_dbscan_predict(df):\n",
    "    \n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    r  = np.sqrt(x**2+y**2)\n",
    "    d  = np.sqrt(x**2+y**2+z**2)\n",
    "    a  = np.arctan2(y,x)\n",
    "\n",
    "    x2 = df['x']/d\n",
    "    y2 = df['y']/d\n",
    "    z2 = df['z']/r\n",
    "\n",
    "    r2 = np.sqrt(x2**2 + y2**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    phi2 = np.arctan2(y2, x2)\n",
    "    phi2_deg = np.degrees(np.arctan2(y2, x2))\n",
    "    \n",
    "    for angle in range(-180,180,1):\n",
    "    \n",
    "        df1 = df.loc[(df.phi_deg>(angle-1.0)) & (df.phi_deg<(angle+1.0))]\n",
    "    \n",
    "        x  = df1.x.values\n",
    "        y  = df1.y.values\n",
    "        z  = df1.z.values\n",
    "        r  = np.sqrt(x**2+y**2)\n",
    "        d  = np.sqrt(x**2+y**2+z**2)\n",
    "        a  = np.arctan2(y,x)\n",
    "\n",
    "        x2 = df1['x']/d\n",
    "        y2 = df1['y']/d\n",
    "        z2 = df1['z']/r\n",
    "    \n",
    "        r2 = np.sqrt(x2**2 + y2**2)\n",
    "        theta= np.arctan2(r, z)\n",
    "        theta1 = np.arctan2(r2, z2)\n",
    "\n",
    "    \n",
    "        tan_dip = phi/theta\n",
    "        tan_dip1 = phi/z2\n",
    "        z2_1 = 1/z2\n",
    "        z2_2 = phi/z2 + 1/z2\n",
    "\n",
    "        dip_angle = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "        dip_angle1 = np.arctan2(z, (np.sqrt(x**2 +y**2)) * np.arccos(x2/np.sqrt(x**2 + y**2)))\n",
    "        scores = []\n",
    "\n",
    "        a_start,a_step,a_num = 0.00100,0.0000095,150\n",
    "    \n",
    "        params  = [(i,m, x,y,z,d,r,r2,z2, a, a_start,a_step) for i in range(a_num) for m in [-1,1]]\n",
    "\n",
    "        if 1: \n",
    "            pool = Pool(processes=1)\n",
    "            ls   = pool.map( one_loop1, params )\n",
    "\n",
    "\n",
    "        if 0:\n",
    "            ls = [ one_loop(param) for param in  params ]\n",
    "\n",
    "\n",
    "        ##------------------------------------------------\n",
    "\n",
    "        num_hits=len(df)\n",
    "        labels = np.zeros(num_hits,np.int32)\n",
    "        counts = np.zeros(num_hits,np.int32)\n",
    "        for l in ls:\n",
    "            c = make_counts(l)\n",
    "            idx = np.where((c-counts>0) & (c<20))[0]\n",
    "            labels[idx] = l[idx] + labels.max()\n",
    "            counts = make_counts(labels)\n",
    "       \n",
    "\n",
    "#     cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "#                          metric='braycurtis',cluster_selection_method='leaf',algorithm='best', \n",
    "#                          leaf_size=50)\n",
    "    \n",
    "#     X = preprocess(df)\n",
    "#     l1 = pd.Series(labels)\n",
    "#     labels = np.unique(l1)\n",
    "   \n",
    "# #   print(X.shape)\n",
    "# #   print(len(labels_org))\n",
    "# #   print(len(labels_org[labels_org ==0]))\n",
    "# #   print(len(labels_org[labels_org ==-1]))\n",
    "    \n",
    "#     n_labels = 0\n",
    "#     while n_labels < len(labels):\n",
    "#         n_labels = len(labels)\n",
    "#         max_len = np.max(l1)\n",
    "#         s = list(l1[l1 == 0].keys())\n",
    "#         X = X[s]\n",
    "#         print(X.shape)\n",
    "#         if X.shape[0] <= 1:\n",
    "#             break\n",
    "#         l = cl.fit_predict(X)+max_len\n",
    "# #         print(len(l))\n",
    "#         l1[l1 == 0] = l\n",
    "#         labels = np.unique(l1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extend(submission,hits):\n",
    "    df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     df = submission.append(hits)\n",
    "#     print(df.head())\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "    for angle in range(-180,180,1):\n",
    "\n",
    "        print ('\\r %f'%angle, end='',flush=True)\n",
    "        #df1 = df.loc[(df.arctan2>(angle-0.5)/180*np.pi) & (df.arctan2<(angle+0.5)/180*np.pi)]\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "        min_num_neighbours = len(df1)\n",
    "        if min_num_neighbours<4: continue\n",
    "\n",
    "        hit_ids = df1.hit_id.values\n",
    "        x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "        r  = (x**2 + y**2)**0.5\n",
    "        r  = r/1000\n",
    "        a  = np.arctan2(y,x)\n",
    "        tree = KDTree(np.column_stack([a,r]), metric='euclidean')\n",
    "\n",
    "        track_ids = list(df1.track_id.unique())\n",
    "        num_track_ids = len(track_ids)\n",
    "        min_length=3\n",
    "\n",
    "        for i in range(num_track_ids):\n",
    "            p = track_ids[i]\n",
    "            if p==0: continue\n",
    "\n",
    "            idx = np.where(df1.track_id==p)[0]\n",
    "            if len(idx)<min_length: continue\n",
    "\n",
    "            if angle>0:\n",
    "                idx = idx[np.argsort( z[idx])]\n",
    "            else:\n",
    "                idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "\n",
    "            ## start and end points  ##\n",
    "            idx0,idx1 = idx[0],idx[-1]\n",
    "            a0 = a[idx0]\n",
    "            a1 = a[idx1]\n",
    "            r0 = r[idx0]\n",
    "            r1 = r[idx1]\n",
    "\n",
    "            da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "            dr0 = r[idx[1]] - r[idx[0]]\n",
    "            direction0 = np.arctan2(dr0,da0) \n",
    "\n",
    "            da1 = a[idx[-1]] - a[idx[-2]]\n",
    "            dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "            direction1 = np.arctan2(dr1,da1) \n",
    "\n",
    "\n",
    "            ## extend start point\n",
    "            ns = tree.query([[a0,r0]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "            direction = np.arctan2(r0-r[ns],a0-a[ns])\n",
    "            ns = ns[(r0-r[ns]>0.01) &(np.fabs(direction-direction0)<0.04)]\n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p \n",
    "\n",
    "            ## extend end point\n",
    "            ns = tree.query([[a1,r1]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "\n",
    "            direction = np.arctan2(r[ns]-r1,a[ns]-a1)\n",
    "            ns = ns[(r[ns]-r1>0.01) &(np.fabs(direction-direction1)<0.04)] \n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p\n",
    "    #print ('\\r')\n",
    "#     df = df[['particle_id', 'weight', 'event_id', 'hit_id', 'track_id']]\n",
    "    df = df[['event_id', 'hit_id', 'track_id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackML(df, w1, w2, w3, Niter):\n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    rt  = np.sqrt(x**2+y**2)\n",
    "    r  = np.sqrt(x**2+y**2+z**2)\n",
    "    a0  = np.arctan2(y,x)\n",
    "\n",
    "    \n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    \n",
    "    \n",
    "    z1 = z/rt\n",
    "    z2 = z/r\n",
    "    \n",
    "    theta = np.arctan2(rt, z)\n",
    "    tt = np.tan(theta)\n",
    "    \n",
    "    mm = 1\n",
    "    ls = []\n",
    "    print(Niter)\n",
    "    Niter = Niter.astype(np.int32)\n",
    "    print(Niter)\n",
    "    print(w1, w2, w3, Niter)\n",
    "    for ii in range(Niter):\n",
    "        mm = mm * (-1)\n",
    "#         print(np.pi)\n",
    "        a1 = a0+mm*(rt+0.000005*rt**2)/1000*(ii/2)/180*np.pi\n",
    "#         print(a0, a1)\n",
    "        saa = np.sin(a1)\n",
    "        caa = np.cos(a1)\n",
    "        \n",
    "        deps = 0.0000025\n",
    "        X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2]))\n",
    "#         print(X.shape)\n",
    "        \n",
    "        cx = [w1,w1,w2,w3]\n",
    "        for jj in range(1,X.shape[1]): \n",
    "             X[:,jj] = X[:,jj]*cx[jj]\n",
    "        l= DBSCAN(eps=0.0035,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "        ls.append(l)\n",
    "        \n",
    "    num_hits=len(df)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackML2(df, w1, w2, w3, Niter):\n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    rt  = np.sqrt(x**2+y**2)\n",
    "    r  = np.sqrt(x**2+y**2+z**2)\n",
    "    a0  = np.arctan2(y,x)\n",
    "\n",
    "    \n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    \n",
    "    \n",
    "    z1 = z/rt\n",
    "    z2 = z/r\n",
    "    \n",
    "    theta = np.arctan2(rt, z)\n",
    "    tt = np.tan(theta)\n",
    "    \n",
    "    mm = 1\n",
    "    ls = []\n",
    "    \n",
    "    for ii in range(Niter):\n",
    "        mm = mm * (-1)\n",
    "#         print(np.pi)\n",
    "        a1 = a0+mm*(rt+0.000005*rt**2)/1000*(ii/2)/180*np.pi\n",
    "#         print(a0, a1)\n",
    "        saa = np.sin(a1)\n",
    "        caa = np.cos(a1)\n",
    "        \n",
    "        deps = 0.0000025\n",
    "        X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2]))\n",
    "#         print(X.shape)\n",
    "        \n",
    "        cx = [w1,w1,w2,w3]\n",
    "        for jj in range(1,X.shape[1]): \n",
    "             X[:,jj] = X[:,jj]*cx[jj]\n",
    "        l= DBSCAN(eps=0.0035,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "        ls.append(l)\n",
    "        \n",
    "    num_hits=len(df)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## \n",
    "### working function ###\n",
    "# ########################\n",
    "# trackML <- function(dfh,w1,w2,w3,Niter){\n",
    "#   dfh[,s1:=hit_id]\n",
    "#   dfh[,N1:=1L] \n",
    "#   dfh[,r:=sqrt(x*x+y*y+z*z)]\n",
    "#   dfh[,rt:=sqrt(x*x+y*y)]\n",
    "#   dfh[,a0:=atan2(y,x)]\n",
    "#   dfh[,z1:=z/rt]\n",
    "#   dfh[,z2:=z/r]\n",
    "#   mm     <-  1\n",
    "#   for (ii in 0:Niter) {\n",
    "#     mm <- mm*(-1)\n",
    "#     dfh[,a1:=a0+mm*(rt+0.000005*rt^2)/1000*(ii/2)/180*pi]\n",
    "#     dfh[,sina1:=sin(a1)]\n",
    "#     dfh[,cosa1:=cos(a1)]\n",
    "#     dfs=scale(dfh[,.(sina1,cosa1,z1,z2)])\n",
    "# \tcx <- c(w1,w1,w2,w3)\n",
    "#     for (jj in 1:ncol(dfs)) dfs[,jj] <- dfs[,jj]*cx[jj]\n",
    "#     res=dbscan(dfs,eps=0.0035,minPts = 1)\n",
    "#     dfh[,s2:=res$cluster]\n",
    "#     dfh[,N2:=.N, by=s2]\n",
    "#     maxs1 <- max(dfh$s1)\n",
    "#     dfh[,s1:=ifelse(N2>N1 & N2<20,s2+maxs1,s1)]\n",
    "#     dfh[,s1:=as.integer(as.factor(s1))]\n",
    "#     dfh[,N1:=.N, by=s1]    \n",
    "#   }\n",
    "#   return(dfh$s1)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fun4BO(w1,w2,w3,Niter):\n",
    "    track_id = trackML(hits, w1,w2,w3,Niter)\n",
    "    \n",
    "    sum_score=0\n",
    "    sum = 0\n",
    "    submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "        data=np.column_stack(([int(event_id),]*len(hits), hits.hit_id.values, track_id))\n",
    "    ).astype(int)\n",
    "    for i in range(2):\n",
    "        submission = extend(submission,hits)\n",
    "        score = score_event(truth, submission)\n",
    "        print('[%2d] score : %0.8f'%(i, score))\n",
    "        sum_score += score\n",
    "        sum += 1\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    sc = sum_score/sum\n",
    "    print(sc)\n",
    "    \n",
    "    return list({'Score':sc, 'Pred':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# function for Bayessian Optimization #\n",
    "#   (needs lists: Score and Pred)     #\n",
    "#######################################\n",
    "# Fun4BO <- function(w1,w2,w3,Niter) { \n",
    "#    dfh$s1 <- trackML(dfh,w1,w2,w3,Niter)\n",
    "#    sub=data.table(event_id=nev,hit_id=dfh$hit_id,track_id=dfh$s1)\n",
    "#    sc <- score(sub,dft)\n",
    "#    list(Score=sc,Pred=0)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     Niter |        w1 |        w2 |        w3 | \n",
      "174.91171047135347\n",
      "174\n",
      "1.0307915160224026 0.6665432240143782 0.23273448176797165 174\n",
      " 179.0000000[ 0] score : 0.53242166\n",
      " 179.0000000[ 1] score : 0.53706344\n",
      "--------------------------------------\n",
      "0.5347425490280032\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-c725d85973b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFun4BO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'w1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Niter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNiter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ucb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.576\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmlf/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmlf/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmlf/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmlf/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmlf/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36madd_observation\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Insert data into preallocated arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Xarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Yarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Expand views to encompass the new data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# def run_dbscan():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "for i,event_id in enumerate(event_ids):\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "    cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "    truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    \n",
    "    truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "    \n",
    "    w1 = [0.9, 1.2]\n",
    "    w2 = [0.3, 0.7]\n",
    "    w3 = [0.1, 0.4]\n",
    "    Niter = [140, 190]\n",
    "    \n",
    "    \n",
    "    bo = BayesianOptimization(Fun4BO,pbounds = {'w1':w1,'w2':w2,'w3':w3,'Niter':Niter})\n",
    "    bo.maximize(init_points = 3, n_iter = 20, acq = \"ucb\", kappa = 2.576, eps = 0.0,verbose = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1.0307915160224026\n",
    "w2 = 0.6665432240143782\n",
    "w3 = 0.23273448176797165\n",
    "Niter = 174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_make_submission():\n",
    "\n",
    "    data_dir = '../data/test'\n",
    "\n",
    "\n",
    "    tic = t = time.time()\n",
    "    event_ids = [ '%09d'%i for i in range(0,125) ]  #(0,125)\n",
    "\n",
    "    if 1:\n",
    "        submissions = []\n",
    "        for i,event_id in enumerate(event_ids):\n",
    "            hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "            cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "\n",
    "            track_id = trackML2(hits, w1, w2, w3, Niter)\n",
    "            #track_id = back_fit(track_id,hits)\n",
    "\n",
    "            toc =  time.time()\n",
    "            print('\\revent_id : %s  , %0.0f min'%(event_id, (toc-tic)/60))\n",
    "\n",
    "            # Prepare submission for an event\n",
    "            submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "                data=np.column_stack(([event_id,]*len(hits), hits.hit_id.values, track_id))\n",
    "            ).astype(int)\n",
    "            submissions.append(submission)\n",
    "            \n",
    "            for i in range(8):\n",
    "                submission = extend(submission,hits)\n",
    "            \n",
    "            \n",
    "            submission.to_csv('../cache/sub2/%s.csv.gz'%event_id,\n",
    "                                index=False, compression='gzip')\n",
    "\n",
    "            #------------------------------------------------------\n",
    "    if 1:\n",
    "\n",
    "        event_ids = [ '%09d'%i for i in range(0,125) ]  #(0,125)\n",
    "        submissions = []\n",
    "        for i,event_id in enumerate(event_ids):\n",
    "            submission  = pd.read_csv('../cache/sub2/%s.csv.gz'%event_id, compression='gzip')\n",
    "            submissions.append(submission)\n",
    "\n",
    "        \n",
    "        # Create submission file\n",
    "        submission = pd.concat(submissions, axis=0)\n",
    "        submission.to_csv('../submissions/sub2/submission_dbscan2.csv.gz',\n",
    "                            index=False, compression='gzip')\n",
    "        print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id : 000000000  , 3 min\n",
      "event_id : 000000001  , 8 min\n",
      "event_id : 000000002  , 13 min\n",
      "event_id : 000000003  , 18 min\n",
      "event_id : 000000004  , 23 min\n",
      "event_id : 000000005  , 28 min\n",
      "event_id : 000000006  , 33 min\n",
      "event_id : 000000007  , 38 min\n",
      "event_id : 000000008  , 44 min\n",
      "event_id : 000000009  , 49 min\n",
      "event_id : 000000010  , 54 min\n",
      "event_id : 000000011  , 59 min\n",
      "event_id : 000000012  , 64 min\n",
      "event_id : 000000013  , 69 min\n",
      "event_id : 000000014  , 75 min\n",
      "event_id : 000000015  , 79 min\n",
      "event_id : 000000016  , 83 min\n",
      "event_id : 000000017  , 89 min\n",
      "event_id : 000000018  , 94 min\n",
      "event_id : 000000019  , 99 min\n",
      "event_id : 000000020  , 103 min\n",
      "event_id : 000000021  , 108 min\n",
      "event_id : 000000022  , 113 min\n",
      "event_id : 000000023  , 119 min\n",
      "event_id : 000000024  , 124 min\n",
      "event_id : 000000025  , 130 min\n",
      "event_id : 000000026  , 134 min\n",
      "event_id : 000000027  , 139 min\n",
      "event_id : 000000028  , 144 min\n",
      "event_id : 000000029  , 148 min\n",
      "event_id : 000000030  , 153 min\n",
      "event_id : 000000031  , 157 min\n",
      "event_id : 000000032  , 162 min\n",
      "event_id : 000000033  , 166 min\n",
      "event_id : 000000034  , 170 min\n",
      "event_id : 000000035  , 176 min\n",
      "event_id : 000000036  , 180 min\n",
      "event_id : 000000037  , 186 min\n",
      "event_id : 000000038  , 191 min\n",
      "event_id : 000000039  , 196 min\n",
      "event_id : 000000040  , 201 min\n",
      "event_id : 000000041  , 207 min\n",
      "event_id : 000000042  , 212 min\n",
      "event_id : 000000043  , 216 min\n",
      "event_id : 000000044  , 222 min\n",
      "event_id : 000000045  , 228 min\n",
      "event_id : 000000046  , 232 min\n",
      "event_id : 000000047  , 236 min\n",
      "event_id : 000000048  , 241 min\n",
      "event_id : 000000049  , 246 min\n",
      "event_id : 000000050  , 251 min\n",
      "event_id : 000000051  , 256 min\n",
      "event_id : 000000052  , 261 min\n",
      "event_id : 000000053  , 266 min\n",
      "event_id : 000000054  , 272 min\n",
      "event_id : 000000055  , 276 min\n",
      "event_id : 000000056  , 281 min\n",
      "event_id : 000000057  , 287 min\n",
      "event_id : 000000058  , 292 min\n",
      "event_id : 000000059  , 297 min\n",
      "event_id : 000000060  , 302 min\n",
      "event_id : 000000061  , 307 min\n",
      "event_id : 000000062  , 312 min\n",
      "event_id : 000000063  , 317 min\n",
      "event_id : 000000064  , 322 min\n",
      "event_id : 000000065  , 327 min\n",
      "event_id : 000000066  , 331 min\n",
      "event_id : 000000067  , 337 min\n",
      "event_id : 000000068  , 342 min\n",
      "event_id : 000000069  , 347 min\n",
      "event_id : 000000070  , 351 min\n",
      "event_id : 000000071  , 356 min\n",
      "event_id : 000000072  , 361 min\n",
      "event_id : 000000073  , 366 min\n",
      "event_id : 000000074  , 372 min\n",
      "event_id : 000000075  , 377 min\n",
      "event_id : 000000076  , 381 min\n",
      "event_id : 000000077  , 386 min\n",
      "event_id : 000000078  , 391 min\n",
      "event_id : 000000079  , 395 min\n",
      "event_id : 000000080  , 401 min\n",
      "event_id : 000000081  , 406 min\n",
      "event_id : 000000082  , 411 min\n",
      "event_id : 000000083  , 416 min\n",
      "event_id : 000000084  , 421 min\n",
      "event_id : 000000085  , 426 min\n",
      "event_id : 000000086  , 431 min\n",
      "event_id : 000000087  , 437 min\n",
      "event_id : 000000088  , 442 min\n",
      "event_id : 000000089  , 447 min\n",
      "event_id : 000000090  , 453 min\n",
      "event_id : 000000091  , 457 min\n",
      "event_id : 000000092  , 462 min\n",
      "event_id : 000000093  , 467 min\n",
      "event_id : 000000094  , 472 min\n",
      "event_id : 000000095  , 477 min\n",
      "event_id : 000000096  , 482 min\n",
      "event_id : 000000097  , 487 min\n",
      "event_id : 000000098  , 492 min\n",
      "event_id : 000000099  , 498 min\n",
      "event_id : 000000100  , 503 min\n",
      "event_id : 000000101  , 509 min\n",
      "event_id : 000000102  , 515 min\n",
      "event_id : 000000103  , 519 min\n",
      "event_id : 000000104  , 524 min\n",
      "event_id : 000000105  , 529 min\n",
      "event_id : 000000106  , 534 min\n",
      "event_id : 000000107  , 539 min\n",
      "event_id : 000000108  , 545 min\n",
      "event_id : 000000109  , 550 min\n",
      "event_id : 000000110  , 555 min\n",
      "event_id : 000000111  , 560 min\n",
      "event_id : 000000112  , 565 min\n",
      "event_id : 000000113  , 571 min\n",
      "event_id : 000000114  , 576 min\n",
      "event_id : 000000115  , 580 min\n",
      "event_id : 000000116  , 585 min\n",
      "event_id : 000000117  , 590 min\n",
      "event_id : 000000118  , 595 min\n",
      "event_id : 000000119  , 600 min\n",
      "event_id : 000000120  , 604 min\n",
      "event_id : 000000121  , 609 min\n",
      "event_id : 000000122  , 614 min\n",
      "event_id : 000000123  , 618 min\n",
      "event_id : 000000124  , 623 min\n",
      " 179.000000013741466\n"
     ]
    }
   ],
   "source": [
    "run_make_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmlf",
   "language": "python",
   "name": "tmlf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
