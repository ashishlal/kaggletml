{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Cluster_Ensembles as CE\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan as _hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import hdbscan\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# https://www.ellicium.com/python-multiprocessing-pool-process/\n",
    "# http://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import hdbscan as _hdbscan\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(hits):        \n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#     for i, rz_scale in enumerate(self.rz_scales):\n",
    "#         X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _eliminate_outliers(clusters,M):\n",
    "    my_labels = np.unique(clusters)\n",
    "    norms=np.zeros((len(my_labels)),np.float32)\n",
    "    indices=np.zeros((len(my_labels)),np.float32)\n",
    "    for i, cluster in tqdm(enumerate(my_labels),total=len(my_labels)):\n",
    "        if cluster == 0:\n",
    "            continue\n",
    "        index = np.argwhere(clusters==cluster)\n",
    "        index = np.reshape(index,(index.shape[0]))\n",
    "        indices[i] = len(index)\n",
    "        x = M[index]\n",
    "        norms[i] = self._test_quadric(x)\n",
    "    threshold1 = np.percentile(norms,90)*5\n",
    "    threshold2 = 25\n",
    "    threshold3 = 6\n",
    "    for i, cluster in enumerate(my_labels):\n",
    "        if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n",
    "            clusters[clusters==cluster]=0\n",
    "            \n",
    "def _test_quadric(x):\n",
    "    if x.size == 0 or len(x.shape)<2:\n",
    "        return 0\n",
    "    Z = np.zeros((x.shape[0],10), np.float32)\n",
    "    Z[:,0] = x[:,0]**2\n",
    "    Z[:,1] = 2*x[:,0]*x[:,1]\n",
    "    Z[:,2] = 2*x[:,0]*x[:,2]\n",
    "    Z[:,3] = 2*x[:,0]\n",
    "    Z[:,4] = x[:,1]**2\n",
    "    Z[:,5] = 2*x[:,1]*x[:,2]\n",
    "    Z[:,6] = 2*x[:,1]\n",
    "    Z[:,7] = x[:,2]**2\n",
    "    Z[:,8] = 2*x[:,2]\n",
    "    Z[:,9] = 1\n",
    "    v, s, t = np.linalg.svd(Z,full_matrices=False)        \n",
    "    smallest_index = np.argmin(np.array(s))\n",
    "    T = np.array(t)\n",
    "    T = T[smallest_index,:]        \n",
    "    norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "def make_counts(labels):\n",
    "    \n",
    "    \n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def one_loop(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r, a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da \n",
    "    zr = z/r\n",
    "    \n",
    "    X = StandardScaler().fit_transform(np.column_stack([aa, aa/zr, zr, 1/zr, aa/zr + 1/zr]))\n",
    "    _,l = dbscan(X, eps=0.0035, min_samples=1,)\n",
    "\n",
    "    return l\n",
    "\n",
    "def one_loop1(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def one_loop2(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def do_dbscan_predict(df):\n",
    "    \n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    r  = np.sqrt(x**2+y**2)\n",
    "    d  = np.sqrt(x**2+y**2+z**2)\n",
    "    a  = np.arctan2(y,x)\n",
    "\n",
    "    x2 = df['x']/d\n",
    "    y2 = df['y']/d\n",
    "    z2 = df['z']/r\n",
    "\n",
    "    r2 = np.sqrt(x2**2 + y2**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    phi2 = np.arctan2(y2, x2)\n",
    "    phi2_deg = np.degrees(np.arctan2(y2, x2))\n",
    "    \n",
    "    for angle in range(-180,180,1):\n",
    "    \n",
    "        df1 = df.loc[(df.phi_deg>(angle-1.0)) & (df.phi_deg<(angle+1.0))]\n",
    "    \n",
    "        x  = df1.x.values\n",
    "        y  = df1.y.values\n",
    "        z  = df1.z.values\n",
    "        r  = np.sqrt(x**2+y**2)\n",
    "        d  = np.sqrt(x**2+y**2+z**2)\n",
    "        a  = np.arctan2(y,x)\n",
    "\n",
    "        x2 = df1['x']/d\n",
    "        y2 = df1['y']/d\n",
    "        z2 = df1['z']/r\n",
    "    \n",
    "        r2 = np.sqrt(x2**2 + y2**2)\n",
    "        theta= np.arctan2(r, z)\n",
    "        theta1 = np.arctan2(r2, z2)\n",
    "\n",
    "    \n",
    "        tan_dip = phi/theta\n",
    "        tan_dip1 = phi/z2\n",
    "        z2_1 = 1/z2\n",
    "        z2_2 = phi/z2 + 1/z2\n",
    "\n",
    "        dip_angle = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "        dip_angle1 = np.arctan2(z, (np.sqrt(x**2 +y**2)) * np.arccos(x2/np.sqrt(x**2 + y**2)))\n",
    "        scores = []\n",
    "\n",
    "        a_start,a_step,a_num = 0.00100,0.0000095,150\n",
    "    \n",
    "        params  = [(i,m, x,y,z,d,r,r2,z2, a, a_start,a_step) for i in range(a_num) for m in [-1,1]]\n",
    "\n",
    "        if 1: \n",
    "            pool = Pool(processes=1)\n",
    "            ls   = pool.map( one_loop1, params )\n",
    "\n",
    "\n",
    "        if 0:\n",
    "            ls = [ one_loop(param) for param in  params ]\n",
    "\n",
    "\n",
    "        ##------------------------------------------------\n",
    "\n",
    "        num_hits=len(df)\n",
    "        labels = np.zeros(num_hits,np.int32)\n",
    "        counts = np.zeros(num_hits,np.int32)\n",
    "        for l in ls:\n",
    "            c = make_counts(l)\n",
    "            idx = np.where((c-counts>0) & (c<20))[0]\n",
    "            labels[idx] = l[idx] + labels.max()\n",
    "            counts = make_counts(labels)\n",
    "       \n",
    "\n",
    "#     cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "#                          metric='braycurtis',cluster_selection_method='leaf',algorithm='best', \n",
    "#                          leaf_size=50)\n",
    "    \n",
    "#     X = preprocess(df)\n",
    "#     l1 = pd.Series(labels)\n",
    "#     labels = np.unique(l1)\n",
    "   \n",
    "# #   print(X.shape)\n",
    "# #   print(len(labels_org))\n",
    "# #   print(len(labels_org[labels_org ==0]))\n",
    "# #   print(len(labels_org[labels_org ==-1]))\n",
    "    \n",
    "#     n_labels = 0\n",
    "#     while n_labels < len(labels):\n",
    "#         n_labels = len(labels)\n",
    "#         max_len = np.max(l1)\n",
    "#         s = list(l1[l1 == 0].keys())\n",
    "#         X = X[s]\n",
    "#         print(X.shape)\n",
    "#         if X.shape[0] <= 1:\n",
    "#             break\n",
    "#         l = cl.fit_predict(X)+max_len\n",
    "# #         print(len(l))\n",
    "#         l1[l1 == 0] = l\n",
    "#         labels = np.unique(l1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extend(submission,hits, x_s, y_s, z_s):\n",
    "    df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     df = submission.append(hits)\n",
    "#     print(df.head())\n",
    "    x1 = df.x.values + x_s\n",
    "    y1 = df.y.values + y_s\n",
    "    z1 = df.z.values + z_s\n",
    "    r = np.sqrt( x1**2 + y1**2)\n",
    "    df = df.assign(d = np.sqrt( x1**2 + y1**2 + z1**2 ))\n",
    "    df = df.assign(r = np.sqrt( x1**2 + y1**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(z1, r))\n",
    "\n",
    "    for angle in range(-180,180,1):\n",
    "\n",
    "        print ('\\r %f'%angle, end='',flush=True)\n",
    "        #df1 = df.loc[(df.arctan2>(angle-0.5)/180*np.pi) & (df.arctan2<(angle+0.5)/180*np.pi)]\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "        df1.x = df1.x.values + x_s\n",
    "        df1.y = df1.y.values + y_s\n",
    "        df1.z = df1.z.values + z_s\n",
    "        min_num_neighbours = len(df1)\n",
    "        if min_num_neighbours<4: continue\n",
    "\n",
    "        hit_ids = df1.hit_id.values\n",
    "        x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "        r  = (x**2 + y**2)**0.5\n",
    "        r  = r/1000\n",
    "        a  = np.arctan2(y,x)\n",
    "        tree = KDTree(np.column_stack([a,r]), metric='euclidean')\n",
    "\n",
    "        track_ids = list(df1.track_id.unique())\n",
    "        num_track_ids = len(track_ids)\n",
    "        min_length=3\n",
    "\n",
    "        for i in range(num_track_ids):\n",
    "            p = track_ids[i]\n",
    "            if p==0: continue\n",
    "\n",
    "            idx = np.where(df1.track_id==p)[0]\n",
    "            if len(idx)<min_length: continue\n",
    "\n",
    "            if angle>0:\n",
    "                idx = idx[np.argsort( z[idx])]\n",
    "            else:\n",
    "                idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "\n",
    "            ## start and end points  ##\n",
    "            idx0,idx1 = idx[0],idx[-1]\n",
    "            a0 = a[idx0]\n",
    "            a1 = a[idx1]\n",
    "            r0 = r[idx0]\n",
    "            r1 = r[idx1]\n",
    "\n",
    "            da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "            dr0 = r[idx[1]] - r[idx[0]]\n",
    "            direction0 = np.arctan2(dr0,da0) \n",
    "\n",
    "            da1 = a[idx[-1]] - a[idx[-2]]\n",
    "            dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "            direction1 = np.arctan2(dr1,da1) \n",
    "\n",
    "\n",
    "            ## extend start point\n",
    "            ns = tree.query([[a0,r0]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "            direction = np.arctan2(r0-r[ns],a0-a[ns])\n",
    "            ns = ns[(r0-r[ns]>0.01) &(np.fabs(direction-direction0)<0.04)]\n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p \n",
    "\n",
    "            ## extend end point\n",
    "            ns = tree.query([[a1,r1]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "\n",
    "            direction = np.arctan2(r[ns]-r1,a[ns]-a1)\n",
    "            ns = ns[(r[ns]-r1>0.01) &(np.fabs(direction-direction1)<0.04)] \n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p\n",
    "    #print ('\\r')\n",
    "#     df = df[['particle_id', 'weight', 'event_id', 'hit_id', 'track_id']]\n",
    "    df = df[['event_id', 'hit_id', 'track_id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "def shift(l, n):\n",
    "    return l[n:] + l[:n]\n",
    "\n",
    "def trackML31(df, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19, Niter, \n",
    "              z_shift, x_shift, y_shift):\n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    \n",
    "#     dz = 0\n",
    "    x = x + x_shift\n",
    "    y = y + y_shift\n",
    "    z = z + z_shift\n",
    "    \n",
    "    rt  = np.sqrt(x**2+y**2)\n",
    "    r  = np.sqrt(x**2+y**2+z**2)\n",
    "    a0  = np.arctan2(y,x)\n",
    "\n",
    "    x2 = x/r\n",
    "    y2 = y/r\n",
    "    \n",
    "    \n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    \n",
    "    \n",
    "    z1 = z/rt\n",
    "    z2 = z/r\n",
    "    \n",
    "    z3 = np.log1p(abs(z/r))*np.sign(z)\n",
    "    \n",
    "    x1 = x/rt\n",
    "    y1 = y/rt\n",
    "    \n",
    "    y3 = np.log1p(abs(y/r))*np.sign(y)\n",
    "    \n",
    "    theta = np.arctan2(rt, z)\n",
    "    theta_deg = np.degrees(np.arctan2(rt, z))\n",
    "    tt = np.tan(theta_deg)\n",
    "    \n",
    "    z4 = np.sqrt(abs(z/rt))\n",
    "    \n",
    "    x4 = np.sqrt(abs(x/r))\n",
    "    y4 = np.sqrt(abs(y/r))\n",
    "    \n",
    "    mm = 1\n",
    "    ls = []\n",
    "    \n",
    "\n",
    "#     def f(x):\n",
    "#         return a0+mm*(rt+ 0.0000145*rt**2)/1000*(x/2)/180*np.pi\n",
    "    \n",
    "    for ii in range(Niter):\n",
    "        mm = mm * (-1)\n",
    "\n",
    "        a1 = a0+mm*(rt+ 0.0000145*rt**2)/1000*(ii/2)/180*np.pi\n",
    "    \n",
    "        da1 = mm*(1 + (2 * 0.0000145 * rt))/1000*(ii/2)/180*np.pi\n",
    "        ia1 = a0*rt + mm*(((rt**2)/2) + (0.0000145*rt**3)/3)/1000*(ii/2)/180*np.pi\n",
    "        \n",
    "        saa = np.sin(a1)\n",
    "        caa = np.cos(a1)\n",
    "        \n",
    "        raa = x*caa + y*saa\n",
    "        \n",
    "        t1 = theta+mm*(rt+ 0.8435*rt**2)/1000*(ii/2)/180*np.pi\n",
    "        ctt = np.cos(t1)\n",
    "        stt = np.sin(t1)\n",
    "        ttt = np.tan(t1)\n",
    "        \n",
    "        mom = np.sqrt(1 + (z2**2)) * np.sign(z)\n",
    "        r0Inv = 2. * np.cos(a1 - t1) / r\n",
    "        \n",
    "        r0Inv_d = -2. * np.sin(a1-t1) * da1 /r\n",
    "        \n",
    "        X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3, \n",
    "                                                            ctt, stt, z4, x4, y4, raa, r0Inv, da1, ia1,\n",
    "                                                            r0Inv_d, mom]))\n",
    "#         print(X.shape)\n",
    "        \n",
    "#         X = StandardScaler().fit_transform(np.column_stack([caa,saa,z1,z2]))\n",
    "        \n",
    "        cx = [w1,w1,w2,w3, w4, w5, w6, w7, w8, w9, w10, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19]\n",
    "\n",
    "#         cx = [w1,w1,w2,w3]\n",
    "        \n",
    "        X = np.multiply(X, cx)\n",
    "\n",
    "        l= DBSCAN(eps=0.004,min_samples=1,metric='euclidean',n_jobs=4).fit(X).labels_\n",
    "\n",
    "        ls.append(l)\n",
    "    \n",
    "        \n",
    "    num_hits=len(df)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    lss = []\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "    \n",
    "#     lss.append(labels)\n",
    "    \n",
    "#     for i in range(Niter):\n",
    "#         labels1 = np.zeros(num_hits,np.int32)\n",
    "#         counts1 = np.zeros(num_hits,np.int32)\n",
    "#         ls1 = ls.copy()\n",
    "#         ls1 = shift(ls1, 1)\n",
    "#         np.random.shuffle(ls1)\n",
    "#         for l in ls1:\n",
    "#             c = make_counts(l)\n",
    "#             idx = np.where((c-counts>0) & (c<20))[0]\n",
    "#             labels1[idx] = l[idx] + labels1.max()\n",
    "#             counts1 = make_counts(labels1)\n",
    "#         l1 = labels1.copy()\n",
    "#         lss.append(l1)\n",
    "\n",
    "\n",
    "#     labels = np.zeros(num_hits,np.int32)\n",
    "#     counts = np.zeros(num_hits,np.int32)\n",
    "    \n",
    "#     for l in lss:\n",
    "#         c = make_counts(l)\n",
    "#         idx = np.where((c-counts>0) & (c<20))[0]\n",
    "#         labels[idx] = l[idx] + labels.max()\n",
    "#         counts = make_counts(labels)\n",
    "    \n",
    "#     df = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id', 'vlm'],\n",
    "#         data=np.column_stack(([int(0),]*len(hits), hits.hit_id.values, labels, hits.vlm.values))\n",
    "#     )\n",
    "#     df = pd.DataFrame()\n",
    "#     df['hit_id']=hits.hit_id.values\n",
    "#     df['vlm'] = hits.vlm.values\n",
    "#     df['track_id'] = labels\n",
    "    \n",
    "#     for l in np.unique(labels):\n",
    "#         df_l = df[df.track_id == l]\n",
    "#         df_l['vlm_count'] =df_l.groupby('vlm')['vlm'].transform('count')\n",
    "#         same_vlm_multiple_hits = np.any(df_l.vlm_count > 1)\n",
    "#         if same_vlm_multiple_hits == True:\n",
    "#             print(l)\n",
    "#             which_vlm_multiple_hits = list(df_l[df_l.vlm_count > 1].index)  \n",
    "#             which_vlm_multiple_hits.pop(0)\n",
    "            \n",
    "#             df.loc[which_vlm_multiple_hits, 'track_id'] = 9999999999\n",
    "        \n",
    "#     return df.track_id.values\n",
    "#     sub = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "#         data=np.column_stack(([int(0),]*len(df), df.hit_id.values, labels))\n",
    "#     )\n",
    "#     sub['track_count'] = sub.groupby('track_id')['track_id'].transform('count')\n",
    "# #     sub.loc[sub.track_count < 5, 'track_id'] = 0\n",
    "    \n",
    "#     sub1 = sub[sub.track_id <  0]\n",
    "#     sub2 = sub[sub.track_id >= 0]\n",
    "#     L_neg = sub1.track_id.values\n",
    "#     L_pos = sub2.track_id.values\n",
    "#     a = 1\n",
    "#     for l in L_neg:\n",
    "#         for l1 in range(a, np.iinfo(np.int32).max):\n",
    "#             if l1 in L_pos:\n",
    "#                 continue\n",
    "#             sub.loc[sub.track_id == l, 'track_id'] = l1\n",
    "#             a = l1 +1\n",
    "#             break\n",
    "    \n",
    "#     L = list(sub.track_id.values)\n",
    "#     labels = np.zeros(num_hits,np.int32)\n",
    "#     for ii in range(num_hits):\n",
    "#         labels[ii] = L[ii]\n",
    "#     print(np.any(labels < 0))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.13 s, sys: 348 ms, total: 4.48 s\n",
      "Wall time: 923 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# def run_dbscan():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "for i,event_id in enumerate(event_ids):\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "    cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "    truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    \n",
    "    truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5e-05\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trackML31() missing 3 required positional arguments: 'z_shift', 'x_shift', and 'y_shift'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: trackML31() missing 3 required positional arguments: 'z_shift', 'x_shift', and 'y_shift'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w1 = 1.1932215111905984\n",
    "w2 = 0.39740553885387364\n",
    "w3 = 0.3512647720585538\n",
    "w4 = 0.1470\n",
    "w5 = 0.01201\n",
    "w6 = 0.0003864\n",
    "w7 = 0.0205\n",
    "w8 = 0.0049\n",
    "w9 = 0.00121\n",
    "w10 = 1.4930496676654575e-05\n",
    "w11 = 0.0318\n",
    "w12 = 0.000435\n",
    "w13 = 0.00038\n",
    "w14 = 0.00072\n",
    "w15 = 5.5e-05\n",
    "#     w15 = 0.000265\n",
    "w16 = 0.0031\n",
    "w17 = 0.00021\n",
    "w18 = 7.5e-05\n",
    "\n",
    "Niter=247    \n",
    "print(w18)\n",
    "\n",
    "z_shift = 0\n",
    "#     ls = []\n",
    "track_id = trackML31(hits, w1,w2,w3,w4,w5,w6,w7,w8,w9,w10, w11, w12, w13, w14, w15, w16, w17, w18,Niter, \n",
    "                     z_shift)\n",
    "\n",
    "sum_score=0\n",
    "sum = 0\n",
    "submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "    data=np.column_stack(([int(event_id),]*len(hits), hits.hit_id.values, track_id))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    submission = extend(submission,hits)\n",
    "    score = score_event(truth, submission)\n",
    "    print('[%2d] score : %0.8f'%(i, score))\n",
    "    sum_score += score\n",
    "    sum += 1\n",
    "\n",
    "print('--------------------------------------')\n",
    "sc = sum_score/sum\n",
    "print(sc)\n",
    "# caa, saa: 5 mins score 0\n",
    "# caa, saa, z1: 0.3942327679531816, 6 min 14s\n",
    "# z1: 5.99028402551861e-05, 11 mins\n",
    "# caa,saa,z1,z2: 7 mins, 0.5315668141457246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179.0000000[ 0] score : 0.63363358\n",
      " 179.0000000[ 1] score : 0.63765912\n",
      " 179.0000000[ 2] score : 0.63883962\n",
      " 179.0000000[ 3] score : 0.64030808\n",
      " 179.0000000[ 4] score : 0.64120567\n",
      " 179.0000000[ 5] score : 0.64168075\n",
      " 179.0000000[ 6] score : 0.64064708\n",
      " 179.0000000[ 7] score : 0.64116239\n",
      "--------------------------------------\n",
      "0.63939203643381\n"
     ]
    }
   ],
   "source": [
    "num_hits = len(hits)\n",
    "labels = np.zeros(num_hits,np.int32)\n",
    "counts = np.zeros(num_hits,np.int32)\n",
    "for i in range(len(ls)):\n",
    "    labels1 = np.zeros(num_hits,np.int32)\n",
    "    counts1 = np.zeros(num_hits,np.int32)\n",
    "    ls1 = ls.copy()\n",
    "    ls1 = shift(ls1, 1)\n",
    "    np.random.shuffle(ls1)\n",
    "    for l in ls1:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels1[idx] = l[idx] + labels1.max()\n",
    "        counts1 = make_counts(labels1)\n",
    "    l1 = labels1.copy()\n",
    "    lss.append(l1)\n",
    "\n",
    "\n",
    "labels = np.zeros(num_hits,np.int32)\n",
    "counts = np.zeros(num_hits,np.int32)\n",
    "\n",
    "for l in lss:\n",
    "    c = make_counts(l)\n",
    "    idx = np.where((c-counts>0) & (c<20))[0]\n",
    "    labels[idx] = l[idx] + labels.max()\n",
    "    counts = make_counts(labels)\n",
    "\n",
    "sum_score=0\n",
    "sum = 0\n",
    "submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "    data=np.column_stack(([int(event_id),]*len(hits), hits.hit_id.values, labels))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    submission = extend(submission,hits)\n",
    "    score = score_event(truth, submission)\n",
    "    print('[%2d] score : %0.8f'%(i, score))\n",
    "    sum_score += score\n",
    "    sum += 1\n",
    "\n",
    "print('--------------------------------------')\n",
    "sc = sum_score/sum\n",
    "print(sc)\n",
    "\n",
    "# 179.0000000[ 0] score : 0.63363358\n",
    "#  179.0000000[ 1] score : 0.63765912\n",
    "#  179.0000000[ 2] score : 0.63883962\n",
    "#  179.0000000[ 3] score : 0.64030808\n",
    "#  179.0000000[ 4] score : 0.64120567\n",
    "#  179.0000000[ 5] score : 0.64168075\n",
    "#  179.0000000[ 6] score : 0.64064708\n",
    "#  179.0000000[ 7] score : 0.64116239\n",
    "# --------------------------------------\n",
    "# 0.63939203643381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fun4BO2222(params):\n",
    "    df, w1,w2,w3,w4,w5,w6,w7,w8,w9,w10, w11, w12, w13, w14, w15, w16, w17, w18, w19, Niter, z_s, x_s, y_s = params\n",
    "    \n",
    "    l = trackML31(df, w1,w2,w3,w4,w5,w6,w7,w8,w9,w10, w11, w12, w13, w14, w15, w16, w17, w18, w19, Niter, \n",
    "                         z_s, x_s, y_s)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def Fun4BO2(df):\n",
    "    \n",
    "    w1 = 1.1932215111905984\n",
    "    w2 = 0.39740553885387364\n",
    "    w3 = 0.3512647720585538\n",
    "    w4 = 0.1470\n",
    "    w5 = 0.01201\n",
    "    w6 = 0.0003864\n",
    "    w7 = 0.0205\n",
    "    w8 = 0.0049\n",
    "    w9 = 0.00121\n",
    "    w10 = 1.4930496676654575e-05\n",
    "    w11 = 0.0318\n",
    "    w12 = 0.000435\n",
    "    w13 = 0.00038\n",
    "    w14 = 0.00072\n",
    "    w15 = 5.5e-05\n",
    "#     w15 = 0.000265\n",
    "    w16 = 0.0031\n",
    "    w17 = 0.00021\n",
    "    w18 = 7.5e-05\n",
    "    w19 = 0.001\n",
    "    \n",
    "    Niter=247    \n",
    "#     print(w18)\n",
    "   \n",
    "    ls = []\n",
    "    lss = []\n",
    "\n",
    "    z_shift = 0\n",
    "\n",
    "    z_shifts = [0]\n",
    "    z_shift_list = list(np.linspace(-5.5, 5.5, 10))\n",
    "    \n",
    "    z_shifts = z_shifts + z_shift_list\n",
    "    \n",
    "    x_shifts = list(np.linspace(-0.015, 0.015, 10))\n",
    "    \n",
    "    y_shifts = list(np.linspace(-0.015, 0.015, 10))\n",
    "    \n",
    "    params = []\n",
    "    extend_params = []\n",
    "    for z_s in z_shifts:\n",
    "        x_s = 0\n",
    "        y_s = 0\n",
    "        if z_s != 0:\n",
    "            x_s = x_shifts.pop(0)\n",
    "            y_s = y_shifts.pop(0)\n",
    "        extend_params.append((x_s,y_s,z_s))\n",
    "        params.append((df, w1,w2,w3,w4,w5,w6,w7,w8,w9,w10, w11, w12, w13, w14, w15, w16, w17, w18, w19,\n",
    "                       Niter, z_s, x_s, y_s))\n",
    "    \n",
    "    pool = Pool(processes=11, maxtasksperchild=1)\n",
    "    ls1 = pool.map(Fun4BO2222, params, chunksize=1)\n",
    "    pool.close()\n",
    "    \n",
    "    ls = ls + ls1\n",
    "    num_hits = len(df)\n",
    "#     lss = []\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    for i in range(len(ls)):\n",
    "        labels1 = np.zeros(num_hits,np.int32)\n",
    "        counts1 = np.zeros(num_hits,np.int32)\n",
    "        ls1 = ls.copy()\n",
    "        ls1 = shift(ls1, 1)\n",
    "        np.random.shuffle(ls1)\n",
    "        for l in ls1:\n",
    "            c = make_counts(l)\n",
    "            idx = np.where((c-counts>0) & (c<20))[0]\n",
    "            labels1[idx] = l[idx] + labels1.max()\n",
    "            counts1 = make_counts(labels1)\n",
    "        l1 = labels1.copy()\n",
    "        lss.append(l1)\n",
    "\n",
    "\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    \n",
    "    for l in lss:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "        \n",
    "    sum_score=0\n",
    "    sum = 0\n",
    "    submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "        data=np.column_stack(([int(0),]*len(df), df.hit_id.values, labels))\n",
    "    ).astype(int)\n",
    "    \n",
    "    \n",
    "    for i in range(8):\n",
    "        x_s,y_s,z_s = extend_params.pop(0)\n",
    "        submission = extend(submission,df,x_s,y_s,z_s)\n",
    "        score = score_event(truth, submission)\n",
    "        print('[%2d] score : %0.8f'%(i, score))\n",
    "        sum_score += score\n",
    "        sum += 1\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    sc = sum_score/sum\n",
    "    print(sc)\n",
    "    \n",
    "#     return sc\n",
    "#     return labels\n",
    "    return submission.track_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179.0000000[ 0] score : 0.63250940\n",
      " 179.0000000[ 1] score : 0.63143279\n",
      " 179.0000000[ 2] score : 0.63223686\n",
      " 179.0000000[ 3] score : 0.63168426\n",
      " 179.0000000[ 4] score : 0.63430704\n",
      " 179.0000000[ 5] score : 0.63529427\n",
      " 179.0000000[ 6] score : 0.63755903\n",
      " 179.0000000[ 7] score : 0.63711653\n",
      "--------------------------------------\n",
      "0.634017521172\n",
      "CPU times: user 7min 47s, sys: 6 s, total: 7min 53s\n",
      "Wall time: 32min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# def run_dbscan():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "for i,event_id in enumerate(event_ids):\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "    cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "    truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    \n",
    "    truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "    \n",
    "#     bo = BayesianOptimization(Fun4BO,pbounds = {'w1':w1,'w2':w2,'w3':w3,'Niter':Niter})\n",
    "#     bo.maximize(init_points = 3, n_iter = 20, acq = \"ucb\", kappa = 2.576)\n",
    "#     w1 = 1.1932215111905984\n",
    "#     w2 = 0.39740553885387364\n",
    "#     w3 = 0.3512647720585538\n",
    "#     w4 = [0.1, 0.2] # 0.1470 -> 0.55690\n",
    "#     w4 = 0.1470\n",
    "#     w5 = [0.001, 1.2] # 0.7781 -> 0.55646, 0.7235 + N = 247 => 0.56025\n",
    "    \n",
    "#     Niter = 179\n",
    "#     Niter = 247\n",
    "#     w5 = 0.01\n",
    "#     for w6 in [0.012, 0.01201, 0.01202, 0.01203, 0.01204, 0.01205, 0.01206, 0.01207, 0.01208, 0.01209, 0.0121]:\n",
    "#     EPS = 1e-12\n",
    "#     w6 = [0.001, 1.2]\n",
    "#     w6 = 0.0205\n",
    "#     w18 = [0.00001, 0.05]\n",
    "#     w13 = 0.00038\n",
    "#     w14 = 0.0007133505234834969\n",
    "#     for w8 in np.arange(0.00008, 0.00015, 0.000005):\n",
    "#         print(w8)\n",
    "#     Fun4BO2(1)\n",
    "#     for w18 in np.arange(1.0e-05, 9.0e-05, 5.0e-06):\n",
    "#         print(w18)\n",
    "    labels = Fun4BO2(hits)\n",
    "#     Niter = [240, 480]\n",
    "#     w18 = [0.00001, 0.0003]\n",
    "#     bo = BayesianOptimization(Fun4BO2,pbounds = {'w18':w18})\n",
    "#     bo.maximize(init_points = 20, n_iter = 5, acq = \"ucb\", kappa = 2.576)\n",
    "\n",
    "# x/y: 7 | 06m30s |    0.55302 |    0.0100 | \n",
    "# x/y: 0.001: 0.55949\n",
    "# x/y: 0.0001: 0.55949\n",
    "# x/y: 0.002: 0.55959\n",
    "# x/y: 0.003: 0.55915\n",
    "# x/y: 0.0025: 0.55925\n",
    "# x/y: 0.0015: 0.55953\n",
    "# x/r: 0.0015: 0.56186\n",
    "# x/r: 0.002: 0.56334\n",
    "# x/r: 0.0025: 0.563989\n",
    "# x/r: 0.003: 0.56447\n",
    "# x/r: 0.01: 0.569822\n",
    "# x/r: 0.015: 0.56940\n",
    "# x/r: 0.012: 0.5719\n",
    "# x/r: 0.01201: 0.57192\n",
    "# 1.4499999999999993e-05 * rt**2: 0.5720702851970194\n",
    "# 0.0000145\n",
    "# z3: 10 | 07m12s |    0.57208 |    0.0205 | \n",
    "# count: 19: 0.572567, 17: 0.57263\n",
    "# ctt, stt after change:  2 | 07m56s |    0.57345 |    0.0001 | (0.00010567777727496665)\n",
    "# x4: 25 | 09m42s |    0.57359 |    0.0002 | (0.000206214286412982)\n",
    "# x4: 0.000435 (0.5737387485278771) (x4 = np.sqrt(abs(x/r)))\n",
    "# w13: 00038 (ctt,stt): 0.5737528800479372\n",
    "# ensemble of 10: 0.5772859116242378\n",
    "# ensemble of Niter=247 (random shuffle+ shift): 0.5787580886742594\n",
    "# ensemble of Niter=247 (shift only): 0.5743461440542145\n",
    "# ensemble of Niter=247 (random shuffle+ shift+ eps=0.004+vlm): 0.5865991424251623\n",
    "# 14 + ensemble: (0.0007133505234834969) 0.58787\n",
    "# w14 + ensemble: 1 | 30m13s |    0.58787 |    0.0007 | (0.0007133505234834969)\n",
    "# w14: 0.00027 (0.5873896523922799)\n",
    "# test w14, raa = x*caa + y*saa(0.00072: 0.5878990304956998)\n",
    "# test w16: r0Inv1 (21 | 21m40s |    0.58735 |    0.0000 | (1.0002801729384074e-05))\n",
    "# test w16: r0Inv1 (5.5e-06: 0.5881860039044223)\n",
    "# test r0Inv1 (5.5e-06, Niter=246, 0.5867403075395137)\n",
    "# test r0Inv1 (5.5e-06, Niter=247, 0.5872846547180826)\n",
    "# Niter = 247 (0.5880986018552999):\n",
    "# X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3, \n",
    "#                                                             ctt, stt, z4, x4, y4, raa, r0Inv]))\n",
    "#         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w8, w9, w10, w10, w11, w12, w13, w14, w15]\n",
    "# w15: 5.0611615056082495e-05 (17 | 21m37s |    0.58790 |    0.0001 | )\n",
    "# w15 test (5.5e-05: 0.5881768870518835)\n",
    "# w15 alone: 5.5e-05: 0.5870504337495849\n",
    "# w15 again: 5.5e-05 0.5864220587506578 (strange)\n",
    "# w15 again: 5.5e-05 (0.5880689577051738)\n",
    "# w16: 5.5e-06: 0.587602145623185 (bad since w16 was not being used)\n",
    "# after reset: w16 not being used - 0.5880689577051738\n",
    "# a2: 0.0206: 0.58157\n",
    "# org (no shift + ensemble): 0.5859135274547416\n",
    "# org (with shift + ensemble + ia1): 0.5901965251117371\n",
    "# org (with shift + ensemble + no ia1): 0.5901656684266057\n",
    "# r0Inv_d1: 7.401866174854672e-05, 0.58592 (7.5e-05: 0.5892)\n",
    "# multiprocessing - 0.6377253549867099\n",
    "# with x_shift, y_shift: 0.6439847670333751 (r, 10z, 5x, 5 y_shifts) (12 hr, 10 processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_runs = np.random.randint(0, 50, (50, 15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: Cluster_Ensembles: cluster_ensembles: due to a rather large number of cells in your data-set, using only 'HyperGraph Partitioning Algorithm' (HGPA) and 'Meta-CLustering Algorithm' (MCLA) as ensemble consensus functions.\n",
      "\n",
      "\n",
      "*****\n",
      "INFO: Cluster_Ensembles: HGPA: consensus clustering using HGPA.\n",
      "\n",
      "#\n",
      "INFO: Cluster_Ensembles: wgraph: writing wgraph_HGPA.\n",
      "INFO: Cluster_Ensembles: wgraph: 15000 vertices and 2500 non-zero hyper-edges.\n",
      "#\n",
      "\n",
      "#\n",
      "INFO: Cluster_Ensembles: sgraph: calling shmetis for hypergraph partitioning.\n",
      "INFO: Cluster_Ensembles: sgraph: (hyper)-graph partitioning completed; loading wgraph_HGPA.part.50\n",
      "#\n",
      "\n",
      "INFO: Cluster_Ensembles: cluster_ensembles: HGPA at 0.02434736018512536.\n",
      "*****\n",
      "\n",
      "*****\n",
      "INFO: Cluster_Ensembles: MCLA: consensus clustering using MCLA.\n",
      "INFO: Cluster_Ensembles: MCLA: preparing graph for meta-clustering.\n",
      "INFO: Cluster_Ensembles: MCLA: done filling hypergraph adjacency matrix. Starting computation of Jaccard similarity matrix.\n",
      "INFO: Cluster_Ensembles: MCLA: starting computation of Jaccard similarity matrix.\n",
      "INFO: Cluster_Ensembles: MCLA: done computing the matrix of pairwise Jaccard similarity scores.\n",
      "\n",
      "#\n",
      "INFO: Cluster_Ensembles: wgraph: writing wgraph_MCLA.\n",
      "#\n",
      "\n",
      "#\n",
      "INFO: Cluster_Ensembles: sgraph: calling gpmetis for graph partitioning.\n",
      "INFO: Cluster_Ensembles: sgraph: (hyper)-graph partitioning completed; loading wgraph_MCLA.part.50\n",
      "#\n",
      "INFO: Cluster_Ensembles: MCLA: delivering 50 clusters.\n",
      "INFO: Cluster_Ensembles: MCLA: average posterior probability is 0.0033593082240468202\n",
      "\n",
      "INFO: Cluster_Ensembles: cluster_ensembles: MCLA at 0.036501449656072035.\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "consensus_clustering_labels = CE.cluster_ensembles(cluster_runs, verbose = True, N_clusters_max = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus_clustering_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = 1.1932215111905984\n",
    "w2 = 0.39740553885387364\n",
    "w3 = 0.3512647720585538\n",
    "w4 = 0.1470\n",
    "w5 = 0.01201\n",
    "w6 = 0.0003864\n",
    "w7 = 0.0205\n",
    "w8 = 0.0049\n",
    "w9 = 0.00121\n",
    "w10 = 1.4930496676654575e-05\n",
    "w11 = 0.0318\n",
    "w12 = 0.000435\n",
    "w13 = 0.00038\n",
    "w14 = 0.00072\n",
    "w15 = 5.5e-05\n",
    "#     w15 = 0.000265\n",
    "w16 = 0.0031\n",
    "w17 = 0.00021\n",
    "w18 = 7.5e-05\n",
    "w19 = 0.001\n",
    "\n",
    "Niter=247    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_make_submission():\n",
    "\n",
    "    data_dir = '../data/test'\n",
    "\n",
    "\n",
    "    tic = t = time.time()\n",
    "    L = range(125)\n",
    "    L = L[::-1]\n",
    "    event_ids = [ '%09d'%i for i in L ]  #(0,125)\n",
    "\n",
    "    if 1:\n",
    "        submissions = []\n",
    "        for i,event_id in enumerate(event_ids):\n",
    "            hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "            cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "            \n",
    "            labels = Fun4BO2(hits)\n",
    "\n",
    "            toc =  time.time()\n",
    "            print('\\revent_id : %s  , %0.0f min'%(event_id, (toc-tic)/60))\n",
    "\n",
    "            # Prepare submission for an event\n",
    "            submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "                data=np.column_stack(([event_id,]*len(hits), hits.hit_id.values, labels))\n",
    "            ).astype(int)\n",
    "            submissions.append(submission)\n",
    "            \n",
    "#             for i in range(8):\n",
    "#                 submission = extend(submission,hits)\n",
    "            \n",
    "            \n",
    "            submission.to_csv('../cache/sub3/%s.csv.gz'%event_id,\n",
    "                                index=False, compression='gzip')\n",
    "\n",
    "            #------------------------------------------------------\n",
    "    if 1:\n",
    "\n",
    "        event_ids = [ '%09d'%i for i in range(0,125) ]  #(0,125)\n",
    "        submissions = []\n",
    "        for i,event_id in enumerate(event_ids):\n",
    "            submission  = pd.read_csv('../cache/sub3/%s.csv.gz'%event_id, compression='gzip')\n",
    "            submissions.append(submission)\n",
    "\n",
    "        \n",
    "        # Create submission file\n",
    "        submission = pd.concat(submissions, axis=0)\n",
    "        submission.to_csv('../submissions/sub3/submission-0030.csv.gz',\n",
    "                            index=False, compression='gzip')\n",
    "        print(len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id : 000000124  , 43 min\n",
      "event_id : 000000123  , 101 min\n",
      "event_id : 000000122  , 150 min\n",
      "event_id : 000000121  , 215 min\n",
      "event_id : 000000120  , 272 min\n",
      "event_id : 000000119  , 307 min\n",
      "event_id : 000000118  , 342 min\n",
      "event_id : 000000117  , 389 min\n",
      "event_id : 000000116  , 454 min\n",
      "event_id : 000000115  , 507 min\n",
      "event_id : 000000114  , 560 min\n",
      "event_id : 000000113  , 622 min\n",
      "event_id : 000000112  , 665 min\n",
      "event_id : 000000111  , 697 min\n",
      "event_id : 000000110  , 727 min\n",
      "event_id : 000000109  , 756 min\n",
      "event_id : 000000108  , 786 min\n",
      "event_id : 000000107  , 818 min\n"
     ]
    }
   ],
   "source": [
    "run_make_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmlkt",
   "language": "python",
   "name": "tmlkt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
