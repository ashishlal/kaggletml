{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require(data.table)\n",
    "# require(bit64)\n",
    "# require(dbscan)\n",
    "# require(doParallel)\n",
    "# require(rBayesianOptimization)\n",
    "# path='../input/train_1/'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan as _hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import hdbscan\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# https://www.ellicium.com/python-multiprocessing-pool-process/\n",
    "# http://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import hdbscan as _hdbscan\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(hits):        \n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#     for i, rz_scale in enumerate(self.rz_scales):\n",
    "#         X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "def make_counts(labels):\n",
    "    \n",
    "    \n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def one_loop(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r, a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da \n",
    "    zr = z/r\n",
    "    \n",
    "    X = StandardScaler().fit_transform(np.column_stack([aa, aa/zr, zr, 1/zr, aa/zr + 1/zr]))\n",
    "    _,l = dbscan(X, eps=0.0035, min_samples=1,)\n",
    "\n",
    "    return l\n",
    "\n",
    "def one_loop1(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def one_loop2(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def do_dbscan_predict(df):\n",
    "    \n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    r  = np.sqrt(x**2+y**2)\n",
    "    d  = np.sqrt(x**2+y**2+z**2)\n",
    "    a  = np.arctan2(y,x)\n",
    "\n",
    "    x2 = df['x']/d\n",
    "    y2 = df['y']/d\n",
    "    z2 = df['z']/r\n",
    "\n",
    "    r2 = np.sqrt(x2**2 + y2**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    phi2 = np.arctan2(y2, x2)\n",
    "    phi2_deg = np.degrees(np.arctan2(y2, x2))\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    a_start,a_step,a_num = 0.00100,0.0000095,150\n",
    "\n",
    "    params  = [(i,m, x,y,z,d,r,r2,z2, a, a_start,a_step) for i in range(a_num) for m in [-1,1]]\n",
    "\n",
    "    if 1: \n",
    "        pool = Pool(processes=1)\n",
    "        ls   = pool.map( one_loop1, params )\n",
    "\n",
    "\n",
    "    if 0:\n",
    "        ls = [ one_loop(param) for param in  params ]\n",
    "\n",
    "\n",
    "    ##------------------------------------------------\n",
    "\n",
    "    num_hits=len(df)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "       \n",
    "\n",
    "#     cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "#                          metric='braycurtis',cluster_selection_method='leaf',algorithm='best', \n",
    "#                          leaf_size=50)\n",
    "    \n",
    "#     X = preprocess(df)\n",
    "#     l1 = pd.Series(labels)\n",
    "#     labels = np.unique(l1)\n",
    "   \n",
    "# #   print(X.shape)\n",
    "# #   print(len(labels_org))\n",
    "# #   print(len(labels_org[labels_org ==0]))\n",
    "# #   print(len(labels_org[labels_org ==-1]))\n",
    "    \n",
    "#     n_labels = 0\n",
    "#     while n_labels < len(labels):\n",
    "#         n_labels = len(labels)\n",
    "#         max_len = np.max(l1)\n",
    "#         s = list(l1[l1 == 0].keys())\n",
    "#         X = X[s]\n",
    "#         print(X.shape)\n",
    "#         if X.shape[0] <= 1:\n",
    "#             break\n",
    "#         l = cl.fit_predict(X)+max_len\n",
    "# #         print(len(l))\n",
    "#         l1[l1 == 0] = l\n",
    "#         labels = np.unique(l1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "var1 = 0\n",
    "def extend(submission,hits):\n",
    "    df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     if var1 != 0:\n",
    "#         df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     else:\n",
    "#         df = hits\n",
    "#     df = submission.append(hits)\n",
    "#     print(df.head())\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "    for angle in range(-180,180,1):\n",
    "\n",
    "        print ('\\r %f'%angle, end='',flush=True)\n",
    "        #df1 = df.loc[(df.arctan2>(angle-0.5)/180*np.pi) & (df.arctan2<(angle+0.5)/180*np.pi)]\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "#         if len(df1) == 0:\n",
    "#             continue\n",
    "        min_num_neighbours = len(df1)\n",
    "        if min_num_neighbours<4: continue\n",
    "\n",
    "        hit_ids = df1.hit_id.values\n",
    "        x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "        r  = (x**2 + y**2)**0.5\n",
    "        r  = r/1000\n",
    "        a  = np.arctan2(y,x)\n",
    "        tree = KDTree(np.column_stack([a,r]), metric='euclidean')\n",
    "\n",
    "#         print(df1.head())\n",
    "        track_ids = list(df1.track_id.unique())\n",
    "        num_track_ids = len(track_ids)\n",
    "        min_length=3\n",
    "\n",
    "        for i in range(num_track_ids):\n",
    "            p = track_ids[i]\n",
    "            if p==0: continue\n",
    "\n",
    "            idx = np.where(df1.track_id==p)[0]\n",
    "            if len(idx)<min_length: continue\n",
    "\n",
    "            if angle>0:\n",
    "                idx = idx[np.argsort( z[idx])]\n",
    "            else:\n",
    "                idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "\n",
    "            ## start and end points  ##\n",
    "            idx0,idx1 = idx[0],idx[-1]\n",
    "            a0 = a[idx0]\n",
    "            a1 = a[idx1]\n",
    "            r0 = r[idx0]\n",
    "            r1 = r[idx1]\n",
    "\n",
    "            da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "            dr0 = r[idx[1]] - r[idx[0]]\n",
    "            direction0 = np.arctan2(dr0,da0) \n",
    "\n",
    "            da1 = a[idx[-1]] - a[idx[-2]]\n",
    "            dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "            direction1 = np.arctan2(dr1,da1) \n",
    "\n",
    "\n",
    "            ## extend start point\n",
    "            ns = tree.query([[a0,r0]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "            direction = np.arctan2(r0-r[ns],a0-a[ns])\n",
    "            ns = ns[(r0-r[ns]>0.01) &(np.fabs(direction-direction0)<0.04)]\n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p \n",
    "\n",
    "            ## extend end point\n",
    "            ns = tree.query([[a1,r1]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "\n",
    "            direction = np.arctan2(r[ns]-r1,a[ns]-a1)\n",
    "            ns = ns[(r[ns]-r1>0.01) &(np.fabs(direction-direction1)<0.04)] \n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p\n",
    "    #print ('\\r')\n",
    "#     df = df[['particle_id', 'weight', 'event_id', 'hit_id', 'track_id']]\n",
    "    df = df[['event_id', 'hit_id', 'track_id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rz_scales=[0.65, 0.965, 1.528]\n",
    "# def _eliminate_outliers(labels,M):\n",
    "#         norms=np.zeros((len(labels)),np.float32)\n",
    "#         indices=np.zeros((len(labels)),np.float32)\n",
    "#         for i, cluster in tqdm(enumerate(labels),total=len(labels)):\n",
    "#             if cluster == 0:\n",
    "#                 continue\n",
    "#             index = np.argwhere(self.clusters==cluster)\n",
    "#             index = np.reshape(index,(index.shape[0]))\n",
    "#             indices[i] = len(index)\n",
    "#             x = M[index]\n",
    "#             norms[i] = self._test_quadric(x)\n",
    "#         threshold1 = np.percentile(norms,90)*5\n",
    "#         threshold2 = 25\n",
    "#         threshold3 = 6\n",
    "#         for i, cluster in enumerate(labels):\n",
    "#             if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n",
    "#                 self.clusters[self.clusters==cluster]=0\n",
    "\n",
    "# def _test_quadric(x):\n",
    "#     if x.size == 0 or len(x.shape)<2:\n",
    "#         return 0\n",
    "#     Z = np.zeros((x.shape[0],10), np.float32)\n",
    "#     Z[:,0] = x[:,0]**2\n",
    "#     Z[:,1] = 2*x[:,0]*x[:,1]\n",
    "#     Z[:,2] = 2*x[:,0]*x[:,2]\n",
    "#     Z[:,3] = 2*x[:,0]\n",
    "#     Z[:,4] = x[:,1]**2\n",
    "#     Z[:,5] = 2*x[:,1]*x[:,2]\n",
    "#     Z[:,6] = 2*x[:,1]\n",
    "#     Z[:,7] = x[:,2]**2\n",
    "#     Z[:,8] = 2*x[:,2]\n",
    "#     Z[:,9] = 1\n",
    "#     v, s, t = np.linalg.svd(Z,full_matrices=False)        \n",
    "#     smallest_index = np.argmin(np.array(s))\n",
    "#     T = np.array(t)\n",
    "#     T = T[smallest_index,:]        \n",
    "#     norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n",
    "#     return norm\n",
    "\n",
    "# def _preprocess(hits):\n",
    "#     x = hits.x.values\n",
    "#     y = hits.y.values\n",
    "#     z = hits.z.values\n",
    "\n",
    "#     r = np.sqrt(x**2 + y**2 + z**2)\n",
    "#     hits['x2'] = x/r\n",
    "#     hits['y2'] = y/r\n",
    "\n",
    "#     r = np.sqrt(x**2 + y**2)\n",
    "#     hits['z2'] = z/r\n",
    "\n",
    "#     ss = StandardScaler()\n",
    "#     X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#     for i, rz_scale in enumerate(self.rz_scales):\n",
    "#         X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_dbscan():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "for i,event_id in enumerate(event_ids):\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "    cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "    truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    \n",
    "    truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "    \n",
    "    w1 = [0.9, 1.2]\n",
    "    w2 = [0.3, 0.7]\n",
    "    w3 = [0.1, 0.4]\n",
    "    Niter = [140, 190]\n",
    "    \n",
    "    \n",
    "#     bo = BayesianOptimization(Fun4BO,pbounds = {'w1':w1,'w2':w2,'w3':w3,'Niter':Niter})\n",
    "#     bo.maximize(init_points = 3, n_iter = 20, acq = \"ucb\", kappa = 2.576)\n",
    "    w1 = 1.1932215111905984\n",
    "    w2 = 0.39740553885387364\n",
    "    w3 = 0.3512647720585538\n",
    "    w4 = 0.1470\n",
    "    w5 = 0.2691\n",
    "    w6 = 0.0020\n",
    "    w6 = [0.0000001, 1.2] \n",
    "    w10 = [0.00001, 1.2] \n",
    "    w12 = [0.00001, 0.1] \n",
    "    Niter = 179\n",
    "    w12 = 0.00045\n",
    "#     Fun4BO21(w12)\n",
    "#     for w12 in np.arange(0.0002, 0.003, 0.00005):\n",
    "#         print(w12)\n",
    "#         Fun4BO21(w12)\n",
    "#     bo = BayesianOptimization(Fun4BO21,pbounds = {'w12':w12})\n",
    "#     bo.maximize(init_points = 20, n_iter = 5, acq = \"ucb\", kappa = 2.576)\n",
    "\n",
    "# z1+z2: 13 | 05m39s |    0.55616 |    0.1124 | \n",
    "# z1 * z2: 13 | 05m40s |    0.55637 |    0.0404 | \n",
    "# tt = cos(theta), theta = np.arctan2(rt,z): 4 | 06m01s |    0.55196 |    0.2711 | \n",
    "# tt, theta = np.arctan2(rt2,z2);  8 | 05m39s |    0.55604 |    0.0005 | \n",
    "# cos + sin: 15 | 05m39s |    0.55714 |    0.2691 | \n",
    "# cos-sin: 9 | 05m51s |    0.55714 |    0.0020 | \n",
    "# cos+sin/cos-sin: 8 | 06m03s |    0.55694 |    0.0012 | \n",
    "# ctt, stt, ctt+stt/ctt-stt: 8 | 06m03s |    0.55273 |    0.0000 | \n",
    "# ctt: 10 | 05m39s |    0.55613 |    0.0047 | \n",
    "# caa * stt: 7 | 05m38s |    0.55613 |    0.0022 |\n",
    "# saa * ctt: 15 | 05m38s |    0.55613 |    0.0033 |\n",
    "# c1 = caa*stt, c2 = saa*ctt, c1+c2: 10 | 05m43s |    0.55622 |    0.0571 | \n",
    "# caa * saa: 7 | 05m45s |    0.55639 |    0.1548 | \n",
    "# caa/saa: 13 | 05m43s |    0.55613 |    0.000001 | \n",
    "# xx1, xx2: 12 | 06m04s |    0.55613 |    0.000001 |    0.000001 | \n",
    "\n",
    "# hdbscan - braycurtis: 0.14424123982821765\n",
    "# hdbscan - euclidean : 0.1311\n",
    "\n",
    "# eps: 0.0040: 0.57549\n",
    "# tt: 21 | 07m08s |    0.57208 |    0.0000 |  (1e-07)\n",
    "# ctt, stt (after new eqn: 25 | 07m33s |    0.57254 |    0.0026 | \n",
    "# ctt, stt: 0.00261: 0.5727074941034839\n",
    "# caa, saa, z1, z2, rt/r, x/r, z3, ctt, stt, y1, y3:\n",
    "# w1 = 1.1932215111905984, w2 = 0.39740553885387364, w3 = 0.3512647720585538, w4 = 0.1470, w5 = 0.01201\n",
    "# w6 = 0.0205, w7 = 0.00261, w8 = 0.0049, w9 = 0.0012 (0.5717942069958433)\n",
    "# \n",
    "# X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3])):\n",
    "# w1 = 1.1932215111905984 w2 = 0.39740553885387364 w3 = 0.3512647720585538 w4 = 0.1470 w5 = 0.01201 w6 = 0.0003864 \n",
    "# w7 = 0.0205 w8 = 0.0049 w9 = 0.00121 (0.57343)\n",
    "# ctt, stt: 13 | 08m16s |    0.57343 |    0.0003 | (0.00032967312140735677)\n",
    "# ctt, stt: 21 | 08m20s |    0.57343 |    0.0000 |  (1.4930496676654575e-05)\n",
    "# ctt, stt: 15 | 08m14s |    0.57351 |    0.8435 | (t1 = theta+mm*(rt+ w11*rt**2)/1000*(ii/2)/180*np.pi)\n",
    "# z4: 0.0245 (0.5735925042985041)\n",
    "# # z4 0.0318 (0.5736635664313068)\n",
    "# x4: 0.00001 (0.5735421714482896)\n",
    "# x4: 0.00025 (0.5736999491677117)\n",
    "# x4: 0.00045 (0.5737240529228144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trackML32(w13):\n",
    "#     w1 = 1.1932215111905984\n",
    "#     w2 = 0.39740553885387364\n",
    "#     w3 = 0.3512647720585538\n",
    "#     w4 = 0.1470\n",
    "#     w5 = 0.01201\n",
    "#     w6 = 0.0003864\n",
    "#     w7 = 0.0205\n",
    "#     w8 = 0.0049\n",
    "#     w9 = 0.00121\n",
    "#     w10 = 1.4930496676654575e-05\n",
    "#     w11 = 0.0318\n",
    "#     w12 = 0.00045\n",
    "#     Niter=179\n",
    "#     Niter=247\n",
    "\n",
    "#     df = hits\n",
    "#     x  = df.x.values\n",
    "#     y  = df.y.values\n",
    "#     z  = df.z.values\n",
    "\n",
    "#     dz = 0\n",
    "#     z = z + dz\n",
    "\n",
    "#     rt  = np.sqrt(x**2+y**2)\n",
    "#     r  = np.sqrt(x**2+y**2+z**2)\n",
    "#     a0  = np.arctan2(y,x)\n",
    "\n",
    "#     x2 = x/r\n",
    "#     y2 = y/r\n",
    "\n",
    "\n",
    "#     phi = np.arctan2(y, x)\n",
    "#     phi_deg= np.degrees(np.arctan2(y, x))\n",
    "\n",
    "\n",
    "#     z1 = z/rt\n",
    "#     z2 = z/r\n",
    "\n",
    "#     z3 = np.log1p(abs(z/r))*np.sign(z)\n",
    "\n",
    "#     x1 = x/rt\n",
    "#     y1 = y/rt\n",
    "\n",
    "#     y3 = np.log1p(abs(y/r))*np.sign(y)\n",
    "\n",
    "#     theta = np.arctan2(rt, z)\n",
    "#     theta_deg = np.degrees(np.arctan2(rt, z))\n",
    "#     tt = np.tan(theta_deg)\n",
    "\n",
    "#     z4 = np.sqrt(abs(z/rt))\n",
    "\n",
    "#     x4 = np.sqrt(abs(x/r))\n",
    "#     y4 = np.sqrt(abs(y/r))\n",
    "\n",
    "#     mm = 1\n",
    "#     ls = []\n",
    "\n",
    "#     #     print(Niter)\n",
    "#     #     Niter = Niter.astype(np.int32)\n",
    "#     #     print(Niter)\n",
    "#     #     print(w1, w2, w3, Niter)\n",
    "\n",
    "#     for ii in range(Niter):\n",
    "#         mm = mm * (-1)\n",
    "\n",
    "#         a1 = a0+mm*(rt+ 0.0000145*rt**2)/1000*(ii/2)/180*np.pi\n",
    "#     #         print(a0, a1)\n",
    "#         saa = np.sin(a1)\n",
    "#         caa = np.cos(a1)\n",
    "\n",
    "#         t1 = theta+mm*(rt+ 0.8435*rt**2)/1000*(ii/2)/180*np.pi\n",
    "#         ctt = np.cos(t1)\n",
    "#         stt = np.sin(t1)\n",
    "#         ttt = np.tan(t1)\n",
    "#         deps = 0.0000025\n",
    "#     #         X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, z3, ctt, stt]))\n",
    "#         X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3, \n",
    "#                                                             ctt, stt, z4, x4]))\n",
    "#     #         print(X.shape)\n",
    "\n",
    "#     #         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w7]\n",
    "#         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w8, w9, w10, w10, w11, w12]\n",
    "#         for jj in range(X.shape[1]): \n",
    "#              X[:,jj] = X[:,jj]*cx[jj]\n",
    "#         deps = 0.0000025\n",
    "#     #         cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,cluster_selection_method='leaf',metric='euclidean', leaf_size=50)\n",
    "#     #         l = cl.fit_predict(X)+1\n",
    "\n",
    "#         l= DBSCAN(eps=0.0035,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     #         l= DBSCAN(eps=w7,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#         ls.append(l)\n",
    "\n",
    "# #     for ii in range(Niter):\n",
    "# #         mm = mm * (1)\n",
    "\n",
    "# #         a1 = a0+mm*(rt+0.000005*rt**2)/1000*(ii/2)/180*np.pi\n",
    "# #     #         print(a0, a1)\n",
    "# #         saa = np.sin(a1)\n",
    "# #         caa = np.cos(a1)\n",
    "\n",
    "# #         t1 = theta+mm*(rt+ 0.8435*rt**2)/1000*(ii/2)/180*np.pi\n",
    "# #         ctt = np.cos(t1)\n",
    "# #         stt = np.sin(t1)\n",
    "# #         ttt = np.tan(t1)\n",
    "# #         deps = 0.0000025\n",
    "# #     #         X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, z3, ctt, stt]))\n",
    "# #         X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3, \n",
    "# #                                                             ctt, stt, z4, x4]))\n",
    "        \n",
    "# #         X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2]))\n",
    "# #     #         print(X.shape)\n",
    "\n",
    "# #     #         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w7]\n",
    "# #         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w8, w9, w10, w10, w11, w12]\n",
    "# #         cx = [w13,w13,w2,w3]\n",
    "# #         for jj in range(X.shape[1]): \n",
    "# #              X[:,jj] = X[:,jj]*cx[jj]\n",
    "# #         deps = 0.0000025\n",
    "# #     #         cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,cluster_selection_method='leaf',metric='euclidean', leaf_size=50)\n",
    "# #     #         l = cl.fit_predict(X)+1\n",
    "\n",
    "# #         l= DBSCAN(eps=0.0035,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "# #     #         l= DBSCAN(eps=w7,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "# #         ls.append(l)\n",
    "\n",
    "#     X = _preprocess(hits) \n",
    "#     cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "#                          metric='braycurtis',cluster_selection_method='leaf',algorithm='best', leaf_size=50)\n",
    "#     labels = np.unique(ls)\n",
    "#     self._eliminate_outliers(labels,X)          \n",
    "#     max_len = np.max(self.clusters)\n",
    "#     mask = self.clusters == 0\n",
    "#     self.clusters[mask] = cl.fit_predict(X[mask])+max_len\n",
    "#     return self.clusters\n",
    "    \n",
    "#     num_hits=len(df)\n",
    "#     labels = np.zeros(num_hits,np.int32)\n",
    "#     counts = np.zeros(num_hits,np.int32)\n",
    "#     for l in ls:\n",
    "#         c = make_counts(l)\n",
    "#         idx = np.where((c-counts>0) & (c<20))[0]\n",
    "#         labels[idx] = l[idx] + labels.max()\n",
    "#         counts = make_counts(labels)\n",
    "\n",
    "#     track_id = labels\n",
    "#     sum_score=0\n",
    "#     sum = 0\n",
    "#     submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "#         data=np.column_stack(([int(event_id),]*len(hits), hits.hit_id.values, track_id))\n",
    "#     ).astype(int)\n",
    "#     for i in range(8):\n",
    "#         submission = extend(submission,hits)\n",
    "#         score = score_event(truth, submission)\n",
    "#         print('[%2d] score : %0.8f'%(i, score))\n",
    "#         sum_score += score\n",
    "#         sum += 1\n",
    "\n",
    "#     print('--------------------------------------')\n",
    "#     sc = sum_score/sum\n",
    "#     print(sc)\n",
    "# # org score: 0.5737240529228144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackML32(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import hdbscan\n",
    "# from scipy import stats\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# class Clusterer(object):\n",
    "#     def __init__(self,rz_scales=[0.65, 0.965, 1.528]):                        \n",
    "#         self.rz_scales=rz_scales\n",
    "    \n",
    "#     def _eliminate_outliers(self,labels,M):\n",
    "#         norms=np.zeros((len(labels)),np.float32)\n",
    "#         indices=np.zeros((len(labels)),np.float32)\n",
    "#         for i, cluster in tqdm(enumerate(labels),total=len(labels)):\n",
    "#             if cluster == 0:\n",
    "#                 continue\n",
    "#             index = np.argwhere(self.clusters==cluster)\n",
    "#             index = np.reshape(index,(index.shape[0]))\n",
    "#             indices[i] = len(index)\n",
    "#             x = M[index]\n",
    "#             norms[i] = self._test_quadric(x)\n",
    "#         threshold1 = np.percentile(norms,90)*5\n",
    "#         threshold2 = 25\n",
    "#         threshold3 = 6\n",
    "#         for i, cluster in enumerate(labels):\n",
    "#             if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n",
    "#                 self.clusters[self.clusters==cluster]=0   \n",
    "#     def _test_quadric(self,x):\n",
    "#         if x.size == 0 or len(x.shape)<2:\n",
    "#             return 0\n",
    "#         Z = np.zeros((x.shape[0],10), np.float32)\n",
    "#         Z[:,0] = x[:,0]**2\n",
    "#         Z[:,1] = 2*x[:,0]*x[:,1]\n",
    "#         Z[:,2] = 2*x[:,0]*x[:,2]\n",
    "#         Z[:,3] = 2*x[:,0]\n",
    "#         Z[:,4] = x[:,1]**2\n",
    "#         Z[:,5] = 2*x[:,1]*x[:,2]\n",
    "#         Z[:,6] = 2*x[:,1]\n",
    "#         Z[:,7] = x[:,2]**2\n",
    "#         Z[:,8] = 2*x[:,2]\n",
    "#         Z[:,9] = 1\n",
    "#         v, s, t = np.linalg.svd(Z,full_matrices=False)        \n",
    "#         smallest_index = np.argmin(np.array(s))\n",
    "#         T = np.array(t)\n",
    "#         T = T[smallest_index,:]        \n",
    "#         norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n",
    "#         return norm\n",
    "\n",
    "#     def _preprocess(self, hits):\n",
    "        \n",
    "#         x = hits.x.values\n",
    "#         y = hits.y.values\n",
    "#         z = hits.z.values\n",
    "\n",
    "#         r = np.sqrt(x**2 + y**2 + z**2)\n",
    "#         hits['x2'] = x/r\n",
    "#         hits['y2'] = y/r\n",
    "\n",
    "#         r = np.sqrt(x**2 + y**2)\n",
    "#         hits['z2'] = z/r\n",
    "\n",
    "#         ss = StandardScaler()\n",
    "#         X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#         for i, rz_scale in enumerate(self.rz_scales):\n",
    "#             X[:,i] = X[:,i] * rz_scale\n",
    "       \n",
    "#         return X\n",
    "#     def _init(self, dfh, w1, w2, w3, w4, w5, w6, w7, w8,w9,w10,w11,w12, epsilon, Niter, z_shift):\n",
    "#         # z- shift\n",
    "#         dfh['z'] = dfh['z'].values - z_shift\n",
    "#         dfh['r'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2 + dfh['z'].values ** 2)\n",
    "#         dfh['rt'] = np.sqrt(dfh['x'].values ** 2 + dfh['y'].values ** 2)\n",
    "#         dfh['a0'] = np.arctan2(dfh['y'].values, dfh['x'].values)\n",
    "#         dfh['z1'] = dfh['z'].values / dfh['rt'].values\n",
    "#         dfh['z2'] = dfh['z'].values / dfh['r'].values\n",
    "#         dfh['s1'] = dfh['hit_id']\n",
    "#         dfh['N1'] = 1\n",
    "#         dfh['z1'] = dfh['z'].values / dfh['rt'].values\n",
    "#         dfh['z2'] = dfh['z'].values / dfh['r'].values\n",
    "#         dfh['x1'] = dfh['x'].values / dfh['y'].values\n",
    "#         dfh['x2'] = dfh['x'].values / dfh['r'].values\n",
    "#         dfh['x3'] = dfh['y'].values / dfh['r'].values\n",
    "#         dfh['x4'] = dfh['rt'].values / dfh['r'].values\n",
    "#         dfh['x5'] = dfh['y'].values / dfh['rt'].values\n",
    "        \n",
    "#         dfh['z3'] = np.log1p(abs((dfh['z'].values / dfh['r'].values)))*np.sign(dfh['z'])\n",
    "#         dfh['y3'] = np.log1p(abs((dfh['y'].values / dfh['r'].values)))\n",
    "        \n",
    "#         dfh['z4'] = np.sqrt(abs((dfh['z'].values / dfh['rt'].values)))\n",
    "        \n",
    "#         dfh['x6'] = np.sqrt(abs((dfh['x'].values / dfh['r'].values)))\n",
    "        \n",
    "#         dfh['theta'] = np.arctan2(dfh['rt'].values, dfh['z'].values)\n",
    "#         dfh['theta_deg'] = np.degrees(np.arctan2(dfh['rt'].values, dfh['z'].values))\n",
    "    \n",
    "#         mm = 1\n",
    "# #         print(dfh['s1'].values)\n",
    "#         for ii in tqdm(range(int(Niter))):\n",
    "#             mm = mm * (-1)\n",
    "#             dfh['a1'] = dfh['a0'].values + mm * (dfh['rt'].values + 0.0000145\n",
    "#                                                  * dfh['rt'].values ** 2) / 1000 * (ii / 2) / 180 * np.pi\n",
    "#             dfh['sina1'] = np.sin(dfh['a1'].values)\n",
    "#             dfh['cosa1'] = np.cos(dfh['a1'].values)\n",
    "#             ss = StandardScaler()\n",
    "# #             dfs = ss.fit_transform(dfh[['sina1', 'cosa1', 'z1', 'z2','x1','x2','x3','x4']].values)\n",
    "# #             cx = np.array([w1, w1, w2, w3, w4, w5, w6, w7])\n",
    "#             dfh['t1'] = dfh['theta'].values+mm*(dfh['rt'].values+ 0.8435*dfh['rt'].values**2)/1000*(ii/2)/180*np.pi\n",
    "#             dfh['stt'] = np.sin(dfh['t1'].values)\n",
    "#             dfh['ctt'] = np.cos(dfh['t1'].values)\n",
    "        \n",
    "# #             X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3, \n",
    "# #                                                             ctt, stt, z4, x4]))\n",
    "            \n",
    "#             dfs = ss.fit_transform(dfh[['sina1', 'cosa1', 'z1', 'z2', 'x4', 'x2', 'x3', 'z3', 'x5', 'y3', \n",
    "#                                                             'z4', 'x6']])\n",
    "#     #         print(X.shape)\n",
    "\n",
    "#     #         cx = [w1,w1,w2,w3, w4, w5, w6, w7, w7]\n",
    "#             cx = [w1,w1,w2,w3, w4, w5, w6, w7, w8, w9, w11, w12]\n",
    "#             dfs = np.multiply(dfs, cx)\n",
    "#             clusters = DBSCAN(eps=epsilon, min_samples=1, metric=\"euclidean\", n_jobs=32).fit(dfs).labels_\n",
    "#             dfh['s2'] = clusters\n",
    "#             dfh['N2'] = dfh.groupby('s2')['s2'].transform('count')\n",
    "#             maxs1 = dfh['s1'].max()\n",
    "#             dfh.loc[(dfh['N2'] > dfh['N1']) & (dfh['N2'] < 20),'s1'] = dfh['s2'] + maxs1\n",
    "#             dfh['N1'] = dfh.groupby('s1')['s1'].transform('count')\n",
    "# #         print(dfh['s1'].values)\n",
    "#         return dfh['s1'].values\n",
    "#     def predict(self, hits, w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12, epsilon, Niter, z_shift):         \n",
    "# #         self.clusters = self._init(hits,2.7474448671796874,1.3649721713529086,0.7034918842926337,\n",
    "# #                                         0.0005549122352940002,0.023096034747190672,0.04619756315527515,\n",
    "# #                                         0.2437077420144654,0.009750302717746615,338)\n",
    "\n",
    "        \n",
    "        \n",
    "#         self.clusters = self._init(hits,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,epsilon,Niter, z_shift)\n",
    "\n",
    "                                       \n",
    "# #         X = self._preprocess(hits) \n",
    "# #         cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "# #                              metric='braycurtis',cluster_selection_method='leaf',algorithm='best', leaf_size=50)\n",
    "# #         labels = np.unique(self.clusters)\n",
    "# #         self._eliminate_outliers(labels,X)          \n",
    "# #         max_len = np.max(self.clusters)\n",
    "# #         mask = self.clusters == 0\n",
    "# #         print(X[mask].shape)\n",
    "# #         self.clusters[mask] = cl.fit_predict(X[mask])+max_len\n",
    "#         return self.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [05:59<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.124 9.897752042164421e-05\n"
     ]
    }
   ],
   "source": [
    "# # w1 = 1.1932215111905984\n",
    "# w1 = 1.124\n",
    "# # w2 = 0.39740553885387364\n",
    "# w2 = 0.575\n",
    "# # w3 = 0.3512647720585538\n",
    "# w3 = 0.125\n",
    "# # w4 = 0.1470\n",
    "# w4 = 0.165\n",
    "# # w5 = 0.01201\n",
    "# w5 = 0.015\n",
    "# # w6 = 0.0003864\n",
    "# w6 = 0.0003\n",
    "# # w7 = 0.0205\n",
    "# w7 = 0.025\n",
    "# w8 = 0.0049\n",
    "# # w9 = 0.00121\n",
    "# w9 = 0.0015\n",
    "# w10 = 1.4930496676654575e-05\n",
    "# # w11 = 0.0318\n",
    "# w11 = 0.015\n",
    "# # w12 = 0.00045\n",
    "# w12 = 0.00065\n",
    "# Niter = 247\n",
    "# epsilon = 0.005 # after iteration\n",
    "# z_shift = 55\n",
    "# # for w1 in np.arange(1.0,2.0,0.1):\n",
    "# model = Clusterer()\n",
    "# labels = model.predict(hits,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,epsilon,Niter, z_shift)\n",
    "# submission = create_one_event_submission(0, hits, labels)\n",
    "# score = score_event(truth, submission)\n",
    "# print(w1, score)\n",
    "# # w1: 1.124 - 0.5279192227189358\n",
    "# # w2: 0.575: 0.5293042848848362\n",
    "# # w3: 0.125: 0.5384771485839126\n",
    "# # w4: 0.165: 0.5387405251950346\n",
    "# # w5: 0.015: 0.5397604313593793\n",
    "# # w6: 0.0003: 0.5397817800394703\n",
    "# # w7: 0.025: 0.539861997939812\n",
    "# # w8: 0.0049: 0.539861997939812\n",
    "# # w9: 0.0015 0.5398880399799231\n",
    "# # w10: 0.0015: 0.5398843174199072 (dont use)\n",
    "# # w11: 0.015: 0.5399689874402678\n",
    "# # w12: 0.00065 0.5400076261804324\n",
    "# # epsilon 0.005: 0.541690617197602 ( with stuff commented out)\n",
    "# # with z_shift 5.5: 0.47561716960612915 \n",
    "# # with z_shift 55: 0.01741897458420483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score:  0.0006367412927125178\n"
     ]
    }
   ],
   "source": [
    "# submission = create_one_event_submission(0, hits, labels)\n",
    "# score = score_event(truth, submission)\n",
    "# print(\"Your score: \", score)\n",
    "# # kernel org: 0.5129067228149826\n",
    "# # my org: 0.4321907182011324\n",
    "# # epsilon (0.00975, 0.33325577687966956)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(labels):\n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "def find_labels1(params):\n",
    "    hits, dz, eps = params\n",
    "    a = hits['phi'].values\n",
    "    z = hits['z'].values\n",
    "    zr = hits['zr'].values\n",
    "    zrt = hits['zrt'].values\n",
    "    aa = a + np.sign(z) * dz * z\n",
    "\n",
    "    f0 = np.cos(aa)\n",
    "    f1 = np.sin(aa)\n",
    "    f2 = zr\n",
    "    f3 = zrt\n",
    "    X = StandardScaler().fit_transform(np.column_stack([f0, f1, f3]))\n",
    "\n",
    "def find_labels(params):\n",
    "    hits, dz, eps, ii = params\n",
    "    a0 = hits['phi'].values\n",
    "    z = hits['z'].values\n",
    "    r = hits['zr'].values\n",
    "    rt = hits['zrt'].values\n",
    "#     aa = a + np.sign(z) * dz * z\n",
    "\n",
    "    a1 = a0 - (rt+ 0.0000145*rt**2)/1000*(ii/2)/180*np.pi\n",
    "    \n",
    "    f0 = np.cos(a1)\n",
    "    f1 = np.sin(a1)\n",
    "    f2 = r\n",
    "    f3 = rt\n",
    "    X = StandardScaler().fit_transform(np.column_stack([f0, f1, f3]))\n",
    "\n",
    "    \n",
    "    \n",
    "#     _, l = dbscan(X, eps=0.0048, min_samples=1, n_jobs=8)\n",
    "    _, l = dbscan(X, eps=eps, min_samples=1, n_jobs=8)\n",
    "    return l\n",
    "\n",
    "def add_count(l):\n",
    "    unique, reverse, count = np.unique(l, return_counts=True, return_inverse=True)\n",
    "    c = count[reverse]\n",
    "    c[np.where(l == 0)] = 0\n",
    "    c[np.where(c > 20)] = 0\n",
    "    return (l, c)\n",
    "\n",
    "def do_dbscan_predict(hits, truth, z_s, eps):\n",
    "    start_time = timeit.default_timer()\n",
    "    params = []\n",
    "#     shifts = list(np.random.uniform(low=-5.5, high=5.5, size=(50,)))\n",
    "#     eps = 0.0048 + z_s*10**(-5)\n",
    "    hits['z'] = hits['z'].values + z_s\n",
    "    hits['rt'] = np.sqrt(hits['x'] ** 2 + hits['y'] ** 2)\n",
    "    hits['r'] = np.sqrt(hits['x'] ** 2 + hits['y'] ** 2 + hits['z']**2)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zrt'] = hits['z'] / hits['rt']\n",
    "    hits['phi'] = np.arctan2(hits['y'], hits['x'])\n",
    "    ii = 0\n",
    "    for i in range(0, 20):\n",
    "        dz = i * 0.00001\n",
    "        params.append((hits, dz, eps, ii))\n",
    "        if i > 0:\n",
    "             params.append((hits, -dz, eps, ii))\n",
    "        ii += 1\n",
    "    # Kernel time is limited. So we skip some angles.\n",
    "    for i in range(20, 180):\n",
    "        dz = i * 0.00001\n",
    "        if i % 2 == 0:\n",
    "            params.append((hits, dz, eps,ii))\n",
    "        else:\n",
    "             params.append((hits, -dz, eps,ii))\n",
    "        ii += 1\n",
    "#     print(len(params))\n",
    "    pool = Pool(processes=8)\n",
    "    labels_for_all_steps = pool.map(find_labels, params)\n",
    "    results = [add_count(l) for l in labels_for_all_steps]\n",
    "    pool.close()\n",
    "\n",
    "# for l in ls:\n",
    "#         c = make_counts(l)\n",
    "#         idx = np.where((c-counts>0) & (c<20))[0]\n",
    "#         labels[idx] = l[idx] + labels.max()\n",
    "#         counts = make_counts(labels)\n",
    "        \n",
    "    labels, counts = results[0]\n",
    "    for i in range(1, len(results)):\n",
    "        l, c = results[i]\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts[idx] = c[idx]\n",
    "\n",
    "    submission = create_one_event_submission(0, hits['hit_id'].values, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print('[%5f], z_s: %0.8f, score : %0.8f' % (timeit.default_timer() - start_time, z_s, score))\n",
    "\n",
    "    return labels\n",
    "\n",
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def run_dbscan():\n",
    "    data_dir = '../data/train'\n",
    "\n",
    "    event_ids = ['000001030']\n",
    "    sum = 0\n",
    "    sum_score = 0\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        hits, cells, particles, truth = load_event(data_dir + '/event' + event_id)\n",
    "        num_hits = len(hits)\n",
    "        shifts = list(np.linspace(-5.5, 5.5, 50))\n",
    "        ls = []\n",
    "        labels = do_dbscan_predict(hits, truth, 0, 0.0048)\n",
    "        \n",
    "        submission = create_one_event_submission(0, hits['hit_id'].values, labels)\n",
    "        score = score_event(truth, submission)\n",
    "        print('[%2d] score : %0.8f' % (i, score))\n",
    "        sum_score += score\n",
    "        sum += 1\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    print(sum_score / sum)\n",
    "    \n",
    "def run_dbscan1():\n",
    "    data_dir = '../data/train'\n",
    "\n",
    "    event_ids = ['000001030']\n",
    "    sum = 0\n",
    "    sum_score = 0\n",
    "    \n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        hits, cells, particles, truth = load_event(data_dir + '/event' + event_id)\n",
    "        num_hits = len(hits)\n",
    "        shifts = list(np.linspace(-5.5, 5.5, 50))\n",
    "        ls = []\n",
    "        labels = do_dbscan_predict(hits, truth, 0, 0.0048)\n",
    "        ls.append(labels)\n",
    "        \n",
    "        \n",
    "        labels1 = labels\n",
    "        counts1 = make_counts(labels1)\n",
    "        \n",
    "#         shifts = [-5.5, -5.27551020, -5.05102041, 5.05102041]\n",
    "#         epss = [0.0045, 0.0046, 0.0047]\n",
    "        for z_s in shifts:\n",
    "            e = 0.0048\n",
    "#             for e in epss:\n",
    "            print(z_s, e)\n",
    "            hits, cells, particles, truth = load_event(data_dir + '/event' + event_id)\n",
    "            num_hits = len(hits)\n",
    "            labels = do_dbscan_predict(hits, truth, z_s, e)\n",
    "            ls.append(labels)\n",
    "            c = make_counts(labels)\n",
    "            idx = np.where((c-counts1>0) & (c<20))[0]\n",
    "            labels1[idx] = labels[idx] + labels1.max()\n",
    "            counts1 = make_counts(labels1)\n",
    "\n",
    "            submission = create_one_event_submission(0, hits['hit_id'].values, labels1)\n",
    "            score = score_event(truth, submission)\n",
    "            print('[%.8f] score : %0.8f' % (z_s, score))\n",
    "        \n",
    "#         num_hits = len(hits)\n",
    "        labels = np.zeros(num_hits,np.int32)\n",
    "        counts = np.zeros(num_hits,np.int32)\n",
    "    \n",
    "        for l in ls:\n",
    "            c = make_counts(l)\n",
    "            idx = np.where((c-counts>0) & (c<20))[0]\n",
    "            labels[idx] = l[idx] + labels.max()\n",
    "            counts = make_counts(labels)\n",
    "        submission = create_one_event_submission(0, hits['hit_id'].values, labels)\n",
    "        score = score_event(truth, submission)\n",
    "        print('[%2d] score : %0.8f' % (i, score))\n",
    "        sum_score += score\n",
    "        sum += 1\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    print(sum_score / sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate score by known events\n",
      "[38.135510], z_s: 0.00000000, score : 0.08007820\n",
      "[ 0] score : 0.08007820\n",
      "--------------------------------------\n",
      "0.08007819659883353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print('estimate score by known events')\n",
    "    run_dbscan()\n",
    "\n",
    "#     path_to_test = \"../input/test\"\n",
    "#     test_dataset_submissions = []\n",
    "\n",
    "#     create_submission = True  # True for submission\n",
    "#     if create_submission:\n",
    "#         print('process test events')\n",
    "#         for event_id, hits in load_dataset(path_to_test, parts=['hits']):\n",
    "#             print('Event ID: ', event_id)\n",
    "#             labels = do_dbscan_predict(hits)\n",
    "#             # Prepare submission for an event\n",
    "#             one_submission = create_one_event_submission(event_id, hits['hit_id'].values, labels)\n",
    "#             test_dataset_submissions.append(one_submission)\n",
    "\n",
    "#         # Create submission file\n",
    "#         submussion = pd.concat(test_dataset_submissions, axis=0)\n",
    "#         submussion.to_csv('submission_final.csv', index=False)\n",
    "# org: 0.42 (eps = 0.0045)\n",
    "# after zrt: 0.41 (removed zrt)\n",
    "# eps: 0.0048: 0.43233481315860445\n",
    "# 180 angles: 0.441763680947588\n",
    "# 360: 0.4337772013900576 (less!)\n",
    "# z shift random - 0.06236001632828447\n",
    "# zshift linspace: 0.441763680947588\n",
    "# shifts = [-5.5, -5.27551020, -5.05102041, 5.05102041]: 0.4782573730101376\n",
    "# 50 shits: 0.489\n",
    "# 0.079861982990451 formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c > 3\n",
    "# [46.136068], z_s: 0.00000000, score : 0.44176368\n",
    "# [46.775323], z_s: -5.50000000, score : 0.38450144\n",
    "# [-5.50000000] score : 0.45783910\n",
    "# [45.040371], z_s: -5.27551020, score : 0.29081078\n",
    "# [-5.27551020] score : 0.44779677\n",
    "\n",
    "# c > 7\n",
    "# 44.588105], z_s: 0.00000000, score : 0.44176368\n",
    "# [45.355518], z_s: -5.50000000, score : 0.38450144\n",
    "# [-5.50000000] score : 0.45489699\n",
    "# [45.183360], z_s: -5.27551020, score : 0.29081078\n",
    "# [-5.27551020] score : 0.44521397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate score by known events\n",
    "# 199\n",
    "# [49.976010], z_s: 0.00000000, score : 0.44176368\n",
    "# 199\n",
    "# [50.850307], z_s: -5.50000000, score : 0.38450144\n",
    "# 199\n",
    "# [48.037869], z_s: -5.27551020, score : 0.29081078\n",
    "# 199\n",
    "# [51.146998], z_s: -5.05102041, score : 0.18956037\n",
    "# 199\n",
    "# [51.095927], z_s: -4.82653061, score : 0.10928646\n",
    "# 199\n",
    "# [52.611142], z_s: -4.60204082, score : 0.06405998\n",
    "# 199\n",
    "# [54.109119], z_s: -4.37755102, score : 0.04192227\n",
    "# 199\n",
    "# [50.738167], z_s: -4.15306122, score : 0.02613579\n",
    "# 199\n",
    "# [52.260359], z_s: -3.92857143, score : 0.01964266\n",
    "# 199\n",
    "# [51.789074], z_s: -3.70408163, score : 0.01459217\n",
    "# 199\n",
    "# [52.848817], z_s: -3.47959184, score : 0.01212906\n",
    "# 199\n",
    "# [50.870826], z_s: -3.25510204, score : 0.01070783\n",
    "# 199\n",
    "# [50.235415], z_s: -3.03061224, score : 0.00909727\n",
    "# 199\n",
    "# [49.270647], z_s: -2.80612245, score : 0.00599853\n",
    "# 199\n",
    "# [49.129222], z_s: -2.58163265, score : 0.00649701\n",
    "# 199\n",
    "# [48.286129], z_s: -2.35714286, score : 0.00562762\n",
    "# 199\n",
    "# [49.842293], z_s: -2.13265306, score : 0.00491236\n",
    "# 199\n",
    "# [49.384661], z_s: -1.90816327, score : 0.00477814\n",
    "# 199\n",
    "# [48.717680], z_s: -1.68367347, score : 0.00457546\n",
    "# 199\n",
    "# [48.012776], z_s: -1.45918367, score : 0.00436190\n",
    "# 199\n",
    "# [49.221061], z_s: -1.23469388, score : 0.00420920\n",
    "# 199\n",
    "# [48.068072], z_s: -1.01020408, score : 0.00431361\n",
    "# 199\n",
    "# [47.772906], z_s: -0.78571429, score : 0.00423060\n",
    "# 199\n",
    "# [48.778291], z_s: -0.56122449, score : 0.00381379\n",
    "# 199\n",
    "# [47.947019], z_s: -0.33673469, score : 0.00366165\n",
    "# 199\n",
    "# [48.434124], z_s: -0.11224490, score : 0.00368642\n",
    "# 199\n",
    "# [48.716099], z_s: 0.11224490, score : 0.00366165\n",
    "# 199\n",
    "# [48.707970], z_s: 0.33673469, score : 0.00381379\n",
    "# 199\n",
    "# [48.511677], z_s: 0.56122449, score : 0.00423060\n",
    "# 199\n",
    "# [47.882633], z_s: 0.78571429, score : 0.00431361\n",
    "# 199\n",
    "# [49.330805], z_s: 1.01020408, score : 0.00420920\n",
    "# 199\n",
    "# [49.659249], z_s: 1.23469388, score : 0.00436190\n",
    "# 199\n",
    "# [49.121822], z_s: 1.45918367, score : 0.00457546\n",
    "# 199\n",
    "# [50.480902], z_s: 1.68367347, score : 0.00477814\n",
    "# 199\n",
    "# [50.040903], z_s: 1.90816327, score : 0.00491236\n",
    "# 199\n",
    "# [50.346628], z_s: 2.13265306, score : 0.00562762\n",
    "# 199\n",
    "# [50.045502], z_s: 2.35714286, score : 0.00649701\n",
    "# 199\n",
    "# [50.590774], z_s: 2.58163265, score : 0.00599853\n",
    "# 199\n",
    "# [53.998526], z_s: 2.80612245, score : 0.00909727\n",
    "# 199\n",
    "# [50.601889], z_s: 3.03061224, score : 0.01070783\n",
    "# 199\n",
    "# [50.619714], z_s: 3.25510204, score : 0.01212906\n",
    "# 199\n",
    "# [51.517029], z_s: 3.47959184, score : 0.01459217\n",
    "# 199\n",
    "# [49.824840], z_s: 3.70408163, score : 0.01964266\n",
    "# 199\n",
    "# [49.964861], z_s: 3.92857143, score : 0.02613579\n",
    "# 199\n",
    "# [50.102062], z_s: 4.15306122, score : 0.04192227\n",
    "# 199\n",
    "# [50.318057], z_s: 4.37755102, score : 0.06405998\n",
    "# 199\n",
    "# [51.581126], z_s: 4.60204082, score : 0.10928646\n",
    "# 199\n",
    "# [52.135404], z_s: 4.82653061, score : 0.18956037\n",
    "# 199\n",
    "# [52.178464], z_s: 5.05102041, score : 0.29081078\n",
    "# 199\n",
    "# [51.919961], z_s: 5.27551020, score : 0.38450144\n",
    "# 199\n",
    "# [48.914852], z_s: 5.50000000, score : 0.44176368\n",
    "# [ 0] score : 0.43466177\n",
    "# --------------------------------------\n",
    "# 0.43466176909896603"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmlf",
   "language": "python",
   "name": "tmlf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
