{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require(data.table)\n",
    "# require(bit64)\n",
    "# require(dbscan)\n",
    "# require(doParallel)\n",
    "# require(rBayesianOptimization)\n",
    "# path='../input/train_1/'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan as _hdbscan\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "import hdbscan\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# https://www.ellicium.com/python-multiprocessing-pool-process/\n",
    "# http://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import hdbscan as _hdbscan\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29246455/python-setting-decimal-place-range-without-rounding\n",
    "import math\n",
    "def truncate(f, n):\n",
    "    return math.floor(f * 10 ** n) / 10 ** n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(hits):        \n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "#     for i, rz_scale in enumerate(self.rz_scales):\n",
    "#         X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "def make_counts(labels):\n",
    "    \n",
    "    \n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def one_loop(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r, a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da \n",
    "    zr = z/r\n",
    "    \n",
    "    X = StandardScaler().fit_transform(np.column_stack([aa, aa/zr, zr, 1/zr, aa/zr + 1/zr]))\n",
    "    _,l = dbscan(X, eps=0.0035, min_samples=1,)\n",
    "\n",
    "    return l\n",
    "\n",
    "def one_loop1(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def one_loop2(param):\n",
    "    \n",
    "    # <todo> tune your parameters or design your own features here! \n",
    "    \n",
    "    i,m, x,y,z, d,r,r2,z2,a, a_start,a_step = param\n",
    "    #print('\\r %3d  %+0.8f '%(i,da), end='', flush=True)\n",
    "    \n",
    "    da = m*(a_start - (i*a_step))\n",
    "    aa = a + np.sign(z)*z*da\n",
    "#     if m == 1:\n",
    "#         print(da)\n",
    "    zr = z/r # this is cot(theta), 1/zr is tan(theta)\n",
    "    theta = np.arctan2(r, z)\n",
    "    ct = np.cos(theta)\n",
    "    st = np.sin(theta)\n",
    "    tt = np.tan(theta)\n",
    "#     ctt = np.cot(theta)\n",
    "    z2r = z2/r\n",
    "    z2r2 = z2/r2\n",
    "#     X = StandardScaler().fit_transform(df[['r2', 'theta_1', 'dip_angle', 'z2', 'z2_1', 'z2_2']].values)\n",
    "\n",
    "    caa = np.cos(aa)\n",
    "    saa = np.sin(aa)\n",
    "    taa = np.tan(aa)\n",
    "    ctaa = 1/taa\n",
    "    \n",
    "#     0.000005\n",
    "    deps = 0.0000025\n",
    "    X = StandardScaler().fit_transform(np.column_stack([caa, saa, tt, 1/tt]))\n",
    "    l= DBSCAN(eps=0.0035+i*deps,min_samples=1,metric='euclidean',n_jobs=8).fit(X).labels_\n",
    "#     _,l = dbscan(X, eps=0.0035, min_samples=1,algorithm='auto')\n",
    "    \n",
    "    return l\n",
    "\n",
    "def do_dbscan_predict(df):\n",
    "    \n",
    "    x  = df.x.values\n",
    "    y  = df.y.values\n",
    "    z  = df.z.values\n",
    "    r  = np.sqrt(x**2+y**2)\n",
    "    d  = np.sqrt(x**2+y**2+z**2)\n",
    "    a  = np.arctan2(y,x)\n",
    "\n",
    "    x2 = df['x']/d\n",
    "    y2 = df['y']/d\n",
    "    z2 = df['z']/r\n",
    "\n",
    "    r2 = np.sqrt(x2**2 + y2**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    phi_deg= np.degrees(np.arctan2(y, x))\n",
    "    phi2 = np.arctan2(y2, x2)\n",
    "    phi2_deg = np.degrees(np.arctan2(y2, x2))\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    a_start,a_step,a_num = 0.00100,0.0000095,150\n",
    "\n",
    "    params  = [(i,m, x,y,z,d,r,r2,z2, a, a_start,a_step) for i in range(a_num) for m in [-1,1]]\n",
    "\n",
    "    if 1: \n",
    "        pool = Pool(processes=1)\n",
    "        ls   = pool.map( one_loop1, params )\n",
    "\n",
    "\n",
    "    if 0:\n",
    "        ls = [ one_loop(param) for param in  params ]\n",
    "\n",
    "\n",
    "    ##------------------------------------------------\n",
    "\n",
    "    num_hits=len(df)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "       \n",
    "\n",
    "#     cl = hdbscan.HDBSCAN(min_samples=1,min_cluster_size=7,\n",
    "#                          metric='braycurtis',cluster_selection_method='leaf',algorithm='best', \n",
    "#                          leaf_size=50)\n",
    "    \n",
    "#     X = preprocess(df)\n",
    "#     l1 = pd.Series(labels)\n",
    "#     labels = np.unique(l1)\n",
    "   \n",
    "# #   print(X.shape)\n",
    "# #   print(len(labels_org))\n",
    "# #   print(len(labels_org[labels_org ==0]))\n",
    "# #   print(len(labels_org[labels_org ==-1]))\n",
    "    \n",
    "#     n_labels = 0\n",
    "#     while n_labels < len(labels):\n",
    "#         n_labels = len(labels)\n",
    "#         max_len = np.max(l1)\n",
    "#         s = list(l1[l1 == 0].keys())\n",
    "#         X = X[s]\n",
    "#         print(X.shape)\n",
    "#         if X.shape[0] <= 1:\n",
    "#             break\n",
    "#         l = cl.fit_predict(X)+max_len\n",
    "# #         print(len(l))\n",
    "#         l1[l1 == 0] = l\n",
    "#         labels = np.unique(l1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "\n",
    "## reference----------------------------------------------\n",
    "def do_dbscan0_predict(df):\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    z = df.z.values\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    d = np.sqrt(x**2+y**2+z**2)\n",
    "\n",
    "    X = StandardScaler().fit_transform(np.column_stack([\n",
    "        x/d, y/d, z/r]))\n",
    "    _,labels = dbscan(X,\n",
    "                eps=0.0075,\n",
    "                min_samples=1,\n",
    "                algorithm='auto',\n",
    "                n_jobs=-1)\n",
    "\n",
    "    #labels = hdbscan(X, min_samples=1, min_cluster_size=5, cluster_selection_method='eom')\n",
    "\n",
    "    return labels\n",
    "var1 = 0\n",
    "def extend(submission,hits):\n",
    "    df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     if var1 != 0:\n",
    "#         df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     else:\n",
    "#         df = hits\n",
    "#     df = submission.append(hits)\n",
    "#     print(df.head())\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "    for angle in range(-180,180,1):\n",
    "\n",
    "        print ('\\r %f'%angle, end='',flush=True)\n",
    "        #df1 = df.loc[(df.arctan2>(angle-0.5)/180*np.pi) & (df.arctan2<(angle+0.5)/180*np.pi)]\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "#         if len(df1) == 0:\n",
    "#             continue\n",
    "        min_num_neighbours = len(df1)\n",
    "        if min_num_neighbours<4: continue\n",
    "\n",
    "        hit_ids = df1.hit_id.values\n",
    "        x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "        r  = (x**2 + y**2)**0.5\n",
    "        r  = r/1000\n",
    "        a  = np.arctan2(y,x)\n",
    "        tree = KDTree(np.column_stack([a,r]), metric='euclidean')\n",
    "\n",
    "#         print(df1.head())\n",
    "        track_ids = list(df1.track_id.unique())\n",
    "        num_track_ids = len(track_ids)\n",
    "        min_length=3\n",
    "\n",
    "        for i in range(num_track_ids):\n",
    "            p = track_ids[i]\n",
    "            if p==0: continue\n",
    "\n",
    "            idx = np.where(df1.track_id==p)[0]\n",
    "            if len(idx)<min_length: continue\n",
    "\n",
    "            if angle>0:\n",
    "                idx = idx[np.argsort( z[idx])]\n",
    "            else:\n",
    "                idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "\n",
    "            ## start and end points  ##\n",
    "            idx0,idx1 = idx[0],idx[-1]\n",
    "            a0 = a[idx0]\n",
    "            a1 = a[idx1]\n",
    "            r0 = r[idx0]\n",
    "            r1 = r[idx1]\n",
    "\n",
    "            da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "            dr0 = r[idx[1]] - r[idx[0]]\n",
    "            direction0 = np.arctan2(dr0,da0) \n",
    "\n",
    "            da1 = a[idx[-1]] - a[idx[-2]]\n",
    "            dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "            direction1 = np.arctan2(dr1,da1) \n",
    "\n",
    "\n",
    "            ## extend start point\n",
    "            ns = tree.query([[a0,r0]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "            direction = np.arctan2(r0-r[ns],a0-a[ns])\n",
    "            ns = ns[(r0-r[ns]>0.01) &(np.fabs(direction-direction0)<0.04)]\n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p \n",
    "\n",
    "            ## extend end point\n",
    "            ns = tree.query([[a1,r1]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "\n",
    "            direction = np.arctan2(r[ns]-r1,a[ns]-a1)\n",
    "            ns = ns[(r[ns]-r1>0.01) &(np.fabs(direction-direction1)<0.04)] \n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p\n",
    "    #print ('\\r')\n",
    "#     df = df[['particle_id', 'weight', 'event_id', 'hit_id', 'track_id']]\n",
    "    df = df[['event_id', 'hit_id', 'track_id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_scales=[0.65, 0.965, 1.528]\n",
    "def _eliminate_outliers(labels,M):\n",
    "        norms=np.zeros((len(labels)),np.float32)\n",
    "        indices=np.zeros((len(labels)),np.float32)\n",
    "        for i, cluster in tqdm(enumerate(labels),total=len(labels)):\n",
    "            if cluster == 0:\n",
    "                continue\n",
    "            index = np.argwhere(self.clusters==cluster)\n",
    "            index = np.reshape(index,(index.shape[0]))\n",
    "            indices[i] = len(index)\n",
    "            x = M[index]\n",
    "            norms[i] = self._test_quadric(x)\n",
    "        threshold1 = np.percentile(norms,90)*5\n",
    "        threshold2 = 25\n",
    "        threshold3 = 6\n",
    "        for i, cluster in enumerate(labels):\n",
    "            if norms[i] > threshold1 or indices[i] > threshold2 or indices[i] < threshold3:\n",
    "                self.clusters[self.clusters==cluster]=0\n",
    "\n",
    "def _test_quadric(x):\n",
    "    if x.size == 0 or len(x.shape)<2:\n",
    "        return 0\n",
    "    Z = np.zeros((x.shape[0],10), np.float32)\n",
    "    Z[:,0] = x[:,0]**2\n",
    "    Z[:,1] = 2*x[:,0]*x[:,1]\n",
    "    Z[:,2] = 2*x[:,0]*x[:,2]\n",
    "    Z[:,3] = 2*x[:,0]\n",
    "    Z[:,4] = x[:,1]**2\n",
    "    Z[:,5] = 2*x[:,1]*x[:,2]\n",
    "    Z[:,6] = 2*x[:,1]\n",
    "    Z[:,7] = x[:,2]**2\n",
    "    Z[:,8] = 2*x[:,2]\n",
    "    Z[:,9] = 1\n",
    "    v, s, t = np.linalg.svd(Z,full_matrices=False)        \n",
    "    smallest_index = np.argmin(np.array(s))\n",
    "    T = np.array(t)\n",
    "    T = T[smallest_index,:]        \n",
    "    norm = np.linalg.norm(np.dot(Z,T), ord=2)**2\n",
    "    return norm\n",
    "\n",
    "def _preprocess(hits):\n",
    "    x = hits.x.values\n",
    "    y = hits.y.values\n",
    "    z = hits.z.values\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    hits['x2'] = x/r\n",
    "    hits['y2'] = y/r\n",
    "\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    hits['z2'] = z/r\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "    for i, rz_scale in enumerate(self.rz_scales):\n",
    "        X[:,i] = X[:,i] * rz_scale\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(submission,hits):\n",
    "    df = submission.merge(hits,  on=['hit_id'], how='left')\n",
    "#     df = submission.append(hits)\n",
    "#     print(df.head())\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "    for angle in range(-180,180,1):\n",
    "\n",
    "        print ('\\r %f'%angle, end='',flush=True)\n",
    "        #df1 = df.loc[(df.arctan2>(angle-0.5)/180*np.pi) & (df.arctan2<(angle+0.5)/180*np.pi)]\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "        min_num_neighbours = len(df1)\n",
    "        if min_num_neighbours<4: continue\n",
    "\n",
    "        hit_ids = df1.hit_id.values\n",
    "        x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "        r  = (x**2 + y**2)**0.5\n",
    "        r  = r/1000\n",
    "        a  = np.arctan2(y,x)\n",
    "        tree = KDTree(np.column_stack([a,r]), metric='euclidean')\n",
    "\n",
    "        track_ids = list(df1.track_id.unique())\n",
    "        num_track_ids = len(track_ids)\n",
    "        min_length=3\n",
    "\n",
    "        for i in range(num_track_ids):\n",
    "            p = track_ids[i]\n",
    "            if p==0: continue\n",
    "\n",
    "            idx = np.where(df1.track_id==p)[0]\n",
    "            if len(idx)<min_length: continue\n",
    "\n",
    "            if angle>0:\n",
    "                idx = idx[np.argsort( z[idx])]\n",
    "            else:\n",
    "                idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "\n",
    "            ## start and end points  ##\n",
    "            idx0,idx1 = idx[0],idx[-1]\n",
    "            a0 = a[idx0]\n",
    "            a1 = a[idx1]\n",
    "            r0 = r[idx0]\n",
    "            r1 = r[idx1]\n",
    "\n",
    "            da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "            dr0 = r[idx[1]] - r[idx[0]]\n",
    "            direction0 = np.arctan2(dr0,da0) \n",
    "\n",
    "            da1 = a[idx[-1]] - a[idx[-2]]\n",
    "            dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "            direction1 = np.arctan2(dr1,da1) \n",
    "\n",
    "\n",
    "            ## extend start point\n",
    "            ns = tree.query([[a0,r0]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "            direction = np.arctan2(r0-r[ns],a0-a[ns])\n",
    "            ns = ns[(r0-r[ns]>0.01) &(np.fabs(direction-direction0)<0.04)]\n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p \n",
    "\n",
    "            ## extend end point\n",
    "            ns = tree.query([[a1,r1]], k=min(20,min_num_neighbours), return_distance=False)\n",
    "            ns = np.concatenate(ns)\n",
    "\n",
    "            direction = np.arctan2(r[ns]-r1,a[ns]-a1)\n",
    "            ns = ns[(r[ns]-r1>0.01) &(np.fabs(direction-direction1)<0.04)] \n",
    "\n",
    "            for n in ns:\n",
    "                df.loc[ df.hit_id==hit_ids[n],'track_id' ] = p\n",
    "    #print ('\\r')\n",
    "#     df = df[['particle_id', 'weight', 'event_id', 'hit_id', 'track_id']]\n",
    "    df = df[['event_id', 'hit_id', 'track_id']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(cl1, cl2): # merge cluster 2 to cluster 1\n",
    "    d = pd.DataFrame(data={'s1':cl1,'s2':cl2})\n",
    "    d['N1'] = d.groupby('s1')['s1'].transform('count')\n",
    "    d['N2'] = d.groupby('s2')['s2'].transform('count')\n",
    "    maxs1 = d['s1'].max()\n",
    "    cond = np.where((d['N2'].values>d['N1'].values) & (d['N2'].values<25)) # tìm vị trí hit với nhit của cluster mới > nhits cluster cũ\n",
    "    s1 = d['s1'].values \n",
    "    s1[cond] = d['s2'].values[cond]+maxs1 # gán tất cả các hit đó thuộc về track mới (+maxs1 để tăng label cho track để nó khác ban đầu)\n",
    "    return s1\n",
    "\n",
    "def extract_good_hits(truth, submission):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    return tru[(tru.count_both > 0.5*tru.count_particle) & (tru.count_both > 0.5*tru.count_track)]\n",
    "\n",
    "def fast_score(good_hits_df):\n",
    "    return good_hits_df.weight.sum()\n",
    "\n",
    "\n",
    "def analyze_truth_perspective(truth, submission):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    good_hits = tru[(tru.count_both > 0.5*tru.count_particle) & (tru.count_both > 0.5*tru.count_track)]\n",
    "    score = good_hits.weight.sum()\n",
    "    \n",
    "    anatru = tru.particle_id.value_counts().value_counts().sort_index().to_frame().rename({'particle_id':'true_particle_counts'}, axis=1)\n",
    "    #anatru['true_particle_ratio'] = anatru['true_particle_counts'].values*100/np.sum(anatru['true_particle_counts'])\n",
    "\n",
    "    anatru['good_tracks_counts'] = np.zeros(len(anatru)).astype(int)\n",
    "    anatru['good_tracks_intersect_nhits_avg'] = np.zeros(len(anatru))\n",
    "    anatru['best_detect_intersect_nhits_avg'] = np.zeros(len(anatru))\n",
    "    for nhit in tqdm(range(4,20)):\n",
    "        particle_list  = tru[(tru.count_particle==nhit)].particle_id.unique()\n",
    "        intersect_count = 0\n",
    "        good_tracks_count = 0\n",
    "        good_tracks_intersect = 0\n",
    "        for p in particle_list:\n",
    "            nhit_intersect = tru[tru.particle_id==p].count_both.max()\n",
    "            intersect_count += nhit_intersect\n",
    "            corresponding_track = tru.loc[tru[tru.particle_id==p].count_both.idxmax()].track_id\n",
    "            leng_corresponding_track = len(tru[tru.track_id == corresponding_track])\n",
    "            \n",
    "            if (nhit_intersect >= nhit/2) and (nhit_intersect >= leng_corresponding_track/2):\n",
    "                good_tracks_count += 1\n",
    "                good_tracks_intersect += nhit_intersect\n",
    "        intersect_count = intersect_count/len(particle_list)\n",
    "        anatru.at[nhit,'best_detect_intersect_nhits_avg'] = intersect_count\n",
    "        anatru.at[nhit,'good_tracks_counts'] = good_tracks_count\n",
    "        if good_tracks_count > 0:\n",
    "            anatru.at[nhit,'good_tracks_intersect_nhits_avg'] = good_tracks_intersect/good_tracks_count\n",
    "    \n",
    "    return score, anatru, good_hits\n",
    "\n",
    "def precision(truth, submission,min_hits):\n",
    "    tru = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    tru['count_both'] = tru.groupby(['track_id', 'particle_id']).hit_id.transform('count')    \n",
    "    tru['count_particle'] = tru.groupby(['particle_id']).hit_id.transform('count')\n",
    "    tru['count_track'] = tru.groupby(['track_id']).hit_id.transform('count')\n",
    "    #print('Analyzing predictions...')\n",
    "    predicted_list  = tru[(tru.count_track>=min_hits)].track_id.unique()\n",
    "    good_tracks_count = 0\n",
    "    ghost_tracks_count = 0\n",
    "    fp_weights = 0\n",
    "    tp_weights = 0\n",
    "    for t in predicted_list:\n",
    "        nhit_track = tru[tru.track_id==t].count_track.iloc[0]\n",
    "        nhit_intersect = tru[tru.track_id==t].count_both.max()\n",
    "        corresponding_particle = tru.loc[tru[tru.track_id==t].count_both.idxmax()].particle_id\n",
    "        leng_corresponding_particle = len(tru[tru.particle_id == corresponding_particle])\n",
    "        if (nhit_intersect >= nhit_track/2) and (nhit_intersect >= leng_corresponding_particle/2): #if the predicted track is good\n",
    "            good_tracks_count += 1\n",
    "            tp_weights += tru[(tru.track_id==t)&(tru.particle_id==corresponding_particle)].weight.sum()\n",
    "            fp_weights += tru[(tru.track_id==t)&(tru.particle_id!=corresponding_particle)].weight.sum()\n",
    "        else: # if the predicted track is bad\n",
    "                ghost_tracks_count += 1\n",
    "                fp_weights += tru[(tru.track_id==t)].weight.sum()\n",
    "    all_weights = tru[(tru.count_track>=min_hits)].weight.sum()\n",
    "    precision = tp_weights/all_weights*100\n",
    "    print('Precision: ',precision,', good tracks:', good_tracks_count,', total tracks:',len(predicted_list),\n",
    "           ', loss:', fp_weights, ', reco:', tp_weights, 'reco/loss', tp_weights/fp_weights)\n",
    "    return precision\n",
    "\n",
    "\n",
    "class Clusterer(object):\n",
    "    def __init__(self):                        \n",
    "        self.abc = []\n",
    "          \n",
    "    def initialize(self,dfhits):\n",
    "        self.cluster = range(len(dfhits))\n",
    "        \n",
    "    def Hough_clustering(self,dfh,coef,epsilon,min_samples=1,n_loop=180,verbose=True): # [phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "        merged_cluster = self.cluster\n",
    "        mm = 1\n",
    "        stepii = 0.000005\n",
    "        count_ii = 0\n",
    "        adaptive_eps_coefficient = 1\n",
    "        \n",
    "        for ii in np.arange(0, n_loop*stepii, stepii):\n",
    "            count_ii += 1\n",
    "            for jj in range(2):\n",
    "                mm = mm*(-1)\n",
    "                eps_new = epsilon + count_ii*adaptive_eps_coefficient*10**(-5)\n",
    "                dfh['a1'] = dfh['a0'].values - np.nan_to_num(np.arccos(mm*ii*dfh['rt'].values))\n",
    "                dfh['sina1'] = np.sin(dfh['a1'].values)\n",
    "                dfh['cosa1'] = np.cos(dfh['a1'].values)\n",
    "                dfh['ia1'] = np.nan_to_num(((mm*ii*dfh['rt'].values) * np.nan_to_num(np.arccos(mm*ii*dfh['rt'].values))) - (np.sqrt(1-(mm*ii*dfh['rt'].values)**2)))\n",
    "                ss = StandardScaler()\n",
    "                dfs = ss.fit_transform(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr','ia1', 'fundu', 'mom2']].values) \n",
    "                #dfs = scale_ignore_nan(dfh[['sina1','cosa1','zdivrt','zdivr','xdivr','ydivr']])\n",
    "                dfs = np.multiply(dfs, coef)\n",
    "                new_cluster=DBSCAN(eps=eps_new,min_samples=min_samples,metric='euclidean',n_jobs=4).fit(dfs).labels_\n",
    "                merged_cluster = merge(merged_cluster, new_cluster)\n",
    "                if verbose == True:\n",
    "                    sub = create_one_event_submission(0, hits, merged_cluster)\n",
    "                    good_hits = extract_good_hits(truth, sub)\n",
    "                    score_1 = fast_score(good_hits)\n",
    "#                     print('2r0_inverse:', ii*mm ,'. Score:', score_1)\n",
    "                    #clear_output(wait=True)\n",
    "        self.cluster = merged_cluster\n",
    "\n",
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission  \n",
    "\n",
    "def preprocess_hits(h,dz, theta0):\n",
    "    y = h['y'].values\n",
    "    x = h['x'].values\n",
    "    h['zz'] =  h['z'].values + dz\n",
    "    z = h['zz'].values\n",
    "    h['r'] = np.sqrt(h['x'].values**2+h['y'].values**2+h['zz'].values**2)\n",
    "#     r = h['r'].values\n",
    "    h['rt'] = np.sqrt(h['x'].values**2+h['y'].values**2)\n",
    "    R = h['r'].values\n",
    "    h['a0'] = np.arctan2(h['y'].values,h['x'].values)\n",
    "    h['zdivrt'] = h['zz'].values/h['rt'].values\n",
    "    h['zdivr'] = h['zz'].values/h['r'].values\n",
    "    h['xdivr'] = h['x'].values / h['r'].values\n",
    "    h['ydivr'] = h['y'].values / h['r'].values\n",
    "    h['fundu'] = np.arcsin((y * np.sin(theta0) - x * np.cos(theta0)) / R) / (z - dz)\n",
    "#     h['mom2'] = np.sqrt(1 + ((z/R)**2)).round(1)\n",
    "    zR = z/R\n",
    "    h['mom2'] = [truncate(np.sqrt(1 + (i**2)),4) for i in zR]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hits():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "for i,event_id in enumerate(event_ids):\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "    hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "    cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "    truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "    particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "\n",
    "    truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hits():\n",
    "\n",
    "data_dir = '../data/train'\n",
    "\n",
    "#     event_ids = [\n",
    "#             '000001030',##\n",
    "#             '000001025','000001026','000001027','000001028','000001029',\n",
    "#     ]\n",
    "\n",
    "event_ids = [\n",
    "        '000001030',##\n",
    "\n",
    "]\n",
    "\n",
    "sum=0\n",
    "sum_score=0\n",
    "def get_hits():\n",
    "    for i,event_id in enumerate(event_ids):\n",
    "        particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "        hits  = pd.read_csv(data_dir + '/event%s-hits.csv'%event_id)\n",
    "        cells = pd.read_csv(data_dir + '/event%s-cells.csv'%event_id)\n",
    "        truth = pd.read_csv(data_dir + '/event%s-truth.csv'%event_id)\n",
    "        particles = pd.read_csv(data_dir + '/event%s-particles.csv'%event_id)\n",
    "\n",
    "        truth = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits = pd.merge(hits, truth, how='left', on='hit_id')\n",
    "        return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w1 = [0.9, 1.2]\n",
    "w2 = [0.3, 0.7]\n",
    "w3 = [0.1, 0.4]\n",
    "Niter = [140, 190]\n",
    "\n",
    "\n",
    "#     bo = BayesianOptimization(Fun4BO,pbounds = {'w1':w1,'w2':w2,'w3':w3,'Niter':Niter})\n",
    "#     bo.maximize(init_points = 3, n_iter = 20, acq = \"ucb\", kappa = 2.576)\n",
    "w1 = 1.1932215111905984\n",
    "w2 = 0.39740553885387364\n",
    "w3 = 0.3512647720585538\n",
    "w4 = 0.1470\n",
    "w5 = 0.2691\n",
    "w6 = 0.0020\n",
    "w6 = [0.0000001, 1.2] \n",
    "w10 = [0.00001, 1.2] \n",
    "w12 = [0.00001, 0.1] \n",
    "Niter = 179\n",
    "w12 = 0.00045\n",
    "# Fun4BO21(w12)\n",
    "#     for w12 in np.arange(0.0002, 0.003, 0.00005):\n",
    "#         print(w12)\n",
    "#         Fun4BO21(w12)\n",
    "#     bo = BayesianOptimization(Fun4BO21,pbounds = {'w12':w12})\n",
    "#     bo.maximize(init_points = 20, n_iter = 5, acq = \"ucb\", kappa = 2.576)\n",
    "\n",
    "# z1+z2: 13 | 05m39s |    0.55616 |    0.1124 | \n",
    "# z1 * z2: 13 | 05m40s |    0.55637 |    0.0404 | \n",
    "# tt = cos(theta), theta = np.arctan2(rt,z): 4 | 06m01s |    0.55196 |    0.2711 | \n",
    "# tt, theta = np.arctan2(rt2,z2);  8 | 05m39s |    0.55604 |    0.0005 | \n",
    "# cos + sin: 15 | 05m39s |    0.55714 |    0.2691 | \n",
    "# cos-sin: 9 | 05m51s |    0.55714 |    0.0020 | \n",
    "# cos+sin/cos-sin: 8 | 06m03s |    0.55694 |    0.0012 | \n",
    "# ctt, stt, ctt+stt/ctt-stt: 8 | 06m03s |    0.55273 |    0.0000 | \n",
    "# ctt: 10 | 05m39s |    0.55613 |    0.0047 | \n",
    "# caa * stt: 7 | 05m38s |    0.55613 |    0.0022 |\n",
    "# saa * ctt: 15 | 05m38s |    0.55613 |    0.0033 |\n",
    "# c1 = caa*stt, c2 = saa*ctt, c1+c2: 10 | 05m43s |    0.55622 |    0.0571 | \n",
    "# caa * saa: 7 | 05m45s |    0.55639 |    0.1548 | \n",
    "# caa/saa: 13 | 05m43s |    0.55613 |    0.000001 | \n",
    "# xx1, xx2: 12 | 06m04s |    0.55613 |    0.000001 |    0.000001 | \n",
    "\n",
    "# hdbscan - braycurtis: 0.14424123982821765\n",
    "# hdbscan - euclidean : 0.1311\n",
    "\n",
    "# eps: 0.0040: 0.57549\n",
    "# tt: 21 | 07m08s |    0.57208 |    0.0000 |  (1e-07)\n",
    "# ctt, stt (after new eqn: 25 | 07m33s |    0.57254 |    0.0026 | \n",
    "# ctt, stt: 0.00261: 0.5727074941034839\n",
    "# caa, saa, z1, z2, rt/r, x/r, z3, ctt, stt, y1, y3:\n",
    "# w1 = 1.1932215111905984, w2 = 0.39740553885387364, w3 = 0.3512647720585538, w4 = 0.1470, w5 = 0.01201\n",
    "# w6 = 0.0205, w7 = 0.00261, w8 = 0.0049, w9 = 0.0012 (0.5717942069958433)\n",
    "# \n",
    "# X = StandardScaler().fit_transform(np.column_stack([caa, saa, z1, z2, rt/r, x/r, y/r, z3, y1, y3])):\n",
    "# w1 = 1.1932215111905984 w2 = 0.39740553885387364 w3 = 0.3512647720585538 w4 = 0.1470 w5 = 0.01201 w6 = 0.0003864 \n",
    "# w7 = 0.0205 w8 = 0.0049 w9 = 0.00121 (0.57343)\n",
    "# ctt, stt: 13 | 08m16s |    0.57343 |    0.0003 | (0.00032967312140735677)\n",
    "# ctt, stt: 21 | 08m20s |    0.57343 |    0.0000 |  (1.4930496676654575e-05)\n",
    "# ctt, stt: 15 | 08m14s |    0.57351 |    0.8435 | (t1 = theta+mm*(rt+ w11*rt**2)/1000*(ii/2)/180*np.pi)\n",
    "# z4: 0.0245 (0.5735925042985041)\n",
    "# # z4 0.0318 (0.5736635664313068)\n",
    "# x4: 0.00001 (0.5735421714482896)\n",
    "# x4: 0.00025 (0.5736999491677117)\n",
    "# x4: 0.00045 (0.5737240529228144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering by varying \n",
    "#model = Clusterer()\n",
    "#model.initialize(hits) \n",
    "# c = [1.5,1.5,0.73,0.17,0.027,0.027, 1.] #[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "# min_samples_in_cluster = 1\n",
    "\n",
    "# model = Clusterer()\n",
    "# model.initialize(hits) \n",
    "# hits_with_dz = preprocess_hits(hits, 0)\n",
    "# model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "#                        n_loop=370,verbose=True)\n",
    "\n",
    "# submission = create_one_event_submission(0, hits, model.cluster)\n",
    "# print('\\n')\n",
    "\n",
    "# score = score_event(truth, submission)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(labels):\n",
    "    _,reverse,count = np.unique(labels,return_counts=True,return_inverse=True)\n",
    "    counts = count[reverse]\n",
    "    counts[labels==0]=0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def Fun4BO11(w):\n",
    "    hits = get_hits()\n",
    "    print(w)\n",
    "#     c = [w1,w1,w2,w3,w4,w5]\n",
    "    dzs = list(np.linspace(-5.5, 5.5, 10))\n",
    "    t = list(np.linspace(-np.pi, np.pi, 12))\n",
    "    ls = []\n",
    "    dz = 0\n",
    "#     for dz in dzs:\n",
    "    \n",
    "    c = [1.77,1.77,0.734,0.14,0.0195,0.025,0.0008,0.0001,w]\n",
    "    model = Clusterer()\n",
    "    model.initialize(hits) \n",
    "    hits_with_dz = preprocess_hits(hits, dz, 0)\n",
    "    min_samples_in_cluster = 1\n",
    "    model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                   n_loop=370,verbose=True)\n",
    "\n",
    "    ls.append(model.cluster)\n",
    "    return model.cluster\n",
    "    num_hits=len(hits)\n",
    "    labels = np.zeros(num_hits,np.int32)\n",
    "    counts = np.zeros(num_hits,np.int32)\n",
    "    lss = []\n",
    "    for l in ls:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels[idx] = l[idx] + labels.max()\n",
    "        counts = make_counts(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fun4BO11() missing 1 required positional argument: 'w'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fun4BO11() missing 1 required positional argument: 'w'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels = Fun4BO11()\n",
    "    \n",
    "submission = create_one_event_submission(0, hits, labels)\n",
    "print('-------------------------')\n",
    "\n",
    "score = score_event(truth, submission)\n",
    "print(score)\n",
    "\n",
    "# 20 z_shift: 0.5992258787927022, with extn: 0.6252801561124434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179.0000000[ 0] score : 0.62077650\n",
      " 179.0000000[ 1] score : 0.62456689\n",
      " 179.0000000[ 2] score : 0.62482064\n",
      " 179.0000000[ 3] score : 0.62586284\n",
      " 179.0000000[ 4] score : 0.62595311\n",
      " 179.0000000[ 5] score : 0.62689351\n",
      " 179.0000000[ 6] score : 0.62637153\n",
      " 179.0000000[ 7] score : 0.62699622\n",
      "--------------------------------------\n",
      "0.6252801561124434\n"
     ]
    }
   ],
   "source": [
    "sum_score=0\n",
    "sum = 0\n",
    "submission = pd.DataFrame(columns=['event_id', 'hit_id', 'track_id'],\n",
    "    data=np.column_stack(([int(event_id),]*len(hits), hits.hit_id.values, labels))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    submission = extend(submission,hits)\n",
    "    score = score_event(truth, submission)\n",
    "    print('[%2d] score : %0.8f'%(i, score))\n",
    "    sum_score += score\n",
    "    sum += 1\n",
    "\n",
    "print('--------------------------------------')\n",
    "sc = sum_score/sum\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "-------------------------\n",
      "0.40914053000293865\n",
      "0.1\n",
      "0.1\n",
      "-------------------------\n",
      "0.5504652364349819\n",
      "0.01\n",
      "0.01\n",
      "-------------------------\n",
      "0.556515761900757\n",
      "0.001\n",
      "0.001\n",
      "-------------------------\n",
      "0.5560161334686287\n",
      "0.0001\n",
      "0.0001\n",
      "-------------------------\n",
      "0.556040408488732\n",
      "1e-05\n",
      "1e-05\n",
      "-------------------------\n",
      "0.556040408488732\n"
     ]
    }
   ],
   "source": [
    "for w in [1., 0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    print(w)\n",
    "    labels = Fun4BO11(w)\n",
    "    \n",
    "    submission = create_one_event_submission(0, hits, labels)\n",
    "    print('-------------------------')\n",
    "\n",
    "    score = score_event(truth, submission)\n",
    "    print(score)\n",
    "# w is derivative of arccos\n",
    "# 0.01: 0.48149631563117423\n",
    "# 0.0001: 0.5546588896528468\n",
    "# 1e-05: 0.5548408110536218\n",
    "# w is integral\n",
    "# w: 0.001, 0.5551303348248551, 0.0001: 0.5547\n",
    "# w is fundu, 0.0001: 0.556040408488732\n",
    "# w mom2 with round1: 1.0, 0.1: 0.5491515754493856, 0.001: 0.5559538615783635\n",
    "# w mom2 with trunc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = [1.5,1.5,0.73,0.17,0.027,0.027] #[phi_coef,phi_coef,zdivrt_coef,zdivr_coef,xdivr_coef,ydivr_coef]\n",
    "# min_samples_in_cluster = 1\n",
    "\n",
    "# model = Clusterer()\n",
    "# model.initialize(hits) \n",
    "# hits_with_dz = preprocess_hits(hits, 0)\n",
    "# model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "#                        n_loop=1000,verbose=True)\n",
    "\n",
    "# submission = create_one_event_submission(0, hits, model.cluster)\n",
    "# print('\\n')\n",
    "\n",
    "# score = score_event(truth, submission)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fun4BO(z_shift):\n",
    "#     hits = get_hits()\n",
    "    print(n_loop)\n",
    "#     c = [w1,w1,w2,w3,w4,w5]\n",
    "    c = [1.77,1.77,0.734,0.14,0.0195,0.025]\n",
    "    model = Clusterer()\n",
    "    model.initialize(hits) \n",
    "    hits_with_dz = preprocess_hits(hits, z_shift)\n",
    "    min_samples_in_cluster = 1\n",
    "    model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                       n_loop=370,verbose=True)\n",
    "\n",
    "    submission = create_one_event_submission(0, hits, model.cluster)\n",
    "    print('-------------------------')\n",
    "\n",
    "    score = score_event(truth, submission)\n",
    "    print(score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = [0.001, 3.2] \n",
    "# w2 = [0.001, 3.2] \n",
    "# w3 = [0.001, 3.2] \n",
    "# w4 = [0.001, 3.2] \n",
    "# w5 = [0.001, 3.2] \n",
    "# w6 = [0.001, 3.2] \n",
    "\n",
    "# bo = BayesianOptimization(Fun4BO,pbounds = {'w1':w1, 'w2':w2, 'w3':w3, 'w4': w4, 'w5':w5})\n",
    "# bo.maximize(init_points = 40, n_iter = 5, acq = \"ucb\", kappa = 2.576)\n",
    "\n",
    "# for n_loop in np.arange(300, 460, 10):\n",
    "#     print(n_loop)\n",
    "#     Fun4BO(n_loop)\n",
    "# w1: 1.77 (0.5450052751017225)\n",
    "# w2: 0.734: 0.5453310726331103\n",
    "# w3: 0.14: 0.5457328790148219 \n",
    "# w4: 0.0195: 0.5493144276000794\n",
    "# w5: 0.025: 0.5517975288706574\n",
    "# n_loop: 370: 0.5548408110536218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fun4BO1(z_shift):\n",
    "#     hits = get_hits()\n",
    "    print(z_shift)\n",
    "#     c = [w1,w1,w2,w3,w4,w5]\n",
    "    c = [1.77,1.77,0.734,0.14,0.0195,0.025]\n",
    "    model = Clusterer()\n",
    "    model.initialize(hits) \n",
    "    hits_with_dz = preprocess_hits(hits, z_shift)\n",
    "    min_samples_in_cluster = 1\n",
    "    model.Hough_clustering(hits_with_dz,coef=c,epsilon=0.0048,min_samples=min_samples_in_cluster,\n",
    "                       n_loop=370,verbose=True)\n",
    "\n",
    "    return model.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-------------------------\n",
      "0.5548408110536218\n",
      "-5.5\n",
      "-5.5\n",
      "-------------------------\n",
      "0.4979654839813329\n",
      "-5.49\n",
      "-5.49\n",
      "-------------------------\n",
      "0.4137679016526512\n",
      "-5.48\n",
      "-5.48\n",
      "-------------------------\n",
      "0.3341160065633342\n",
      "-5.470000000000001\n",
      "-5.470000000000001\n",
      "-------------------------\n",
      "0.24724009794324278\n",
      "-5.460000000000001\n",
      "-5.460000000000001\n",
      "-------------------------\n",
      "0.16524925737396182\n",
      "-5.450000000000001\n",
      "-5.450000000000001\n",
      "-------------------------\n",
      "0.11265024162989001\n",
      "-5.440000000000001\n",
      "-5.440000000000001\n",
      "-------------------------\n",
      "0.07752435424025374\n",
      "-5.4300000000000015\n",
      "-5.4300000000000015\n",
      "-------------------------\n",
      "0.05259905731407198\n",
      "-5.420000000000002\n",
      "-5.420000000000002\n",
      "-------------------------\n",
      "0.03663954557608446\n",
      "-5.410000000000002\n",
      "-5.410000000000002\n",
      "-------------------------\n",
      "0.02543864330836862\n",
      "-5.400000000000002\n",
      "-5.400000000000002\n",
      "-------------------------\n",
      "0.017786123775768888\n",
      "-5.390000000000002\n",
      "-5.390000000000002\n",
      "-------------------------\n",
      "0.014817209413121311\n",
      "-5.380000000000003\n",
      "-5.380000000000003\n",
      "-------------------------\n",
      "0.010809243256047374\n",
      "-5.370000000000003\n",
      "-5.370000000000003\n",
      "-------------------------\n",
      "0.00810113849451085\n",
      "-5.360000000000003\n",
      "-5.360000000000003\n",
      "-------------------------\n",
      "0.005897878345124962\n",
      "-5.350000000000003\n",
      "-5.350000000000003\n",
      "-------------------------\n",
      "0.006623764158217235\n",
      "-5.340000000000003\n",
      "-5.340000000000003\n",
      "-------------------------\n",
      "0.004769915620319841\n",
      "-5.330000000000004\n",
      "-5.330000000000004\n",
      "-------------------------\n",
      "0.003859046786439539\n",
      "-5.320000000000004\n",
      "-5.320000000000004\n",
      "-------------------------\n",
      "0.003404480384503086\n",
      "-5.310000000000004\n",
      "-5.310000000000004\n",
      "-------------------------\n",
      "0.0023606889500565344\n",
      "-5.300000000000004\n",
      "-5.300000000000004\n",
      "-------------------------\n",
      "0.0023025202098087357\n",
      "-5.2900000000000045\n",
      "-5.2900000000000045\n",
      "-------------------------\n",
      "0.0018098301977098766\n",
      "-5.280000000000005\n",
      "-5.280000000000005\n",
      "-------------------------\n",
      "0.0022710583796747086\n",
      "-5.270000000000005\n",
      "-5.270000000000005\n",
      "-------------------------\n",
      "0.0018101731977113375\n",
      "-5.260000000000005\n",
      "-5.260000000000005\n",
      "-------------------------\n",
      "0.001865081967945249\n",
      "-5.250000000000005\n",
      "-5.250000000000005\n",
      "-------------------------\n",
      "0.00163025123694487\n",
      "-5.2400000000000055\n",
      "-5.2400000000000055\n",
      "-------------------------\n",
      "0.0017737138075560204\n",
      "-5.230000000000006\n",
      "-5.230000000000006\n",
      "-------------------------\n",
      "0.001483062236317845\n",
      "-5.220000000000006\n",
      "-5.220000000000006\n",
      "-------------------------\n",
      "0.0017985207776616986\n",
      "-5.210000000000006\n",
      "-5.210000000000006\n",
      "-------------------------\n",
      "0.0018145287577298924\n",
      "-5.200000000000006\n",
      "-5.200000000000006\n",
      "-------------------------\n",
      "0.0015028583664021765\n",
      "-5.190000000000007\n",
      "-5.190000000000007\n",
      "-------------------------\n",
      "0.0015157800764572231\n",
      "-5.180000000000007\n",
      "-5.180000000000007\n",
      "-------------------------\n",
      "0.00157665494671655\n",
      "-5.170000000000007\n",
      "-5.170000000000007\n",
      "-------------------------\n",
      "0.001512000706441123\n",
      "-5.160000000000007\n",
      "-5.160000000000007\n",
      "-------------------------\n",
      "0.0014287686360865543\n",
      "-5.1500000000000075\n",
      "-5.1500000000000075\n",
      "-------------------------\n",
      "0.0013353187656884578\n",
      "-5.140000000000008\n",
      "-5.140000000000008\n",
      "-------------------------\n",
      "0.00161203055686725\n",
      "-5.130000000000008\n",
      "-5.130000000000008\n",
      "-------------------------\n",
      "0.0014344002661105448\n",
      "-5.120000000000008\n",
      "-5.120000000000008\n",
      "-------------------------\n",
      "0.0013215736856299037\n",
      "-5.110000000000008\n",
      "-5.110000000000008\n",
      "-------------------------\n",
      "0.001285905865477959\n",
      "-5.1000000000000085\n",
      "-5.1000000000000085\n",
      "-------------------------\n",
      "0.0013096526455791202\n",
      "-5.090000000000009\n",
      "-5.090000000000009\n",
      "-------------------------\n",
      "0.0013140234855977399\n",
      "-5.080000000000009\n",
      "-5.080000000000009\n",
      "-------------------------\n",
      "0.0012113017151601452\n",
      "-5.070000000000009\n",
      "-5.070000000000009\n",
      "-------------------------\n",
      "0.001305434745561152\n",
      "-5.060000000000009\n",
      "-5.060000000000009\n",
      "-------------------------\n",
      "0.0011500874248993724\n",
      "-5.05000000000001\n",
      "-5.05000000000001\n",
      "-------------------------\n",
      "0.0011486562248932753\n",
      "-5.04000000000001\n",
      "-5.04000000000001\n",
      "-------------------------\n",
      "0.0013527917257628924\n",
      "-5.03000000000001\n",
      "-5.03000000000001\n",
      "-------------------------\n",
      "0.0011885522850632326\n",
      "-5.02000000000001\n",
      "-5.02000000000001\n",
      "-------------------------\n",
      "0.0010222425843547532\n",
      "-5.0100000000000104\n",
      "-5.0100000000000104\n",
      "-------------------------\n",
      "0.0011939939250864141\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "\n",
    "l = Fun4BO1(0)\n",
    "ls.append(l)\n",
    "\n",
    "submission = create_one_event_submission(0, hits, l)\n",
    "print('-------------------------')\n",
    "\n",
    "score = score_event(truth, submission)\n",
    "print(score)\n",
    "    \n",
    "for z_shift in np.arange(-5.5, -5.0, 0.01):\n",
    "    print(z_shift)\n",
    "    l = Fun4BO1(z_shift)\n",
    "    ls.append(l)\n",
    "    \n",
    "    submission = create_one_event_submission(0, hits, l)\n",
    "    print('-------------------------')\n",
    "\n",
    "    score = score_event(truth, submission)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "def shift(l, n):\n",
    "    return l[n:] + l[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "-------------------------\n",
      "0.003404480384503086\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.003477309274813337\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.008167709524794441\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.09337532294777887\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.08776587767388264\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.08342650437539689\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.08240054602102631\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.07966614256937776\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.076529576216016\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.07606311400402886\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.48944954441505506\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.48044408712669173\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4802140455257118\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4705229772644278\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.46263888044084167\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.456371384074142\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.44879866510188227\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4425283377151707\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.43669056503030174\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.44695245708401743\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4372868422428419\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.43137942665767626\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.42587694873423576\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4221313046882793\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.41691711275606685\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4145299931358978\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4143914019453073\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.41070966150962307\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.40757509029626987\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.40714325038443017\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4037476490299649\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4023479890240024\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4013648936698144\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39928354747094785\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39757015206364876\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3958736495664217\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3939364083781691\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39376760238744996\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3934814140162307\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39194686959969366\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3902642117825255\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3892936207683908\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.41193410041483924\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.4055240640475325\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.40021782066492784\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39739321067289507\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3947369927415796\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.39177844670897616\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.3900792165517374\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.5004033367417182\n",
      "-------------------------\n",
      "-------------------------\n",
      "0.485189209696906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = np.zeros(num_hits,np.int32)\n",
    "counts = np.zeros(num_hits,np.int32)\n",
    "\n",
    "num_hits = len(hits)\n",
    "lss = []\n",
    "for i in range(len(ls)):\n",
    "    labels1 = np.zeros(num_hits,np.int32)\n",
    "    counts1 = np.zeros(num_hits,np.int32)\n",
    "    ls1 = ls.copy()\n",
    "    ls1 = shift(ls1, 1)\n",
    "    np.random.shuffle(ls1)\n",
    "    for l in ls1:\n",
    "        c = make_counts(l)\n",
    "        idx = np.where((c-counts>0) & (c<20))[0]\n",
    "        labels1[idx] = l[idx] + labels1.max()\n",
    "        counts1 = make_counts(labels1)\n",
    "    l1 = labels1.copy()\n",
    "    lss.append(l1)\n",
    "\n",
    "\n",
    "labels = np.zeros(num_hits,np.int32)\n",
    "counts = np.zeros(num_hits,np.int32)\n",
    "\n",
    "for l in lss:\n",
    "    c = make_counts(l)\n",
    "    idx = np.where((c-counts>0) & (c<20))[0]\n",
    "    labels[idx] = l[idx] + labels.max()\n",
    "    counts = make_counts(labels)\n",
    "    \n",
    "    submission = create_one_event_submission(0, hits, labels)\n",
    "    print('-------------------------')\n",
    "    print('-------------------------')\n",
    "\n",
    "    score = score_event(truth, submission)\n",
    "    print(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "-------------------------\n",
      "0.485189209696906\n"
     ]
    }
   ],
   "source": [
    "submission = create_one_event_submission(0, hits, labels)\n",
    "print('-------------------------')\n",
    "print('-------------------------')\n",
    "\n",
    "score = score_event(truth, submission)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmlf",
   "language": "python",
   "name": "tmlf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
