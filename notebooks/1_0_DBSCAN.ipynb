{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.score import score_event\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer(object):\n",
    "    \n",
    "    def __init__(self, eps):\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def _preprocess(self, hits):\n",
    "        \n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        hits['x2'] = x/r\n",
    "        hits['y2'] = y/r\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = z/r\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def predict(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        \n",
    "        cl = DBSCAN(eps=self.eps, min_samples=1, algorithm='kd_tree')\n",
    "        labels = cl.fit_predict(X)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer2(object):\n",
    "    \n",
    "    def __init__(self, eps):\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def _preprocess(self, hits):\n",
    "        \n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "        \n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        \n",
    "        hits['x2'] = x+(x/r)\n",
    "        hits['y2'] = y+(y/r)\n",
    "\n",
    "#         r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = z+(z/r)\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def predict(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        \n",
    "        cl = DBSCAN(eps=self.eps, min_samples=1, algorithm='kd_tree')\n",
    "        labels = cl.fit_predict(X)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer3(object):\n",
    "    \n",
    "    def __init__(self, eps):\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def _preprocess(self, hits):\n",
    "        \n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "        \n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        \n",
    "        hits['x2'] = (x/r)\n",
    "        hits['y2'] = (y/r)\n",
    "\n",
    "#         r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = (z/r)\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def predict(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        \n",
    "        cl = DBSCAN(eps=self.eps, min_samples=1, algorithm='auto')\n",
    "        labels = cl.fit_predict(X)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer1(object):\n",
    "    \n",
    "    def __init__(self, eps):\n",
    "        self.eps = eps\n",
    "        self.clf = DBSCAN(eps=self.eps, min_samples=1, algorithm='kd_tree')\n",
    "    \n",
    "    def _preprocess(self, hits):\n",
    "        \n",
    "        x = hits.x.values\n",
    "        y = hits.y.values\n",
    "        z = hits.z.values\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2 + z**2)\n",
    "        hits['x2'] = x/r\n",
    "        hits['y2'] = y/r\n",
    "\n",
    "        r = np.sqrt(x**2 + y**2)\n",
    "        hits['z2'] = z/r\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(hits[['x2', 'y2', 'z2']].values)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, hits):\n",
    "        X = self._preprocess(hits)\n",
    "        self.clf.fit(hits)\n",
    "        \n",
    "    def predict_test(self, hits):\n",
    "        X_test = self._preprocess(hits)\n",
    "        labels = self.clf.predict(X_test)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def predict(self, hits):\n",
    "        \n",
    "        X = self._preprocess(hits)\n",
    "        \n",
    "        cl = DBSCAN(eps=self.eps, min_samples=1, algorithm='kd_tree')\n",
    "        labels = cl.fit_predict(X)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event ID:  0\n",
      "Event ID:  1\n",
      "Event ID:  2\n",
      "Event ID:  3\n",
      "Event ID:  4\n",
      "Event ID:  5\n",
      "Event ID:  6\n",
      "Event ID:  7\n",
      "Event ID:  8\n",
      "Event ID:  9\n",
      "Event ID:  10\n",
      "Event ID:  11\n",
      "Event ID:  12\n",
      "Event ID:  13\n",
      "Event ID:  14\n",
      "Event ID:  15\n",
      "Event ID:  16\n",
      "Event ID:  17\n",
      "Event ID:  18\n",
      "Event ID:  19\n",
      "Event ID:  20\n",
      "Event ID:  21\n",
      "Event ID:  22\n",
      "Event ID:  23\n",
      "Event ID:  24\n",
      "Event ID:  25\n",
      "Event ID:  26\n",
      "Event ID:  27\n",
      "Event ID:  28\n",
      "Event ID:  29\n",
      "Event ID:  30\n",
      "Event ID:  31\n",
      "Event ID:  32\n",
      "Event ID:  33\n",
      "Event ID:  34\n",
      "Event ID:  35\n",
      "Event ID:  36\n",
      "Event ID:  37\n",
      "Event ID:  38\n",
      "Event ID:  39\n",
      "Event ID:  40\n",
      "Event ID:  41\n",
      "Event ID:  42\n",
      "Event ID:  43\n",
      "Event ID:  44\n",
      "Event ID:  45\n",
      "Event ID:  46\n",
      "Event ID:  47\n",
      "Event ID:  48\n",
      "Event ID:  49\n",
      "Event ID:  50\n",
      "Event ID:  51\n",
      "Event ID:  52\n",
      "Event ID:  53\n",
      "Event ID:  54\n",
      "Event ID:  55\n",
      "Event ID:  56\n",
      "Event ID:  57\n",
      "Event ID:  58\n",
      "Event ID:  59\n",
      "Event ID:  60\n",
      "Event ID:  61\n",
      "Event ID:  62\n",
      "Event ID:  63\n",
      "Event ID:  64\n",
      "Event ID:  65\n",
      "Event ID:  66\n",
      "Event ID:  67\n",
      "Event ID:  68\n",
      "Event ID:  69\n",
      "Event ID:  70\n",
      "Event ID:  71\n",
      "Event ID:  72\n",
      "Event ID:  73\n",
      "Event ID:  74\n",
      "Event ID:  75\n",
      "Event ID:  76\n",
      "Event ID:  77\n",
      "Event ID:  78\n",
      "Event ID:  79\n",
      "Event ID:  80\n",
      "Event ID:  81\n",
      "Event ID:  82\n",
      "Event ID:  83\n",
      "Event ID:  84\n",
      "Event ID:  85\n",
      "Event ID:  86\n",
      "Event ID:  87\n",
      "Event ID:  88\n",
      "Event ID:  89\n",
      "Event ID:  90\n",
      "Event ID:  91\n",
      "Event ID:  92\n",
      "Event ID:  93\n",
      "Event ID:  94\n",
      "Event ID:  95\n",
      "Event ID:  96\n",
      "Event ID:  97\n",
      "Event ID:  98\n",
      "Event ID:  99\n",
      "Event ID:  100\n",
      "Event ID:  101\n",
      "Event ID:  102\n",
      "Event ID:  103\n",
      "Event ID:  104\n",
      "Event ID:  105\n",
      "Event ID:  106\n",
      "Event ID:  107\n",
      "Event ID:  108\n",
      "Event ID:  109\n",
      "Event ID:  110\n",
      "Event ID:  111\n",
      "Event ID:  112\n",
      "Event ID:  113\n",
      "Event ID:  114\n",
      "Event ID:  115\n",
      "Event ID:  116\n",
      "Event ID:  117\n",
      "Event ID:  118\n",
      "Event ID:  119\n",
      "Event ID:  120\n",
      "Event ID:  121\n",
      "Event ID:  122\n",
      "Event ID:  123\n",
      "Event ID:  124\n"
     ]
    }
   ],
   "source": [
    "# this scores 0.2078 on the public LB\n",
    "path_to_test = \"../data/test\"\n",
    "test_dataset_submissions = []\n",
    "\n",
    "create_submission = True # True for submission \n",
    "\n",
    "if create_submission:\n",
    "    for event_id, hits, cells in load_dataset(path_to_test, parts=['hits', 'cells']):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=0.008)\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        test_dataset_submissions.append(one_submission)\n",
    "        \n",
    "        print('Event ID: ', event_id)\n",
    "\n",
    "    # Create submission file\n",
    "    submussion = pd.concat(test_dataset_submissions, axis=0)\n",
    "    IDENTIFIER = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    submussion.to_csv('../submissions/sub-DBSCAN-sample-{}.csv.gz'.format(IDENTIFIER), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event ID: 1000, Score: 0.1988556913647304\n",
      "Event ID: 1001, Score: 0.2041585884317101\n",
      "Event ID: 1002, Score: 0.18318435313069215\n",
      "Event ID: 1003, Score: 0.21273428168683847\n",
      "Event ID: 1004, Score: 0.19506492906816675\n",
      "Event ID: 1005, Score: 0.19742897826381522\n",
      "Event ID: 1006, Score: 0.19686337989951777\n",
      "Event ID: 1007, Score: 0.2044755523190459\n",
      "Event ID: 1008, Score: 0.20589788785782545\n",
      "Event ID: 1009, Score: 0.20768824783976925\n",
      "Event ID: 1010, Score: 0.20025412138170395\n",
      "Event ID: 1011, Score: 0.19694834768548108\n",
      "Event ID: 1012, Score: 0.2111250609405797\n",
      "Event ID: 1013, Score: 0.21081737736211192\n",
      "Event ID: 1014, Score: 0.18935862442764448\n",
      "Event ID: 1015, Score: 0.19712979797213848\n",
      "Event ID: 1016, Score: 0.20185737963029127\n",
      "Event ID: 1017, Score: 0.18970268605903584\n",
      "Event ID: 1018, Score: 0.21665023512426124\n",
      "Event ID: 1019, Score: 0.18462860835130326\n",
      "Event ID: 1020, Score: 0.21833513864549936\n",
      "Event ID: 1021, Score: 0.1999660563467387\n",
      "Event ID: 1022, Score: 0.20446955745160267\n",
      "Event ID: 1023, Score: 0.20102918945317846\n",
      "Event ID: 1024, Score: 0.20169663430235119\n",
      "Event ID: 1025, Score: 0.18046792188147265\n",
      "Event ID: 1026, Score: 0.19714375754463306\n",
      "Event ID: 1027, Score: 0.21012120075851085\n",
      "Event ID: 1028, Score: 0.20079247400861783\n",
      "Event ID: 1029, Score: 0.2034249735033674\n",
      "Event ID: 1030, Score: 0.21155372836597053\n",
      "Event ID: 1031, Score: 0.2006539308633819\n",
      "Event ID: 1032, Score: 0.20997003066736397\n",
      "Event ID: 1033, Score: 0.21758516525868268\n",
      "Event ID: 1034, Score: 0.19697658009113184\n",
      "Event ID: 1035, Score: 0.21327076119716804\n",
      "Event ID: 1036, Score: 0.20139465434181147\n",
      "Event ID: 1037, Score: 0.2037033218447943\n",
      "Event ID: 1038, Score: 0.19517862659288449\n",
      "Event ID: 1039, Score: 0.20807887177079465\n",
      "Event ID: 1040, Score: 0.19209192763673855\n",
      "Event ID: 1041, Score: 0.20424218892048884\n",
      "Event ID: 1042, Score: 0.21126172078606942\n",
      "Event ID: 1043, Score: 0.197437154004092\n",
      "Event ID: 1044, Score: 0.20861543283928857\n",
      "Event ID: 1045, Score: 0.19568447829283828\n",
      "Event ID: 1046, Score: 0.1976900188772106\n",
      "Event ID: 1047, Score: 0.2170528548957025\n",
      "Event ID: 1048, Score: 0.2040231372072867\n",
      "Event ID: 1049, Score: 0.20672611417353437\n",
      "Event ID: 1050, Score: 0.20569632370098978\n",
      "Event ID: 1051, Score: 0.20802865217958105\n",
      "Event ID: 1052, Score: 0.2068383962517702\n",
      "Event ID: 1053, Score: 0.20306892184826236\n",
      "Event ID: 1054, Score: 0.2146078932630644\n",
      "Event ID: 1055, Score: 0.20127889534410315\n",
      "Event ID: 1056, Score: 0.21696620008997908\n",
      "Event ID: 1057, Score: 0.21907513332642498\n",
      "Event ID: 1058, Score: 0.2069944925724485\n",
      "Event ID: 1059, Score: 0.21981013303025065\n",
      "Event ID: 1060, Score: 0.20768070714745987\n",
      "Event ID: 1061, Score: 0.21949525288414407\n",
      "Event ID: 1062, Score: 0.2329826117843396\n",
      "Event ID: 1063, Score: 0.20074779241122087\n",
      "Event ID: 1064, Score: 0.21341800495633834\n",
      "Event ID: 1065, Score: 0.19650544446507262\n",
      "Event ID: 1066, Score: 0.19966956507641953\n",
      "Event ID: 1067, Score: 0.21256426559534702\n",
      "Event ID: 1068, Score: 0.20384017861852965\n",
      "Event ID: 1069, Score: 0.2032479969349229\n",
      "Event ID: 1070, Score: 0.20710703140645562\n",
      "Event ID: 1071, Score: 0.20472549289661401\n",
      "Event ID: 1072, Score: 0.2095334754585565\n",
      "Event ID: 1073, Score: 0.2000559700864086\n",
      "Event ID: 1074, Score: 0.2102343688169738\n",
      "Event ID: 1075, Score: 0.19723270446215113\n",
      "Event ID: 1076, Score: 0.22180051295268927\n",
      "Event ID: 1077, Score: 0.21910709746197837\n",
      "Event ID: 1078, Score: 0.21361758674311204\n",
      "Event ID: 1079, Score: 0.20037695016935686\n",
      "Event ID: 1080, Score: 0.19369472606517307\n",
      "Event ID: 1081, Score: 0.2096440362149511\n",
      "Event ID: 1082, Score: 0.2200844468811738\n",
      "Event ID: 1083, Score: 0.2084052727661856\n",
      "Event ID: 1084, Score: 0.1942202973670257\n",
      "Event ID: 1085, Score: 0.19450871538229297\n",
      "Event ID: 1086, Score: 0.20169653687024175\n",
      "Event ID: 1087, Score: 0.1882960815610204\n",
      "Event ID: 1088, Score: 0.2011176224684732\n",
      "Event ID: 1089, Score: 0.1975018487169109\n",
      "Event ID: 1090, Score: 0.207578958167364\n",
      "Event ID: 1091, Score: 0.20429096189982368\n",
      "Event ID: 1092, Score: 0.19914164407951507\n",
      "Event ID: 1093, Score: 0.20110530791079095\n",
      "Event ID: 1094, Score: 0.2033688116310196\n",
      "Event ID: 1095, Score: 0.20461954110110808\n",
      "Event ID: 1096, Score: 0.1904639563342774\n",
      "Event ID: 1097, Score: 0.19508257258997858\n",
      "Event ID: 1098, Score: 0.203659674305527\n",
      "Event ID: 1099, Score: 0.20215996651291387\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bbccb6bb0ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event ID: {}, Score: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean score: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_scores' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "train_sample_scores = []\n",
    "\n",
    "for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "\n",
    "    # Track pattern recognition\n",
    "    model = Clusterer(eps=0.008)\n",
    "    labels = model.predict(hits)\n",
    "\n",
    "    # Prepare submission for an event\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "    score = score_event(truth, one_submission)\n",
    "\n",
    "    train_sample_scores.append(score)\n",
    "\n",
    "#     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "#     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "print('Mean score: %.3f' % (np.mean(train_sample_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.204\n"
     ]
    }
   ],
   "source": [
    "print('Mean score: %.3f' % (np.mean(train_sample_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.001, Mean score: 0.0363\n",
      "eps: 0.002, Mean score: 0.1014\n",
      "eps: 0.003, Mean score: 0.1469\n",
      "eps: 0.004, Mean score: 0.1753\n",
      "eps: 0.005, Mean score: 0.1924\n",
      "eps: 0.006, Mean score: 0.2015\n",
      "eps: 0.007, Mean score: 0.2050\n",
      "eps: 0.008, Mean score: 0.2041\n",
      "eps: 0.009, Mean score: 0.1993\n",
      "eps: 0.100, Mean score: 0.0001\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=eps)\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        score = score_event(truth, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.3f}, Mean score: {1:.4f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.1]\n",
    "mean = [0.0363, 0.1014, 0.1469, 0.1753,0.1924, 0.2015, 0.2050, 0.2041, 0.1993, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad4c3a8f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eps, mean)\n",
    "plt.xlim(0.0, 0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event ID:  0\n",
      "Event ID:  1\n",
      "Event ID:  2\n",
      "Event ID:  3\n",
      "Event ID:  4\n",
      "Event ID:  5\n",
      "Event ID:  6\n",
      "Event ID:  7\n",
      "Event ID:  8\n",
      "Event ID:  9\n",
      "Event ID:  10\n",
      "Event ID:  11\n",
      "Event ID:  12\n",
      "Event ID:  13\n",
      "Event ID:  14\n",
      "Event ID:  15\n",
      "Event ID:  16\n",
      "Event ID:  17\n",
      "Event ID:  18\n",
      "Event ID:  19\n",
      "Event ID:  20\n",
      "Event ID:  21\n",
      "Event ID:  22\n",
      "Event ID:  23\n",
      "Event ID:  24\n",
      "Event ID:  25\n",
      "Event ID:  26\n",
      "Event ID:  27\n",
      "Event ID:  28\n",
      "Event ID:  29\n",
      "Event ID:  30\n",
      "Event ID:  31\n",
      "Event ID:  32\n",
      "Event ID:  33\n",
      "Event ID:  34\n",
      "Event ID:  35\n",
      "Event ID:  36\n",
      "Event ID:  37\n",
      "Event ID:  38\n",
      "Event ID:  39\n",
      "Event ID:  40\n",
      "Event ID:  41\n",
      "Event ID:  42\n",
      "Event ID:  43\n",
      "Event ID:  44\n",
      "Event ID:  45\n",
      "Event ID:  46\n",
      "Event ID:  47\n",
      "Event ID:  48\n",
      "Event ID:  49\n",
      "Event ID:  50\n",
      "Event ID:  51\n",
      "Event ID:  52\n",
      "Event ID:  53\n",
      "Event ID:  54\n",
      "Event ID:  55\n",
      "Event ID:  56\n",
      "Event ID:  57\n",
      "Event ID:  58\n",
      "Event ID:  59\n",
      "Event ID:  60\n",
      "Event ID:  61\n",
      "Event ID:  62\n",
      "Event ID:  63\n",
      "Event ID:  64\n",
      "Event ID:  65\n",
      "Event ID:  66\n",
      "Event ID:  67\n",
      "Event ID:  68\n",
      "Event ID:  69\n",
      "Event ID:  70\n",
      "Event ID:  71\n",
      "Event ID:  72\n",
      "Event ID:  73\n",
      "Event ID:  74\n",
      "Event ID:  75\n",
      "Event ID:  76\n",
      "Event ID:  77\n",
      "Event ID:  78\n",
      "Event ID:  79\n",
      "Event ID:  80\n",
      "Event ID:  81\n",
      "Event ID:  82\n",
      "Event ID:  83\n",
      "Event ID:  84\n",
      "Event ID:  85\n",
      "Event ID:  86\n",
      "Event ID:  87\n",
      "Event ID:  88\n",
      "Event ID:  89\n",
      "Event ID:  90\n",
      "Event ID:  91\n",
      "Event ID:  92\n",
      "Event ID:  93\n",
      "Event ID:  94\n",
      "Event ID:  95\n",
      "Event ID:  96\n",
      "Event ID:  97\n",
      "Event ID:  98\n",
      "Event ID:  99\n",
      "Event ID:  100\n",
      "Event ID:  101\n",
      "Event ID:  102\n",
      "Event ID:  103\n",
      "Event ID:  104\n",
      "Event ID:  105\n",
      "Event ID:  106\n",
      "Event ID:  107\n",
      "Event ID:  108\n",
      "Event ID:  109\n",
      "Event ID:  110\n",
      "Event ID:  111\n",
      "Event ID:  112\n",
      "Event ID:  113\n",
      "Event ID:  114\n",
      "Event ID:  115\n",
      "Event ID:  116\n",
      "Event ID:  117\n",
      "Event ID:  118\n",
      "Event ID:  119\n",
      "Event ID:  120\n",
      "Event ID:  121\n",
      "Event ID:  122\n",
      "Event ID:  123\n",
      "Event ID:  124\n"
     ]
    }
   ],
   "source": [
    "# this scores 0.2079 on the public LB\n",
    "path_to_test = \"../data/test\"\n",
    "test_dataset_submissions = []\n",
    "\n",
    "create_submission = True # True for submission \n",
    "\n",
    "if create_submission:\n",
    "    for event_id, hits, cells in load_dataset(path_to_test, parts=['hits', 'cells']):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=0.007)\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        test_dataset_submissions.append(one_submission)\n",
    "        \n",
    "        print('Event ID: ', event_id)\n",
    "\n",
    "    # Create submission file\n",
    "    submussion = pd.concat(test_dataset_submissions, axis=0)\n",
    "    IDENTIFIER = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    submussion.to_csv('../submissions/sub-DBSCAN-sample-{}.csv.gz'.format(IDENTIFIER), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.0061, Mean score: 0.2020\n",
      "eps: 0.0062, Mean score: 0.2026\n",
      "eps: 0.0063, Mean score: 0.2031\n",
      "eps: 0.0064, Mean score: 0.2035\n",
      "eps: 0.0065, Mean score: 0.2039\n",
      "eps: 0.0066, Mean score: 0.2041\n",
      "eps: 0.0067, Mean score: 0.2045\n",
      "eps: 0.0068, Mean score: 0.2048\n",
      "eps: 0.0069, Mean score: 0.2049\n",
      "eps: 0.0070, Mean score: 0.2050\n",
      "eps: 0.0071, Mean score: 0.2051\n",
      "eps: 0.0072, Mean score: 0.2052\n",
      "eps: 0.0073, Mean score: 0.2052\n",
      "eps: 0.0074, Mean score: 0.2052\n",
      "eps: 0.0075, Mean score: 0.2050\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.0061, 0.0062, 0.0063, 0.0064, 0.0065, 0.0066, 0.0067, 0.0068, 0.0069, 0.007, 0.0071, 0.0072, 0.0073, 0.0074, 0.0075]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=eps)\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        score = score_event(truth, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.4f}, Mean score: {1:.6f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.0072, Mean score: 0.20521408\n",
      "eps: 0.0073, Mean score: 0.20519609\n",
      "eps: 0.0074, Mean score: 0.20521875\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.0072, 0.0073, 0.0074]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "\n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=eps)\n",
    "        labels = model.predict(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        score = score_event(truth, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.4f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "event_prefix = \"event000001000\"\n",
    "hits, cells, particles, truth = load_event(os.path.join(path_to_train, event_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hit_id          x         y       z  volume_id  layer_id  module_id\n",
      "0       1 -64.409897 -7.163700 -1502.5          7         2          1\n",
      "1       2 -55.336102  0.635342 -1502.5          7         2          1\n",
      "2       3 -83.830498 -1.143010 -1502.5          7         2          1\n",
      "3       4 -96.109100 -8.241030 -1502.5          7         2          1\n",
      "4       5 -62.673599 -9.371200 -1502.5          7         2          1\n",
      "(120939, 7)\n",
      "   hit_id  ch0  ch1     value\n",
      "0       1  209  617  0.013832\n",
      "1       1  210  617  0.079887\n",
      "2       1  209  618  0.211723\n",
      "3       2   68  446  0.334087\n",
      "4       3   58  954  0.034005\n",
      "(664996, 4)\n",
      "        particle_id        vx        vy        vz         px         py  \\\n",
      "0  4503668346847232 -0.009288  0.009861 -0.077879  -0.055269   0.323272   \n",
      "1  4503737066323968 -0.009288  0.009861 -0.077879  -0.948125   0.470892   \n",
      "2  4503805785800704 -0.009288  0.009861 -0.077879  -0.886484   0.105749   \n",
      "3  4503874505277440 -0.009288  0.009861 -0.077879   0.257539  -0.676718   \n",
      "4  4503943224754176 -0.009288  0.009861 -0.077879  16.439400 -15.548900   \n",
      "\n",
      "          pz  q  nhits  \n",
      "0  -0.203492 -1      8  \n",
      "1   2.010060  1     11  \n",
      "2   0.683881 -1      0  \n",
      "3   0.991616  1     12  \n",
      "4 -39.824902  1      3  \n",
      "(12263, 9)\n",
      "   hit_id         particle_id         tx        ty      tz            tpx  \\\n",
      "0       1                   0 -64.411598 -7.164120 -1502.5  250710.000000   \n",
      "1       2   22525763437723648 -55.338501  0.630805 -1502.5      -0.570605   \n",
      "2       3                   0 -83.828003 -1.145580 -1502.5  626295.000000   \n",
      "3       4  297237712845406208 -96.122902 -8.230360 -1502.5      -0.225235   \n",
      "4       5  418835796137607168 -62.659401 -9.375040 -1502.5      -0.281806   \n",
      "\n",
      "             tpy           tpz    weight  \n",
      "0 -149908.000000 -956385.00000  0.000000  \n",
      "1       0.028390     -15.49220  0.000010  \n",
      "2 -169767.000000 -760877.00000  0.000000  \n",
      "3      -0.050968      -3.70232  0.000008  \n",
      "4      -0.023487      -6.57318  0.000009  \n",
      "(120939, 9)\n"
     ]
    }
   ],
   "source": [
    "print(hits.head())\n",
    "print(hits.shape)\n",
    "print(cells.head())\n",
    "print(cells.shape)\n",
    "print(particles.head())\n",
    "print(particles.shape)\n",
    "print(truth.head())\n",
    "print(truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth1 = pd.merge(truth, particles, how='left', on='particle_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120939, 17)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>particle_id</th>\n",
       "      <th>tx</th>\n",
       "      <th>ty</th>\n",
       "      <th>tz</th>\n",
       "      <th>tpx</th>\n",
       "      <th>tpy</th>\n",
       "      <th>tpz</th>\n",
       "      <th>weight</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>q</th>\n",
       "      <th>nhits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.411598</td>\n",
       "      <td>-7.164120</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>250710.000000</td>\n",
       "      <td>-149908.000000</td>\n",
       "      <td>-956385.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22525763437723648</td>\n",
       "      <td>-55.338501</td>\n",
       "      <td>0.630805</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.570605</td>\n",
       "      <td>0.028390</td>\n",
       "      <td>-15.49220</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.015802</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>1.16279</td>\n",
       "      <td>-0.569670</td>\n",
       "      <td>-0.011187</td>\n",
       "      <td>-15.49600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-83.828003</td>\n",
       "      <td>-1.145580</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>626295.000000</td>\n",
       "      <td>-169767.000000</td>\n",
       "      <td>-760877.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>297237712845406208</td>\n",
       "      <td>-96.122902</td>\n",
       "      <td>-8.230360</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.225235</td>\n",
       "      <td>-0.050968</td>\n",
       "      <td>-3.70232</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>-0.015051</td>\n",
       "      <td>5.75865</td>\n",
       "      <td>-0.240629</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-3.70766</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>418835796137607168</td>\n",
       "      <td>-62.659401</td>\n",
       "      <td>-9.375040</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.281806</td>\n",
       "      <td>-0.023487</td>\n",
       "      <td>-6.57318</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.018366</td>\n",
       "      <td>-0.016865</td>\n",
       "      <td>4.19268</td>\n",
       "      <td>-0.268943</td>\n",
       "      <td>-0.058487</td>\n",
       "      <td>-6.58619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hit_id         particle_id         tx        ty      tz            tpx  \\\n",
       "0       1                   0 -64.411598 -7.164120 -1502.5  250710.000000   \n",
       "1       2   22525763437723648 -55.338501  0.630805 -1502.5      -0.570605   \n",
       "2       3                   0 -83.828003 -1.145580 -1502.5  626295.000000   \n",
       "3       4  297237712845406208 -96.122902 -8.230360 -1502.5      -0.225235   \n",
       "4       5  418835796137607168 -62.659401 -9.375040 -1502.5      -0.281806   \n",
       "\n",
       "             tpy           tpz    weight        vx        vy       vz  \\\n",
       "0 -149908.000000 -956385.00000  0.000000       NaN       NaN      NaN   \n",
       "1       0.028390     -15.49220  0.000010 -0.015802  0.006381  1.16279   \n",
       "2 -169767.000000 -760877.00000  0.000000       NaN       NaN      NaN   \n",
       "3      -0.050968      -3.70232  0.000008 -0.000486 -0.015051  5.75865   \n",
       "4      -0.023487      -6.57318  0.000009  0.018366 -0.016865  4.19268   \n",
       "\n",
       "         px        py        pz    q  nhits  \n",
       "0       NaN       NaN       NaN  NaN    NaN  \n",
       "1 -0.569670 -0.011187 -15.49600  1.0   10.0  \n",
       "2       NaN       NaN       NaN  NaN    NaN  \n",
       "3 -0.240629  0.012071  -3.70766 -1.0   11.0  \n",
       "4 -0.268943 -0.058487  -6.58619  1.0   10.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth2 = truth1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103305, 17)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits1 = pd.merge(hits, truth1, how='left', on='hit_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120939, 23)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>module_id</th>\n",
       "      <th>particle_id</th>\n",
       "      <th>tx</th>\n",
       "      <th>ty</th>\n",
       "      <th>...</th>\n",
       "      <th>tpz</th>\n",
       "      <th>weight</th>\n",
       "      <th>vx</th>\n",
       "      <th>vy</th>\n",
       "      <th>vz</th>\n",
       "      <th>px</th>\n",
       "      <th>py</th>\n",
       "      <th>pz</th>\n",
       "      <th>q</th>\n",
       "      <th>nhits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-64.409897</td>\n",
       "      <td>-7.163700</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.411598</td>\n",
       "      <td>-7.164120</td>\n",
       "      <td>...</td>\n",
       "      <td>-956385.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-55.336102</td>\n",
       "      <td>0.635342</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22525763437723648</td>\n",
       "      <td>-55.338501</td>\n",
       "      <td>0.630805</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.49220</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.015802</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>1.16279</td>\n",
       "      <td>-0.569670</td>\n",
       "      <td>-0.011187</td>\n",
       "      <td>-15.49600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-83.830498</td>\n",
       "      <td>-1.143010</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-83.828003</td>\n",
       "      <td>-1.145580</td>\n",
       "      <td>...</td>\n",
       "      <td>-760877.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-96.109100</td>\n",
       "      <td>-8.241030</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>297237712845406208</td>\n",
       "      <td>-96.122902</td>\n",
       "      <td>-8.230360</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.70232</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>-0.015051</td>\n",
       "      <td>5.75865</td>\n",
       "      <td>-0.240629</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-3.70766</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-62.673599</td>\n",
       "      <td>-9.371200</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>418835796137607168</td>\n",
       "      <td>-62.659401</td>\n",
       "      <td>-9.375040</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57318</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.018366</td>\n",
       "      <td>-0.016865</td>\n",
       "      <td>4.19268</td>\n",
       "      <td>-0.268943</td>\n",
       "      <td>-0.058487</td>\n",
       "      <td>-6.58619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hit_id          x         y       z  volume_id  layer_id  module_id  \\\n",
       "0       1 -64.409897 -7.163700 -1502.5          7         2          1   \n",
       "1       2 -55.336102  0.635342 -1502.5          7         2          1   \n",
       "2       3 -83.830498 -1.143010 -1502.5          7         2          1   \n",
       "3       4 -96.109100 -8.241030 -1502.5          7         2          1   \n",
       "4       5 -62.673599 -9.371200 -1502.5          7         2          1   \n",
       "\n",
       "          particle_id         tx        ty  ...             tpz    weight  \\\n",
       "0                   0 -64.411598 -7.164120  ...   -956385.00000  0.000000   \n",
       "1   22525763437723648 -55.338501  0.630805  ...       -15.49220  0.000010   \n",
       "2                   0 -83.828003 -1.145580  ...   -760877.00000  0.000000   \n",
       "3  297237712845406208 -96.122902 -8.230360  ...        -3.70232  0.000008   \n",
       "4  418835796137607168 -62.659401 -9.375040  ...        -6.57318  0.000009   \n",
       "\n",
       "         vx        vy       vz        px        py        pz    q  nhits  \n",
       "0       NaN       NaN      NaN       NaN       NaN       NaN  NaN    NaN  \n",
       "1 -0.015802  0.006381  1.16279 -0.569670 -0.011187 -15.49600  1.0   10.0  \n",
       "2       NaN       NaN      NaN       NaN       NaN       NaN  NaN    NaN  \n",
       "3 -0.000486 -0.015051  5.75865 -0.240629  0.012071  -3.70766 -1.0   11.0  \n",
       "4  0.018366 -0.016865  4.19268 -0.268943 -0.058487  -6.58619  1.0   10.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.0072, Mean score: 0.20551536\n",
      "eps: 0.0073, Mean score: 0.20562315\n",
      "eps: 0.0074, Mean score: 0.20572021\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.0072, 0.0073, 0.0074]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "        \n",
    "        truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "        hits2 = hits1.dropna()\n",
    "        truth2 = truth1.dropna()\n",
    "        \n",
    "        hits3 = hits2[hits2.nhits > 3]\n",
    "        truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "        # Track pattern recognition\n",
    "        model = Clusterer(eps=eps)\n",
    "        labels = model.predict(hits3)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "        score = score_event(truth3, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.4f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "# for eps in [0.0075, 0.0076, 0.0077, 0.0078, 0.0079, 0.008, 0.009]:\n",
    "#     train_sample_scores = []\n",
    "#     for event_id, hits, cells, particles, truth in load_dataset(path_to_train):\n",
    "        \n",
    "#         truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "#         hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "#         hits2 = hits1.dropna()\n",
    "#         truth2 = truth1.dropna()\n",
    "        \n",
    "#         hits3 = hits2[hits2.nhits > 3]\n",
    "#         truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "#         # Track pattern recognition\n",
    "#         model = Clusterer(eps=eps)\n",
    "#         labels = model.predict(hits3)\n",
    "\n",
    "#         # Prepare submission for an event\n",
    "#         one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "#         score = score_event(truth3, one_submission)\n",
    "\n",
    "#         train_sample_scores.append(score)\n",
    "\n",
    "#     #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "#     #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "#     print('eps: {0:.4f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))\n",
    "\n",
    "# eps: 0.0075, Mean score: 0.20580227\n",
    "# eps: 0.0076, Mean score: 0.20572321\n",
    "# eps: 0.0077, Mean score: 0.20564800\n",
    "# eps: 0.0078, Mean score: 0.20545296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [25:55,  2.83s/it]"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train\"\n",
    "\n",
    "eps = 0.0075\n",
    "model = Clusterer1(eps=eps)\n",
    "train_hits = pd.DataFrame()\n",
    "train_sample_scores = []\n",
    "for event_id, hits, cells, particles, truth in tqdm(load_dataset(path_to_train)):\n",
    "\n",
    "    truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "    hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "\n",
    "    hits2 = hits1.dropna()\n",
    "    truth2 = truth1.dropna()\n",
    "\n",
    "    hits3 = hits2[hits2.nhits > 3]\n",
    "    truth3 = truth2[truth2.nhits > 3]\n",
    "\n",
    "#     train_hits = train_hits.append(hits3)\n",
    "    model.fit(hits3)\n",
    "    del hits1, truth1\n",
    "    del hits2, truth2\n",
    "    del hits3, truth3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284374539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot serialize a bytes object larger than 4 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-60d789e38ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../cache/train_hits_3161.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a bytes object larger than 4 GiB"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "with open('../cache/train_hits_3161.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this scores 0.2079 on the public LB\n",
    "path_to_test = \"../data/test\"\n",
    "test_dataset_submissions = []\n",
    "\n",
    "create_submission = True # True for submission \n",
    "\n",
    "if create_submission:\n",
    "    for event_id, hits, cells, particles in load_dataset(path_to_test, parts=['hits', 'cells', 'particles']):\n",
    "\n",
    "        # Track pattern recognition\n",
    "#         model = Clusterer(eps=0.007)\n",
    "        labels = model.predict_test(hits)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "        test_dataset_submissions.append(one_submission)\n",
    "        \n",
    "        print('Event ID: ', event_id)\n",
    "\n",
    "    # Create submission file\n",
    "    submussion = pd.concat(test_dataset_submissions, axis=0)\n",
    "    IDENTIFIER = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    submussion.to_csv('../submissions/sub-DBSCAN-sample-{}.csv.gz'.format(IDENTIFIER), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.000095\n",
      "Score for event 1001: 0.000496\n",
      "Score for event 1002: 0.000142\n",
      "Score for event 1003: 0.000383\n",
      "Score for event 1004: 0.000193\n",
      "Mean score: 0.000262\n"
     ]
    }
   ],
   "source": [
    "dataset_submissions = []\n",
    "dataset_scores = []\n",
    "path_to_train = \"../data/train\"\n",
    "for event_id, hits, cells, particles, truth in load_dataset(path_to_train, skip=0, nevents=5):\n",
    "        \n",
    "    # Track pattern recognition\n",
    "    model = Clusterer2(eps=0.075)\n",
    "    labels = model.predict(hits)\n",
    "        \n",
    "    # Prepare submission for an event\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels)\n",
    "    dataset_submissions.append(one_submission)\n",
    "    \n",
    "    # Score for the event\n",
    "    score = score_event(truth, one_submission)\n",
    "    dataset_scores.append(score)\n",
    "    \n",
    "    print(\"Score for event %d: %.6f\" % (event_id, score))\n",
    "    \n",
    "print('Mean score: %.6f' % (np.mean(dataset_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.001000, Mean score: 0.00000000\n",
      "eps: 0.002000, Mean score: 0.00000000\n",
      "eps: 0.003000, Mean score: 0.00000000\n",
      "eps: 0.004000, Mean score: 0.00025171\n",
      "eps: 0.005000, Mean score: 0.00053165\n",
      "eps: 0.006000, Mean score: 0.00051397\n",
      "eps: 0.007000, Mean score: 0.00049043\n",
      "eps: 0.008000, Mean score: 0.00046006\n",
      "eps: 0.009000, Mean score: 0.00040250\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train, nevents=5):\n",
    "        \n",
    "        truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "        hits2 = hits1.dropna()\n",
    "        truth2 = truth1.dropna()\n",
    "        \n",
    "        hits3 = hits2[hits2.nhits > 3]\n",
    "        truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "        # Track pattern recognition\n",
    "        model = Clusterer2(eps=eps)\n",
    "        labels = model.predict(hits3)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "        score = score_event(truth3, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.6f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.010000, Mean score: 0.00036549\n",
      "eps: 0.100000, Mean score: 0.00031835\n",
      "eps: 1.000000, Mean score: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.01, 0.1, 1.0]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train, nevents=5):\n",
    "        \n",
    "        truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "        hits2 = hits1.dropna()\n",
    "        truth2 = truth1.dropna()\n",
    "        \n",
    "        hits3 = hits2[hits2.nhits > 3]\n",
    "        truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "        # Track pattern recognition\n",
    "        model = Clusterer2(eps=eps)\n",
    "        labels = model.predict(hits3)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "        score = score_event(truth3, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.6f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.001000, Mean score: 0.11695307\n",
      "eps: 0.002000, Mean score: 0.14883210\n",
      "eps: 0.003000, Mean score: 0.14835454\n",
      "eps: 0.004000, Mean score: 0.13623065\n",
      "eps: 0.005000, Mean score: 0.11959371\n",
      "eps: 0.006000, Mean score: 0.10101758\n",
      "eps: 0.007000, Mean score: 0.08231689\n",
      "eps: 0.008000, Mean score: 0.06673448\n",
      "eps: 0.009000, Mean score: 0.05183591\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train, nevents=5):\n",
    "        \n",
    "        truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "        hits2 = hits1.dropna()\n",
    "        truth2 = truth1.dropna()\n",
    "        \n",
    "        hits3 = hits2[hits2.nhits > 3]\n",
    "        truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "        # Track pattern recognition\n",
    "        model = Clusterer3(eps=eps)\n",
    "        labels = model.predict(hits3)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "        score = score_event(truth3, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.6f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.003100, Mean score: 0.14754640\n",
      "eps: 0.003200, Mean score: 0.14627944\n",
      "eps: 0.003300, Mean score: 0.14557765\n",
      "eps: 0.003400, Mean score: 0.14465905\n",
      "eps: 0.003500, Mean score: 0.14367359\n",
      "eps: 0.003600, Mean score: 0.14261856\n",
      "eps: 0.003700, Mean score: 0.14097457\n",
      "eps: 0.003800, Mean score: 0.13942491\n",
      "eps: 0.003900, Mean score: 0.13794391\n"
     ]
    }
   ],
   "source": [
    "path_to_train = \"../data/train_100_events\"\n",
    "\n",
    "\n",
    "for eps in [0.0031, 0.0032, 0.0033, 0.0034, 0.0035, 0.0036, 0.0037, 0.0038, 0.0039]:\n",
    "    train_sample_scores = []\n",
    "    for event_id, hits, cells, particles, truth in load_dataset(path_to_train, nevents=5):\n",
    "        \n",
    "        truth1 = pd.merge(truth, particles, how='left', on='particle_id')\n",
    "        hits1 = pd.merge(hits, truth1, how='left', on='hit_id')\n",
    "        \n",
    "        hits2 = hits1.dropna()\n",
    "        truth2 = truth1.dropna()\n",
    "        \n",
    "        hits3 = hits2[hits2.nhits > 3]\n",
    "        truth3 = truth2[truth2.nhits > 3]\n",
    "        \n",
    "        # Track pattern recognition\n",
    "        model = Clusterer3(eps=eps)\n",
    "        labels = model.predict(hits3)\n",
    "\n",
    "        # Prepare submission for an event\n",
    "        one_submission = create_one_event_submission(event_id, hits3, labels)\n",
    "        score = score_event(truth3, one_submission)\n",
    "\n",
    "        train_sample_scores.append(score)\n",
    "\n",
    "    #     print(\"Score for event %d: %.3f\" % (event_id, score))\n",
    "\n",
    "    #     print('Event ID: {}, Score: {}'.format(event_id, score))\n",
    "    print('eps: {0:.6f}, Mean score: {1:.8f}'.format(eps, (np.mean(train_sample_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braycurtis\n",
      "braycurtis 10565 35296 0.23369642250193956\n",
      "[[0.23369642250193956, 'braycurtis']]\n"
     ]
    }
   ],
   "source": [
    "#  most imp phi, then r\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "hits = pd.read_csv('../cache/hits_1000_4_15_4_50.csv')\n",
    "# rz_scales=[0.65, 0.965, 1.528]\n",
    "rz_scales=[1., 1., 1.]\n",
    "hits['x2'] = hits['x2'] * rz_scales[0]\n",
    "hits['y2'] = hits['y2'] * rz_scales[1]\n",
    "hits['z2'] = hits['z2'] * rz_scales[2]\n",
    "\n",
    "x = hits.x.values\n",
    "y = hits.y.values\n",
    "z = hits.z.values\n",
    "\n",
    "x2 = hits.x2.values\n",
    "y2 = hits.y2.values\n",
    "z2 = hits.z2.values\n",
    "hits['rho'] = np.sqrt(x**2 + y**2 + z**2)\n",
    "hits['r'] = np.sqrt(x2**2 + y2**2)\n",
    "hits['phi'] = np.degrees(np.arctan2(hits['y2'], hits['x2']))\n",
    "hits['phi_1'] = np.round(np.degrees(np.arctan2(hits['y2'], hits['x2'])), 5)\n",
    "hits['theta'] = np.degrees(np.arctan2(hits['r'], hits['z2']))\n",
    "phi = hits['phi'].values\n",
    "theta = hits['theta'].values\n",
    "rho = hits['rho'].values\n",
    "r = hits['r'].values\n",
    "hits['tan_dip'] = phi/theta\n",
    "hits['tan_dip1'] = phi/z2\n",
    "hits['z2_1'] = 1/z2\n",
    "hits['z2_2'] = phi/z2 + 1/z2\n",
    "# hits['dip'] = np.degrees(np.arctan2(hits['phi'], hits['z2']))\n",
    "hits['dip_angle'] = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "scores = []\n",
    "for m in ['braycurtis']: #Tuning/Grid Search\n",
    "    print(m)\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=0.011, min_samples=1,metric='euclidean',n_jobs=8)\n",
    "        \n",
    "#             cl = Clusterer2()\n",
    "        hits['track_id'] = dbscan.fit_predict(scl.fit_transform(hits[['z2', 'phi', 'r', 'tan_dip1', 'z2_1', 'z2_2']].values))\n",
    "#             hits['track_id'] = cl.predict(hits)\n",
    "        score = score_event(hits, hits[['event_id','hit_id','track_id']])\n",
    "        print(m, len(hits['particle_id'].unique()), len(hits['track_id'].unique()), score)\n",
    "        scores.append([score, m])\n",
    "    except e:\n",
    "        print(\"ERROR:\", e)\n",
    "#         break #Remove to test all\n",
    "#     break #Remove to test more samples\n",
    "print(sorted(scores, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braycurtis\n",
      "braycurtis 10565 35467 0.23360570376754058\n",
      "[[0.23360570376754058, 'braycurtis']]\n"
     ]
    }
   ],
   "source": [
    "#  most imp phi, then r\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "hits = pd.read_csv('../cache/hits_1000_4_15_4_50.csv')\n",
    "# rz_scales=[0.65, 0.965, 1.528]\n",
    "rz_scales=[1., 1., 1.]\n",
    "hits['x2'] = hits['x2'] * rz_scales[0]\n",
    "hits['y2'] = hits['y2'] * rz_scales[1]\n",
    "hits['z2'] = hits['z2'] * rz_scales[2]\n",
    "\n",
    "x = hits.x.values\n",
    "y = hits.y.values\n",
    "z = hits.z.values\n",
    "\n",
    "x2 = hits.x2.values\n",
    "y2 = hits.y2.values\n",
    "z2 = hits.z2.values\n",
    "hits['rho'] = np.sqrt(x**2 + y**2 + z**2)\n",
    "hits['r'] = np.sqrt(x2**2 + y2**2)\n",
    "hits['phi'] = np.arctan2(hits['y2'], hits['x2'])\n",
    "hits['phi_1'] = np.round(np.degrees(np.arctan2(hits['y2'], hits['x2'])), 5)\n",
    "hits['theta'] = np.arctan2(hits['r'], hits['z2'])\n",
    "phi = hits['phi'].values\n",
    "theta = hits['theta'].values\n",
    "rho = hits['rho'].values\n",
    "r = hits['r'].values\n",
    "hits['tan_dip'] = phi/theta\n",
    "hits['tan_dip1'] = phi/z2\n",
    "hits['z2_1'] = 1/z2\n",
    "hits['z2_2'] = phi/z2 + 1/z2\n",
    "# hits['dip'] = np.degrees(np.arctan2(hits['phi'], hits['z2']))\n",
    "hits['dip_angle'] = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "scores = []\n",
    "for m in ['braycurtis']: #Tuning/Grid Search\n",
    "    print(m)\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=0.011, min_samples=1,metric='euclidean',n_jobs=8)\n",
    "        \n",
    "#             cl = Clusterer2()\n",
    "        hits['track_id'] = dbscan.fit_predict(scl.fit_transform(hits[['z2', 'phi', 'r', 'tan_dip1', 'z2_1', 'z2_2']].values))\n",
    "#             hits['track_id'] = cl.predict(hits)\n",
    "        score = score_event(hits, hits[['event_id','hit_id','track_id']])\n",
    "        print(m, len(hits['particle_id'].unique()), len(hits['track_id'].unique()), score)\n",
    "        scores.append([score, m])\n",
    "    except e:\n",
    "        print(\"ERROR:\", e)\n",
    "#         break #Remove to test all\n",
    "#     break #Remove to test more samples\n",
    "print(sorted(scores, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh['r'] = np.sqrt(dfh.x**2+dfh.y**2+dfh.z**2) (rho)\n",
    "        dfh['rt'] = np.sqrt(dfh.x**2+dfh.y**2) (r)\n",
    "        dfh['a0'] = np.arctan2(dfh.y,dfh.x) (phi)\n",
    "        dfh['r2'] = np.sqrt(dfh.x**2+dfh.y**2) (r)\n",
    "        dfh['z1'] = dfh['z']/dfh['rt'] (z2)      \n",
    "\n",
    "dfh['a1'] = dfh['a0']+dz*dfh['z']*np.sign(dfh['z'].values) (phi increments)\n",
    "            dfh['x1'] = dfh['a1']/dfh['z1'] ((phi/z2, same as tan_dip1))\n",
    "            dfh['x2'] = 1/dfh['z1'] (1/z2)\n",
    "            dfh['x3'] = dfh['x1']+dfh['x2'] (phi/z2 + 1/z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braycurtis\n",
      "braycurtis 10565 35467 0.23360570376754058\n",
      "[[0.23360570376754058, 'braycurtis']]\n"
     ]
    }
   ],
   "source": [
    "#  most imp phi, then r\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "hits = pd.read_csv('../cache/hits_1000_4_15_4_50.csv')\n",
    "# rz_scales=[0.65, 0.965, 1.528]\n",
    "rz_scales=[1., 1., 1.]\n",
    "hits['x2'] = hits['x2'] * rz_scales[0]\n",
    "hits['y2'] = hits['y2'] * rz_scales[1]\n",
    "hits['z2'] = hits['z2'] * rz_scales[2]\n",
    "\n",
    "x = hits.x.values\n",
    "y = hits.y.values\n",
    "z = hits.z.values\n",
    "\n",
    "x2 = hits.x2.values\n",
    "y2 = hits.y2.values\n",
    "z2 = hits.z2.values\n",
    "hits['rho'] = np.sqrt(x**2 + y**2 + z**2)\n",
    "hits['r'] = np.sqrt(x**2 + y**2)\n",
    "hits['r2'] = np.sqrt(x2**2 + y2**2)\n",
    "hits['phi'] = np.arctan2(hits['y'], hits['x'])\n",
    "hits['phi2'] = np.arctan2(hits['y2'], hits['x2'])\n",
    "hits['phi_1'] = np.round(np.degrees(np.arctan2(hits['y'], hits['x'])), 5)\n",
    "hits['theta'] = np.arctan2(hits['r'], hits['z'])\n",
    "hits['theta'] = np.arctan2(hits['r2'], hits['z2'])\n",
    "phi = hits['phi'].values\n",
    "theta = hits['theta'].values\n",
    "rho = hits['rho'].values\n",
    "r = hits['r'].values\n",
    "hits['tan_dip'] = phi/theta\n",
    "hits['tan_dip1'] = phi/z2\n",
    "hits['z2_1'] = 1/z2\n",
    "hits['z2_2'] = phi/z2 + 1/z2\n",
    "# hits['dip'] = np.degrees(np.arctan2(hits['phi'], hits['z2']))\n",
    "# hits['dip_angle'] = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "scores = []\n",
    "for m in ['braycurtis']: #Tuning/Grid Search\n",
    "    print(m)\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=0.011, min_samples=1,metric='euclidean',n_jobs=8)\n",
    "        \n",
    "#             cl = Clusterer2()\n",
    "        hits['track_id'] = dbscan.fit_predict(scl.fit_transform(hits[['z2', 'phi2', 'r2','tan_dip1', 'z2_1', 'z2_2']].values))\n",
    "#             hits['track_id'] = cl.predict(hits)\n",
    "        score = score_event(hits, hits[['event_id','hit_id','track_id']])\n",
    "        print(m, len(hits['particle_id'].unique()), len(hits['track_id'].unique()), score)\n",
    "        scores.append([score, m])\n",
    "    except e:\n",
    "        print(\"ERROR:\", e)\n",
    "#         break #Remove to test all\n",
    "#     break #Remove to test more samples\n",
    "print(sorted(scores, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braycurtis\n",
      "braycurtis 10565 35467 0.23360570376754058\n",
      "[[0.23360570376754058, 'braycurtis']]\n"
     ]
    }
   ],
   "source": [
    "#  most imp phi, then r\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "hits = pd.read_csv('../cache/hits_1000_4_15_4_50.csv')\n",
    "# rz_scales=[0.65, 0.965, 1.528]\n",
    "rz_scales=[1., 1., 1.]\n",
    "hits['x2'] = hits['x2'] * rz_scales[0]\n",
    "hits['y2'] = hits['y2'] * rz_scales[1]\n",
    "hits['z2'] = hits['z2'] * rz_scales[2]\n",
    "\n",
    "x = hits.x.values\n",
    "y = hits.y.values\n",
    "z = hits.z.values\n",
    "\n",
    "x2 = hits.x2.values\n",
    "y2 = hits.y2.values\n",
    "z2 = hits.z2.values\n",
    "hits['rho'] = np.sqrt(x**2 + y**2 + z**2)\n",
    "hits['r'] = np.sqrt(x**2 + y**2)\n",
    "hits['r2'] = np.sqrt(x2**2 + y2**2)\n",
    "hits['phi'] = np.arctan2(hits['y'], hits['x'])\n",
    "hits['phi2'] = np.arctan2(hits['y2'], hits['x2'])\n",
    "hits['phi_1'] = np.round(np.degrees(np.arctan2(hits['y'], hits['x'])), 5)\n",
    "hits['theta'] = np.arctan2(hits['r'], hits['z'])\n",
    "hits['theta'] = np.arctan2(hits['r2'], hits['z2'])\n",
    "phi = hits['phi'].values\n",
    "theta = hits['theta'].values\n",
    "rho = hits['rho'].values\n",
    "r = hits['r'].values\n",
    "hits['tan_dip'] = phi/theta\n",
    "hits['tan_dip1'] = phi/z2\n",
    "hits['z2_1'] = 1/z2\n",
    "hits['z2_2'] = phi/z2 + 1/z2\n",
    "# hits['dip'] = np.degrees(np.arctan2(hits['phi'], hits['z2']))\n",
    "hits['dip_angle'] = np.arctan2(z2, (np.sqrt(x2**2 +y2**2)) * np.arccos(x2/np.sqrt(x2**2 + y2**2)))\n",
    "scores = []\n",
    "for m in ['braycurtis']: #Tuning/Grid Search\n",
    "    print(m)\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=0.011, min_samples=1,metric='euclidean',n_jobs=8)\n",
    "        \n",
    "#             cl = Clusterer2()\n",
    "        hits['track_id'] = dbscan.fit_predict(scl.fit_transform(hits[['z2', 'phi2', 'r2', 'tan_dip1', 'z2_1', 'z2_2']].values))\n",
    "#             hits['track_id'] = cl.predict(hits)\n",
    "        score = score_event(hits, hits[['event_id','hit_id','track_id']])\n",
    "        print(m, len(hits['particle_id'].unique()), len(hits['track_id'].unique()), score)\n",
    "        scores.append([score, m])\n",
    "    except e:\n",
    "        print(\"ERROR:\", e)\n",
    "#         break #Remove to test all\n",
    "#     break #Remove to test more samples\n",
    "print(sorted(scores, reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml",
   "language": "python",
   "name": "tml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
